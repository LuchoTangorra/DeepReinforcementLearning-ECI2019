{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "TP DRL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56Ku39PjHySn",
        "colab_type": "text"
      },
      "source": [
        "# Este trabajo fue realizado por Luciano Tangorra, utilizando como guia el material brindado por la cátedra de Aprendizaje profundo por refuerzo, de la ECI 2019, y conocimiento previo obtenido en su universidad (UNICEN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiJ_x1FN1fac",
        "colab_type": "text"
      },
      "source": [
        "# Jugador aleatorio con _Football_ en Google Colaboratory\n",
        "\n",
        "Ejemplo de jugador aleatorio para el entorno [_Football_](https://github.com/google-research/football) versión `academy_empty_goal_close`. \n",
        "\n",
        "Ejecución en Google Colab: cuaderno completo, incluye instalación de dependencias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdRyJ3HU1fag",
        "colab_type": "text"
      },
      "source": [
        "### Añadir repositorio para drivers OpenGL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kMEp7t11fai",
        "colab_type": "code",
        "outputId": "5374e7e0-df1a-4fad-d4c2-8bec6ad6af76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!add-apt-repository ppa:ubuntu-x-swat/updates\n",
        "!apt-get dist-upgrade"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Updated versions of X.org drivers, libraries, etc. for Ubuntu.\n",
            "\n",
            "This PPA is for stable upstream releases of X.org components. If you're looking for something even more bleeding-edge, please see the xorg-edgers PPA.\n",
            "\n",
            "While Ubuntu does not \"officially/formally\" support these packages, if you discover problems when installing these debs please feel free to report bugs to launchpad. However, please make sure to clearly state that you are running packages from this PPA so we know the fixes need to come here.\n",
            "\n",
            "If you are upgrading from one release to another with this PPA activated, please install the ppa-purge package and use it to downgrade everything in here beforehand. sudo ppa-purge ppa:ubuntu-x-swat/updates will do it.\n",
            "\n",
            "[Directions for packaging drivers can be seen at https://wiki.ubuntu.com/X/DriverBuilding]\n",
            " More info: https://launchpad.net/~ubuntu-x-swat/+archive/ubuntu/updates\n",
            "Press [ENTER] to continue or Ctrl-c to cancel adding it.\n",
            "\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Get:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [819 B]\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [65.9 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [103 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [12.3 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:15 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:16 http://ppa.launchpad.net/ubuntu-x-swat/updates/ubuntu bionic InRelease [20.7 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [736 kB]\n",
            "Get:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [29.0 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [612 kB]\n",
            "Get:21 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,677 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [4,173 B]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [6,222 B]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [16.8 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [7,216 B]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,263 kB]\n",
            "Get:27 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [805 kB]\n",
            "Get:28 http://ppa.launchpad.net/ubuntu-x-swat/updates/ubuntu bionic/main amd64 Packages [815 B]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [921 kB]\n",
            "Fetched 6,575 kB in 38s (174 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Calculating upgrade... Done\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libnvidia-cfg1-418 libnvidia-common-410 linux-headers-4.15.0-55\n",
            "  linux-headers-4.15.0-55-generic\n",
            "Use 'apt autoremove' to remove them.\n",
            "The following NEW packages will be installed:\n",
            "  libnvidia-cfg1-430 libnvidia-common-430 libnvidia-compute-430\n",
            "  libnvidia-decode-430 libnvidia-encode-430 libnvidia-fbc1-430\n",
            "  libnvidia-gl-430 libnvidia-ifr1-430 linux-headers-4.15.0-58\n",
            "  linux-headers-4.15.0-58-generic nvidia-compute-utils-430 nvidia-dkms-430\n",
            "  nvidia-driver-430 nvidia-kernel-common-430 nvidia-kernel-source-430\n",
            "  nvidia-prime nvidia-utils-430 xserver-xorg-video-nvidia-430\n",
            "The following packages have been kept back:\n",
            "  libcudnn7 libcudnn7-dev libnccl-dev libnccl2\n",
            "The following packages will be upgraded:\n",
            "  base-files cmake cmake-data cuda-drivers dkms libegl-mesa0 libegl1-mesa-dev\n",
            "  libgbm1 libgl1-mesa-dev libgl1-mesa-dri libglapi-mesa libgles2-mesa-dev\n",
            "  libglx-mesa0 libinput-bin libinput10 libldap-2.4-2 libldap-common\n",
            "  libnvidia-cfg1-418 libnvidia-common-418 libnvidia-compute-418\n",
            "  libnvidia-decode-418 libnvidia-encode-418 libnvidia-fbc1-418\n",
            "  libnvidia-gl-418 libnvidia-ifr1-418 libpam-systemd libpoppler73 libpq5\n",
            "  libsystemd0 libudev1 libxnvctrl0 linux-headers-generic linux-libc-dev\n",
            "  mesa-common-dev nvidia-compute-utils-418 nvidia-dkms-418 nvidia-driver-418\n",
            "  nvidia-kernel-common-418 nvidia-kernel-source-418 nvidia-modprobe\n",
            "  nvidia-settings nvidia-utils-418 openjdk-11-jre openjdk-11-jre-headless\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless python3-software-properties\n",
            "  r-cran-foreign r-cran-ggplot2 r-cran-httr r-cran-knitr r-cran-markdown\n",
            "  r-cran-modelr r-cran-nlme r-cran-pkgbuild r-cran-tinytex r-cran-xml2\n",
            "  software-properties-common systemd systemd-sysv udev\n",
            "  xserver-xorg-video-nvidia-418\n",
            "62 upgraded, 18 newly installed, 0 to remove and 4 not upgraded.\n",
            "Need to get 183 MB of archives.\n",
            "After this operation, 95.1 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  nvidia-modprobe 418.87.00-0ubuntu1 [19.2 kB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libxnvctrl0 418.87.00-0ubuntu1 [19.4 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  nvidia-settings 418.87.00-0ubuntu1 [966 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-drivers 418.87.00-1 [2,588 B]\n",
            "Get:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 libnvidia-common-430 all 430.40-0ubuntu0~gpu18.04.1 [9,396 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 base-files amd64 10.1ubuntu2.6 [60.1 kB]\n",
            "Get:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 libnvidia-common-418 all 430.40-0ubuntu0~gpu18.04.1 [6,440 B]\n",
            "Get:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 nvidia-driver-418 amd64 430.40-0ubuntu0~gpu18.04.1 [6,432 B]\n",
            "Get:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 libnvidia-gl-418 amd64 430.40-0ubuntu0~gpu18.04.1 [6,436 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 systemd-sysv amd64 237-3ubuntu10.25 [11.4 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpam-systemd amd64 237-3ubuntu10.25 [108 kB]\n",
            "Get:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 libnvidia-decode-418 amd64 430.40-0ubuntu0~gpu18.04.1 [6,448 B]\n",
            "Get:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 libnvidia-compute-418 amd64 430.40-0ubuntu0~gpu18.04.1 [6,444 B]\n",
            "Get:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 libnvidia-compute-430 amd64 430.40-0ubuntu0~gpu18.04.1 [20.2 MB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsystemd0 amd64 237-3ubuntu10.25 [204 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 systemd amd64 237-3ubuntu10.25 [2,903 kB]\n",
            "Get:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 libnvidia-gl-430 amd64 430.40-0ubuntu0~gpu18.04.1 [32.8 MB]\n",
            "Get:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 nvidia-dkms-418 amd64 430.40-0ubuntu0~gpu18.04.1 [6,424 B]\n",
            "Get:19 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 nvidia-kernel-source-418 amd64 430.40-0ubuntu0~gpu18.04.1 [6,448 B]\n",
            "Get:20 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 nvidia-kernel-source-430 amd64 430.40-0ubuntu0~gpu18.04.1 [11.9 MB]\n",
            "Get:21 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 nvidia-kernel-common-418 amd64 430.40-0ubuntu0~gpu18.04.1 [6,452 B]\n",
            "Get:22 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 nvidia-kernel-common-430 amd64 430.40-0ubuntu0~gpu18.04.1 [9,396 B]\n",
            "Get:23 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 nvidia-dkms-430 amd64 430.40-0ubuntu0~gpu18.04.1 [25.4 kB]\n",
            "Get:24 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 nvidia-compute-utils-418 amd64 430.40-0ubuntu0~gpu18.04.1 [6,456 B]\n",
            "Get:25 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 nvidia-compute-utils-430 amd64 430.40-0ubuntu0~gpu18.04.1 [72.4 kB]\n",
            "Get:26 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 libnvidia-decode-430 amd64 430.40-0ubuntu0~gpu18.04.1 [1,031 kB]\n",
            "Get:27 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 libnvidia-encode-418 amd64 430.40-0ubuntu0~gpu18.04.1 [6,444 B]\n",
            "Get:28 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 libnvidia-encode-430 amd64 430.40-0ubuntu0~gpu18.04.1 [50.0 kB]\n",
            "Get:29 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 nvidia-utils-418 amd64 430.40-0ubuntu0~gpu18.04.1 [6,436 B]\n",
            "Get:30 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 nvidia-utils-430 amd64 430.40-0ubuntu0~gpu18.04.1 [337 kB]\n",
            "Get:31 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 xserver-xorg-video-nvidia-418 amd64 430.40-0ubuntu0~gpu18.04.1 [6,460 B]\n",
            "Get:32 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 libnvidia-cfg1-418 amd64 430.40-0ubuntu0~gpu18.04.1 [6,440 B]\n",
            "Get:33 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 libnvidia-cfg1-430 amd64 430.40-0ubuntu0~gpu18.04.1 [70.2 kB]\n",
            "Get:34 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 xserver-xorg-video-nvidia-430 amd64 430.40-0ubuntu0~gpu18.04.1 [1,481 kB]\n",
            "Get:35 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 libnvidia-ifr1-418 amd64 430.40-0ubuntu0~gpu18.04.1 [6,444 B]\n",
            "Get:36 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 libnvidia-ifr1-430 amd64 430.40-0ubuntu0~gpu18.04.1 [68.2 kB]\n",
            "Get:37 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 libnvidia-fbc1-418 amd64 430.40-0ubuntu0~gpu18.04.1 [6,444 B]\n",
            "Get:38 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 libnvidia-fbc1-430 amd64 430.40-0ubuntu0~gpu18.04.1 [43.7 kB]\n",
            "Get:39 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 nvidia-driver-430 amd64 430.40-0ubuntu0~gpu18.04.1 [403 kB]\n",
            "Get:40 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-foreign amd64 0.8.72-1ppabionic0 [233 kB]\n",
            "Get:41 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-ggplot2 all 3.2.1-1cran1ppabionic0 [3,773 kB]\n",
            "Get:42 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-httr all 1.4.1-1cran1ppabionic0 [470 kB]\n",
            "Get:43 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-markdown amd64 1.1-1cran1ppabionic0 [117 kB]\n",
            "Get:44 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-knitr all 1.24-1cran1ppabionic0 [1,161 kB]\n",
            "Get:45 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-modelr all 0.1.5-1cran1ppabionic0 [200 kB]\n",
            "Get:46 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-nlme amd64 3.1.141-1ppabionic0 [2,238 kB]\n",
            "Get:47 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-pkgbuild all 1.0.4-1cran1ppabionic0 [130 kB]\n",
            "Get:48 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-tinytex all 0.15-1cran1ppabionic0 [97.3 kB]\n",
            "Get:49 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 r-cran-xml2 amd64 1.2.2-1cran1ppabionic0 [293 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 udev amd64 237-3ubuntu10.25 [1,101 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libudev1 amd64 237-3ubuntu10.25 [53.5 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 cmake amd64 3.10.2-1ubuntu2.18.04.1 [3,152 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 cmake-data all 3.10.2-1ubuntu2.18.04.1 [1,332 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 dkms all 2.3-3ubuntu9.5 [68.1 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libegl-mesa0 amd64 19.0.8-0ubuntu0~18.04.1 [99.4 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgbm1 amd64 19.0.8-0ubuntu0~18.04.1 [27.9 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgl1-mesa-dri amd64 19.0.8-0ubuntu0~18.04.1 [7,499 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libglx-mesa0 amd64 19.0.8-0ubuntu0~18.04.1 [141 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libglapi-mesa amd64 19.0.8-0ubuntu0~18.04.1 [25.1 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgl1-mesa-dev amd64 19.0.8-0ubuntu0~18.04.1 [6,044 B]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libegl1-mesa-dev amd64 19.0.8-0ubuntu0~18.04.1 [19.7 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 mesa-common-dev amd64 19.0.8-0ubuntu0~18.04.1 [602 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgles2-mesa-dev amd64 19.0.8-0ubuntu0~18.04.1 [43.4 kB]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libinput-bin amd64 1.10.4-1ubuntu0.18.04.1 [11.1 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libinput10 amd64 1.10.4-1ubuntu0.18.04.1 [86.1 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libldap-common all 2.4.45+dfsg-1ubuntu1.3 [16.9 kB]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libldap-2.4-2 amd64 2.4.45+dfsg-1ubuntu1.3 [155 kB]\n",
            "Get:68 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpoppler73 amd64 0.62.0-2ubuntu2.10 [800 kB]\n",
            "Get:69 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpq5 amd64 10.10-0ubuntu0.18.04.1 [108 kB]\n",
            "Get:70 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 linux-headers-4.15.0-58 all 4.15.0-58.64 [11.0 MB]\n",
            "Get:71 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 linux-headers-4.15.0-58-generic amd64 4.15.0-58.64 [1,148 kB]\n",
            "Get:72 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 linux-headers-generic amd64 4.15.0.58.60 [2,444 B]\n",
            "Get:73 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 linux-libc-dev amd64 4.15.0-58.64 [1,043 kB]\n",
            "Get:74 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 nvidia-prime all 0.8.8.2 [8,412 B]\n",
            "Get:75 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 openjdk-11-jre amd64 11.0.4+11-1ubuntu2~18.04.3 [34.4 kB]\n",
            "Get:76 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 openjdk-11-jre-headless amd64 11.0.4+11-1ubuntu2~18.04.3 [37.4 MB]\n",
            "Get:77 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk-headless amd64 8u222-b10-1ubuntu1~18.04.1 [8,267 kB]\n",
            "Get:78 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre-headless amd64 8u222-b10-1ubuntu1~18.04.1 [27.4 MB]\n",
            "Get:79 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 software-properties-common all 0.96.24.32.11 [9,996 B]\n",
            "Get:80 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-software-properties all 0.96.24.32.11 [23.6 kB]\n",
            "Fetched 183 MB in 18min 52s (162 kB/s)\n",
            "Extracting templates from packages: 100%\n",
            "(Reading database ... 131289 files and directories currently installed.)\n",
            "Preparing to unpack .../base-files_10.1ubuntu2.6_amd64.deb ...\n",
            "Unpacking base-files (10.1ubuntu2.6) over (10.1ubuntu2.5) ...\n",
            "Setting up base-files (10.1ubuntu2.6) ...\n",
            "Installing new version of config file /etc/issue ...\n",
            "Installing new version of config file /etc/issue.net ...\n",
            "Installing new version of config file /etc/lsb-release ...\n",
            "(Reading database ... 131289 files and directories currently installed.)\n",
            "Preparing to unpack .../systemd-sysv_237-3ubuntu10.25_amd64.deb ...\n",
            "Unpacking systemd-sysv (237-3ubuntu10.25) over (237-3ubuntu10.24) ...\n",
            "Preparing to unpack .../libpam-systemd_237-3ubuntu10.25_amd64.deb ...\n",
            "Unpacking libpam-systemd:amd64 (237-3ubuntu10.25) over (237-3ubuntu10.24) ...\n",
            "Preparing to unpack .../libsystemd0_237-3ubuntu10.25_amd64.deb ...\n",
            "Unpacking libsystemd0:amd64 (237-3ubuntu10.25) over (237-3ubuntu10.24) ...\n",
            "Setting up libsystemd0:amd64 (237-3ubuntu10.25) ...\n",
            "(Reading database ... 131289 files and directories currently installed.)\n",
            "Preparing to unpack .../systemd_237-3ubuntu10.25_amd64.deb ...\n",
            "Unpacking systemd (237-3ubuntu10.25) over (237-3ubuntu10.24) ...\n",
            "Preparing to unpack .../udev_237-3ubuntu10.25_amd64.deb ...\n",
            "Unpacking udev (237-3ubuntu10.25) over (237-3ubuntu10.24) ...\n",
            "Preparing to unpack .../libudev1_237-3ubuntu10.25_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (237-3ubuntu10.25) over (237-3ubuntu10.24) ...\n",
            "Setting up libudev1:amd64 (237-3ubuntu10.25) ...\n",
            "(Reading database ... 131289 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cmake_3.10.2-1ubuntu2.18.04.1_amd64.deb ...\n",
            "Unpacking cmake (3.10.2-1ubuntu2.18.04.1) over (3.10.2-1ubuntu2) ...\n",
            "Preparing to unpack .../01-cmake-data_3.10.2-1ubuntu2.18.04.1_all.deb ...\n",
            "Unpacking cmake-data (3.10.2-1ubuntu2.18.04.1) over (3.10.2-1ubuntu2) ...\n",
            "Selecting previously unselected package libnvidia-common-430.\n",
            "Preparing to unpack .../02-libnvidia-common-430_430.40-0ubuntu0~gpu18.04.1_all.deb ...\n",
            "Unpacking libnvidia-common-430 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Preparing to unpack .../03-libnvidia-common-418_430.40-0ubuntu0~gpu18.04.1_all.deb ...\n",
            "Unpacking libnvidia-common-418 (430.40-0ubuntu0~gpu18.04.1) over (418.67-0ubuntu1) ...\n",
            "Preparing to unpack .../04-nvidia-driver-418_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking nvidia-driver-418 (430.40-0ubuntu0~gpu18.04.1) over (418.67-0ubuntu1) ...\n",
            "Preparing to unpack .../05-libnvidia-gl-418_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-gl-418:amd64 (430.40-0ubuntu0~gpu18.04.1) over (418.67-0ubuntu1) ...\n",
            "Preparing to unpack .../06-libnvidia-decode-418_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-decode-418:amd64 (430.40-0ubuntu0~gpu18.04.1) over (418.67-0ubuntu1) ...\n",
            "Preparing to unpack .../07-libnvidia-compute-418_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-compute-418:amd64 (430.40-0ubuntu0~gpu18.04.1) over (418.67-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-compute-430:amd64.\n",
            "Preparing to unpack .../08-libnvidia-compute-430_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-compute-430:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Selecting previously unselected package libnvidia-gl-430:amd64.\n",
            "Preparing to unpack .../09-libnvidia-gl-430_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-gl-430:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Preparing to unpack .../10-nvidia-dkms-418_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Removing all DKMS Modules\n",
            "Done.\n",
            "Unpacking nvidia-dkms-418 (430.40-0ubuntu0~gpu18.04.1) over (418.67-0ubuntu1) ...\n",
            "Preparing to unpack .../11-dkms_2.3-3ubuntu9.5_all.deb ...\n",
            "Unpacking dkms (2.3-3ubuntu9.5) over (2.3-3ubuntu9.4) ...\n",
            "Preparing to unpack .../12-nvidia-kernel-source-418_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking nvidia-kernel-source-418 (430.40-0ubuntu0~gpu18.04.1) over (418.67-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-kernel-source-430.\n",
            "Preparing to unpack .../13-nvidia-kernel-source-430_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking nvidia-kernel-source-430 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Preparing to unpack .../14-nvidia-kernel-common-418_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking nvidia-kernel-common-418:amd64 (430.40-0ubuntu0~gpu18.04.1) over (418.67-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-kernel-common-430.\n",
            "Preparing to unpack .../15-nvidia-kernel-common-430_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking nvidia-kernel-common-430 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Selecting previously unselected package nvidia-dkms-430.\n",
            "Preparing to unpack .../16-nvidia-dkms-430_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking nvidia-dkms-430 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Preparing to unpack .../17-nvidia-compute-utils-418_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "System has not been booted with systemd as init system (PID 1). Can't operate.\n",
            "Unpacking nvidia-compute-utils-418:amd64 (430.40-0ubuntu0~gpu18.04.1) over (418.67-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-compute-utils-430.\n",
            "Preparing to unpack .../18-nvidia-compute-utils-430_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking nvidia-compute-utils-430 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Selecting previously unselected package libnvidia-decode-430:amd64.\n",
            "Preparing to unpack .../19-libnvidia-decode-430_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-decode-430:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Preparing to unpack .../20-libnvidia-encode-418_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-encode-418:amd64 (430.40-0ubuntu0~gpu18.04.1) over (418.67-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-encode-430:amd64.\n",
            "Preparing to unpack .../21-libnvidia-encode-430_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-encode-430:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Preparing to unpack .../22-nvidia-utils-418_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking nvidia-utils-418:amd64 (430.40-0ubuntu0~gpu18.04.1) over (418.67-0ubuntu1) ...\n",
            "Selecting previously unselected package nvidia-utils-430.\n",
            "Preparing to unpack .../23-nvidia-utils-430_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking nvidia-utils-430 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Preparing to unpack .../24-xserver-xorg-video-nvidia-418_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking xserver-xorg-video-nvidia-418:amd64 (430.40-0ubuntu0~gpu18.04.1) over (418.67-0ubuntu1) ...\n",
            "Preparing to unpack .../25-libnvidia-cfg1-418_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-cfg1-418:amd64 (430.40-0ubuntu0~gpu18.04.1) over (418.67-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-cfg1-430:amd64.\n",
            "Preparing to unpack .../26-libnvidia-cfg1-430_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-cfg1-430:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Selecting previously unselected package xserver-xorg-video-nvidia-430.\n",
            "Preparing to unpack .../27-xserver-xorg-video-nvidia-430_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking xserver-xorg-video-nvidia-430 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Preparing to unpack .../28-libnvidia-ifr1-418_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-ifr1-418:amd64 (430.40-0ubuntu0~gpu18.04.1) over (418.67-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-ifr1-430:amd64.\n",
            "Preparing to unpack .../29-libnvidia-ifr1-430_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-ifr1-430:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Preparing to unpack .../30-libnvidia-fbc1-418_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-fbc1-418:amd64 (430.40-0ubuntu0~gpu18.04.1) over (418.67-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-fbc1-430:amd64.\n",
            "Preparing to unpack .../31-libnvidia-fbc1-430_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking libnvidia-fbc1-430:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Selecting previously unselected package nvidia-driver-430.\n",
            "Preparing to unpack .../32-nvidia-driver-430_430.40-0ubuntu0~gpu18.04.1_amd64.deb ...\n",
            "Unpacking nvidia-driver-430 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Preparing to unpack .../33-nvidia-modprobe_418.87.00-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-modprobe (418.87.00-0ubuntu1) over (418.67-0ubuntu1) ...\n",
            "Preparing to unpack .../34-libxnvctrl0_418.87.00-0ubuntu1_amd64.deb ...\n",
            "Unpacking libxnvctrl0:amd64 (418.87.00-0ubuntu1) over (418.67-0ubuntu1) ...\n",
            "Preparing to unpack .../35-nvidia-settings_418.87.00-0ubuntu1_amd64.deb ...\n",
            "Unpacking nvidia-settings (418.87.00-0ubuntu1) over (418.67-0ubuntu1) ...\n",
            "Preparing to unpack .../36-cuda-drivers_418.87.00-1_amd64.deb ...\n",
            "Unpacking cuda-drivers (418.87.00-1) over (418.67-1) ...\n",
            "Preparing to unpack .../37-libegl-mesa0_19.0.8-0ubuntu0~18.04.1_amd64.deb ...\n",
            "Unpacking libegl-mesa0:amd64 (19.0.8-0ubuntu0~18.04.1) over (19.0.2-1ubuntu1.1~18.04.2) ...\n",
            "Preparing to unpack .../38-libgbm1_19.0.8-0ubuntu0~18.04.1_amd64.deb ...\n",
            "Unpacking libgbm1:amd64 (19.0.8-0ubuntu0~18.04.1) over (19.0.2-1ubuntu1.1~18.04.2) ...\n",
            "Preparing to unpack .../39-libgl1-mesa-dri_19.0.8-0ubuntu0~18.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-dri:amd64 (19.0.8-0ubuntu0~18.04.1) over (19.0.2-1ubuntu1.1~18.04.2) ...\n",
            "Preparing to unpack .../40-libglx-mesa0_19.0.8-0ubuntu0~18.04.1_amd64.deb ...\n",
            "Unpacking libglx-mesa0:amd64 (19.0.8-0ubuntu0~18.04.1) over (19.0.2-1ubuntu1.1~18.04.2) ...\n",
            "Preparing to unpack .../41-libglapi-mesa_19.0.8-0ubuntu0~18.04.1_amd64.deb ...\n",
            "Unpacking libglapi-mesa:amd64 (19.0.8-0ubuntu0~18.04.1) over (19.0.2-1ubuntu1.1~18.04.2) ...\n",
            "Preparing to unpack .../42-libgl1-mesa-dev_19.0.8-0ubuntu0~18.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-dev:amd64 (19.0.8-0ubuntu0~18.04.1) over (19.0.2-1ubuntu1.1~18.04.2) ...\n",
            "Preparing to unpack .../43-libegl1-mesa-dev_19.0.8-0ubuntu0~18.04.1_amd64.deb ...\n",
            "Unpacking libegl1-mesa-dev:amd64 (19.0.8-0ubuntu0~18.04.1) over (19.0.2-1ubuntu1.1~18.04.2) ...\n",
            "Preparing to unpack .../44-mesa-common-dev_19.0.8-0ubuntu0~18.04.1_amd64.deb ...\n",
            "Unpacking mesa-common-dev:amd64 (19.0.8-0ubuntu0~18.04.1) over (19.0.2-1ubuntu1.1~18.04.2) ...\n",
            "Preparing to unpack .../45-libgles2-mesa-dev_19.0.8-0ubuntu0~18.04.1_amd64.deb ...\n",
            "Unpacking libgles2-mesa-dev:amd64 (19.0.8-0ubuntu0~18.04.1) over (19.0.2-1ubuntu1.1~18.04.2) ...\n",
            "Preparing to unpack .../46-libinput-bin_1.10.4-1ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libinput-bin (1.10.4-1ubuntu0.18.04.1) over (1.10.4-1) ...\n",
            "Preparing to unpack .../47-libinput10_1.10.4-1ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libinput10:amd64 (1.10.4-1ubuntu0.18.04.1) over (1.10.4-1) ...\n",
            "Preparing to unpack .../48-libldap-common_2.4.45+dfsg-1ubuntu1.3_all.deb ...\n",
            "Unpacking libldap-common (2.4.45+dfsg-1ubuntu1.3) over (2.4.45+dfsg-1ubuntu1.2) ...\n",
            "Preparing to unpack .../49-libldap-2.4-2_2.4.45+dfsg-1ubuntu1.3_amd64.deb ...\n",
            "Unpacking libldap-2.4-2:amd64 (2.4.45+dfsg-1ubuntu1.3) over (2.4.45+dfsg-1ubuntu1.2) ...\n",
            "Preparing to unpack .../50-libpoppler73_0.62.0-2ubuntu2.10_amd64.deb ...\n",
            "Unpacking libpoppler73:amd64 (0.62.0-2ubuntu2.10) over (0.62.0-2ubuntu2.9) ...\n",
            "Preparing to unpack .../51-libpq5_10.10-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libpq5:amd64 (10.10-0ubuntu0.18.04.1) over (10.9-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package linux-headers-4.15.0-58.\n",
            "Preparing to unpack .../52-linux-headers-4.15.0-58_4.15.0-58.64_all.deb ...\n",
            "Unpacking linux-headers-4.15.0-58 (4.15.0-58.64) ...\n",
            "Selecting previously unselected package linux-headers-4.15.0-58-generic.\n",
            "Preparing to unpack .../53-linux-headers-4.15.0-58-generic_4.15.0-58.64_amd64.deb ...\n",
            "Unpacking linux-headers-4.15.0-58-generic (4.15.0-58.64) ...\n",
            "Preparing to unpack .../54-linux-headers-generic_4.15.0.58.60_amd64.deb ...\n",
            "Unpacking linux-headers-generic (4.15.0.58.60) over (4.15.0.55.57) ...\n",
            "Preparing to unpack .../55-linux-libc-dev_4.15.0-58.64_amd64.deb ...\n",
            "Unpacking linux-libc-dev:amd64 (4.15.0-58.64) over (4.15.0-55.60) ...\n",
            "Selecting previously unselected package nvidia-prime.\n",
            "Preparing to unpack .../56-nvidia-prime_0.8.8.2_all.deb ...\n",
            "Unpacking nvidia-prime (0.8.8.2) ...\n",
            "Preparing to unpack .../57-openjdk-11-jre_11.0.4+11-1ubuntu2~18.04.3_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.4+11-1ubuntu2~18.04.3) over (11.0.3+7-1ubuntu2~18.04.1) ...\n",
            "Preparing to unpack .../58-openjdk-11-jre-headless_11.0.4+11-1ubuntu2~18.04.3_amd64.deb ...\n",
            "Unpacking openjdk-11-jre-headless:amd64 (11.0.4+11-1ubuntu2~18.04.3) over (11.0.3+7-1ubuntu2~18.04.1) ...\n",
            "Preparing to unpack .../59-openjdk-8-jdk-headless_8u222-b10-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u222-b10-1ubuntu1~18.04.1) over (8u212-b03-0ubuntu1.18.04.1) ...\n",
            "Preparing to unpack .../60-openjdk-8-jre-headless_8u222-b10-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u222-b10-1ubuntu1~18.04.1) over (8u212-b03-0ubuntu1.18.04.1) ...\n",
            "Preparing to unpack .../61-software-properties-common_0.96.24.32.11_all.deb ...\n",
            "Unpacking software-properties-common (0.96.24.32.11) over (0.96.24.32.9) ...\n",
            "Preparing to unpack .../62-python3-software-properties_0.96.24.32.11_all.deb ...\n",
            "Unpacking python3-software-properties (0.96.24.32.11) over (0.96.24.32.9) ...\n",
            "Preparing to unpack .../63-r-cran-foreign_0.8.72-1ppabionic0_amd64.deb ...\n",
            "Unpacking r-cran-foreign (0.8.72-1ppabionic0) over (0.8.71-3cranppa0bionic0) ...\n",
            "Preparing to unpack .../64-r-cran-ggplot2_3.2.1-1cran1ppabionic0_all.deb ...\n",
            "Unpacking r-cran-ggplot2 (3.2.1-1cran1ppabionic0) over (3.2.0-1cran1ppabionic0) ...\n",
            "Preparing to unpack .../65-r-cran-httr_1.4.1-1cran1ppabionic0_all.deb ...\n",
            "Unpacking r-cran-httr (1.4.1-1cran1ppabionic0) over (1.4.0-1cran1ppabionic0) ...\n",
            "Preparing to unpack .../66-r-cran-markdown_1.1-1cran1ppabionic0_amd64.deb ...\n",
            "Unpacking r-cran-markdown (1.1-1cran1ppabionic0) over (1.0-1cran1~ubuntu18.04.1~ppa1) ...\n",
            "Preparing to unpack .../67-r-cran-knitr_1.24-1cran1ppabionic0_all.deb ...\n",
            "Unpacking r-cran-knitr (1.24-1cran1ppabionic0) over (1.23-1cran1ppabionic0) ...\n",
            "Preparing to unpack .../68-r-cran-modelr_0.1.5-1cran1ppabionic0_all.deb ...\n",
            "Unpacking r-cran-modelr (0.1.5-1cran1ppabionic0) over (0.1.4-1cran1ppabionic0) ...\n",
            "Preparing to unpack .../69-r-cran-nlme_3.1.141-1ppabionic0_amd64.deb ...\n",
            "Unpacking r-cran-nlme (3.1.141-1ppabionic0) over (3.1.140-1ppabionic0) ...\n",
            "Preparing to unpack .../70-r-cran-pkgbuild_1.0.4-1cran1ppabionic0_all.deb ...\n",
            "Unpacking r-cran-pkgbuild (1.0.4-1cran1ppabionic0) over (1.0.3-1cran1ppabionic0) ...\n",
            "Preparing to unpack .../71-r-cran-tinytex_0.15-1cran1ppabionic0_all.deb ...\n",
            "Unpacking r-cran-tinytex (0.15-1cran1ppabionic0) over (0.14-1cran1ppabionic0) ...\n",
            "Preparing to unpack .../72-r-cran-xml2_1.2.2-1cran1ppabionic0_amd64.deb ...\n",
            "Unpacking r-cran-xml2 (1.2.2-1cran1ppabionic0) over (1.2.1-1cran1ppabionic0) ...\n",
            "Setting up r-cran-tinytex (0.15-1cran1ppabionic0) ...\n",
            "Setting up nvidia-prime (0.8.8.2) ...\n",
            "Setting up mesa-common-dev:amd64 (19.0.8-0ubuntu0~18.04.1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Setting up libldap-common (2.4.45+dfsg-1ubuntu1.3) ...\n",
            "Setting up nvidia-kernel-common-430 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up linux-headers-4.15.0-58 (4.15.0-58.64) ...\n",
            "Setting up r-cran-nlme (3.1.141-1ppabionic0) ...\n",
            "Setting up libgbm1:amd64 (19.0.8-0ubuntu0~18.04.1) ...\n",
            "Setting up r-cran-foreign (0.8.72-1ppabionic0) ...\n",
            "Setting up linux-libc-dev:amd64 (4.15.0-58.64) ...\n",
            "Setting up openjdk-11-jre-headless:amd64 (11.0.4+11-1ubuntu2~18.04.3) ...\n",
            "Installing new version of config file /etc/java-11-openjdk/security/default.policy ...\n",
            "Installing new version of config file /etc/java-11-openjdk/security/public_suffix_list.dat ...\n",
            "Setting up nvidia-kernel-common-418:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up libnvidia-common-430 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up cmake-data (3.10.2-1ubuntu2.18.04.1) ...\n",
            "Setting up libglapi-mesa:amd64 (19.0.8-0ubuntu0~18.04.1) ...\n",
            "Setting up libinput-bin (1.10.4-1ubuntu0.18.04.1) ...\n",
            "Setting up r-cran-markdown (1.1-1cran1ppabionic0) ...\n",
            "Setting up r-cran-modelr (0.1.5-1cran1ppabionic0) ...\n",
            "Setting up libpoppler73:amd64 (0.62.0-2ubuntu2.10) ...\n",
            "Setting up libnvidia-cfg1-430:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up libgl1-mesa-dev:amd64 (19.0.8-0ubuntu0~18.04.1) ...\n",
            "Setting up dkms (2.3-3ubuntu9.5) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up libnvidia-compute-430:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up udev (237-3ubuntu10.25) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of restart.\n",
            "Setting up libldap-2.4-2:amd64 (2.4.45+dfsg-1ubuntu1.3) ...\n",
            "Setting up nvidia-kernel-source-430 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up libnvidia-cfg1-418:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up systemd (237-3ubuntu10.25) ...\n",
            "Setting up libegl1-mesa-dev:amd64 (19.0.8-0ubuntu0~18.04.1) ...\n",
            "Setting up nvidia-modprobe (418.87.00-0ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up python3-software-properties (0.96.24.32.11) ...\n",
            "Setting up r-cran-ggplot2 (3.2.1-1cran1ppabionic0) ...\n",
            "Setting up libegl-mesa0:amd64 (19.0.8-0ubuntu0~18.04.1) ...\n",
            "Setting up linux-headers-4.15.0-58-generic (4.15.0-58.64) ...\n",
            "/etc/kernel/header_postinst.d/dkms:\n",
            " * dkms: running auto installation service for kernel 4.15.0-58-generic\n",
            "   ...done.\n",
            "Processing triggers for dbus (1.12.2-1ubuntu1.1) ...\n",
            "Setting up r-cran-xml2 (1.2.2-1cran1ppabionic0) ...\n",
            "Setting up cmake (3.10.2-1ubuntu2.18.04.1) ...\n",
            "Setting up r-cran-pkgbuild (1.0.4-1cran1ppabionic0) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Setting up libnvidia-fbc1-430:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up libxnvctrl0:amd64 (418.87.00-0ubuntu1) ...\n",
            "Setting up r-cran-httr (1.4.1-1cran1ppabionic0) ...\n",
            "Setting up nvidia-dkms-430 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Loading new nvidia-430.40 DKMS files...\n",
            "It is likely that 4.14.79+ belongs to a chroot's host\n",
            "Building for 4.15.0-55-generic and 4.15.0-58-generic\n",
            "Building for architecture x86_64\n",
            "Building initial module for 4.15.0-55-generic\n",
            "\n",
            "Done.\n",
            "\n",
            "nvidia:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/4.15.0-55-generic/updates/dkms/\n",
            "\n",
            "nvidia-modeset.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/4.15.0-55-generic/updates/dkms/\n",
            "\n",
            "nvidia-drm.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/4.15.0-55-generic/updates/dkms/\n",
            "\n",
            "nvidia-uvm.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/4.15.0-55-generic/updates/dkms/\n",
            "\n",
            "depmod...\n",
            "\n",
            "DKMS: install completed.\n",
            "Building initial module for 4.15.0-58-generic\n",
            "Done.\n",
            "\n",
            "nvidia:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/4.15.0-58-generic/updates/dkms/\n",
            "\n",
            "nvidia-modeset.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/4.15.0-58-generic/updates/dkms/\n",
            "\n",
            "nvidia-drm.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/4.15.0-58-generic/updates/dkms/\n",
            "\n",
            "nvidia-uvm.ko:\n",
            "Running module version sanity check.\n",
            " - Original module\n",
            "   - No original module exists within this kernel\n",
            " - Installation\n",
            "   - Installing to /lib/modules/4.15.0-58-generic/updates/dkms/\n",
            "\n",
            "depmod...\n",
            "\n",
            "DKMS: install completed.\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u222-b10-1ubuntu1~18.04.1) ...\n",
            "Setting up linux-headers-generic (4.15.0.58.60) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.4+11-1ubuntu2~18.04.3) ...\n",
            "Setting up software-properties-common (0.96.24.32.11) ...\n",
            "Setting up libnvidia-fbc1-418:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up libnvidia-common-418 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up nvidia-kernel-source-418 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up xserver-xorg-video-nvidia-430 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up libnvidia-decode-430:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up libnvidia-gl-430:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u222-b10-1ubuntu1~18.04.1) ...\n",
            "Setting up nvidia-utils-430 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up libnvidia-compute-418:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up systemd-sysv (237-3ubuntu10.25) ...\n",
            "Setting up r-cran-knitr (1.24-1cran1ppabionic0) ...\n",
            "Setting up libgles2-mesa-dev:amd64 (19.0.8-0ubuntu0~18.04.1) ...\n",
            "Setting up nvidia-utils-418:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up libinput10:amd64 (1.10.4-1ubuntu0.18.04.1) ...\n",
            "Setting up libgl1-mesa-dri:amd64 (19.0.8-0ubuntu0~18.04.1) ...\n",
            "Setting up nvidia-settings (418.87.00-0ubuntu1) ...\n",
            "Setting up nvidia-compute-utils-430 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up libpq5:amd64 (10.10-0ubuntu0.18.04.1) ...\n",
            "Setting up libnvidia-gl-418:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up nvidia-compute-utils-418:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up libnvidia-encode-430:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up libnvidia-ifr1-430:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up nvidia-dkms-418 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up libnvidia-ifr1-418:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up libglx-mesa0:amd64 (19.0.8-0ubuntu0~18.04.1) ...\n",
            "Setting up libnvidia-encode-418:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up xserver-xorg-video-nvidia-418:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up libpam-systemd:amd64 (237-3ubuntu10.25) ...\n",
            "Setting up libnvidia-decode-418:amd64 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up nvidia-driver-430 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up nvidia-driver-418 (430.40-0ubuntu0~gpu18.04.1) ...\n",
            "Setting up cuda-drivers (418.87.00-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ3AOmfa1fdV",
        "colab_type": "text"
      },
      "source": [
        "### Instalar bibliotecas de sistema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs7V8b_k1fdX",
        "colab_type": "code",
        "outputId": "69f833e1-dbe9-44de-a8a2-6b5347dbd6ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt-get install git cmake build-essential libgl1-mesa-dev libsdl2-dev libsdl2-image-dev libsdl2-ttf-dev libsdl2-gfx-dev libboost-all-dev libdirectfb-dev libst-dev mesa-utils xvfb x11vnc libsqlite3-dev glee-dev libsdl-sge-dev python3-pip libosmesa6-dev libgl1-mesa-glx libglfw3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "libboost-all-dev is already the newest version (1.65.1.0ubuntu1).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.4).\n",
            "libgl1-mesa-dev is already the newest version (19.0.8-0ubuntu0~18.04.1).\n",
            "libgl1-mesa-dev set to manually installed.\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libnvidia-cfg1-418 libnvidia-common-410 linux-headers-4.15.0-55\n",
            "  linux-headers-4.15.0-55-generic\n",
            "Use 'apt autoremove' to remove them.\n",
            "The following additional packages will be installed:\n",
            "  gir1.2-ibus-1.0 lib++dfb-1.7-7 liba52-0.7.4 libcaca-dev libcapnp-0.6.1\n",
            "  libdbus-1-dev libdirectfb-1.7-7 libglee0d1 libibus-1.0-5 libibus-1.0-dev\n",
            "  libmirclient-dev libmirclient9 libmircommon-dev libmircommon7\n",
            "  libmircookie-dev libmircookie2 libmircore-dev libmircore1 libmirprotobuf3\n",
            "  libmpeg3-2 libmpeg3-dev libosmesa6 libprotobuf-dev libprotobuf-lite10\n",
            "  libpulse-dev libpulse-mainloop-glib0 libsdl-image1.2 libsdl-sge\n",
            "  libsdl1.2-dev libsdl1.2debian libsdl2-gfx-1.0-0 libsdl2-image-2.0-0\n",
            "  libsdl2-ttf-2.0-0 libslang2-dev libsndio-dev libst1 libudev-dev\n",
            "  libvncclient1 libvncserver1 libxcursor-dev libxinerama-dev libxkbcommon-dev\n",
            "  libxrandr-dev libxv-dev python-pip-whl python3-asn1crypto\n",
            "  python3-cffi-backend python3-crypto python3-cryptography python3-idna\n",
            "  python3-keyring python3-keyrings.alt python3-pkg-resources\n",
            "  python3-secretstorage python3-setuptools python3-six python3-wheel\n",
            "  python3-xdg x11proto-randr-dev x11proto-xinerama-dev x11vnc-data\n",
            "Suggested packages:\n",
            "  libglee0d1-dbg libdirectfb-extra libegl1-mesa | libegl1-x11 libvulkan1\n",
            "  ttf-dejavu-core libsdl2-gfx-doc sqlite3-doc python-crypto-doc\n",
            "  python-cryptography-doc python3-cryptography-vectors gnome-keyring\n",
            "  libkf5wallet-bin gir1.2-gnomekeyring-1.0 python-secretstorage-doc\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  gir1.2-ibus-1.0 glee-dev lib++dfb-1.7-7 liba52-0.7.4 libcaca-dev\n",
            "  libcapnp-0.6.1 libdbus-1-dev libdirectfb-1.7-7 libdirectfb-dev\n",
            "  libgl1-mesa-glx libglee0d1 libglfw3 libibus-1.0-5 libibus-1.0-dev\n",
            "  libmirclient-dev libmirclient9 libmircommon-dev libmircommon7\n",
            "  libmircookie-dev libmircookie2 libmircore-dev libmircore1 libmirprotobuf3\n",
            "  libmpeg3-2 libmpeg3-dev libosmesa6 libosmesa6-dev libprotobuf-dev\n",
            "  libprotobuf-lite10 libpulse-dev libpulse-mainloop-glib0 libsdl-image1.2\n",
            "  libsdl-sge libsdl-sge-dev libsdl1.2-dev libsdl1.2debian libsdl2-dev\n",
            "  libsdl2-gfx-1.0-0 libsdl2-gfx-dev libsdl2-image-2.0-0 libsdl2-image-dev\n",
            "  libsdl2-ttf-2.0-0 libsdl2-ttf-dev libslang2-dev libsndio-dev libsqlite3-dev\n",
            "  libst-dev libst1 libudev-dev libvncclient1 libvncserver1 libxcursor-dev\n",
            "  libxinerama-dev libxkbcommon-dev libxrandr-dev libxv-dev mesa-utils\n",
            "  python-pip-whl python3-asn1crypto python3-cffi-backend python3-crypto\n",
            "  python3-cryptography python3-idna python3-keyring python3-keyrings.alt\n",
            "  python3-pip python3-pkg-resources python3-secretstorage python3-setuptools\n",
            "  python3-six python3-wheel python3-xdg x11proto-randr-dev\n",
            "  x11proto-xinerama-dev x11vnc x11vnc-data xvfb\n",
            "0 upgraded, 77 newly installed, 0 to remove and 4 not upgraded.\n",
            "Need to get 16.6 MB of archives.\n",
            "After this operation, 71.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgl1-mesa-glx amd64 19.0.8-0ubuntu0~18.04.1 [5,396 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libglee0d1 amd64 5.4.0-2 [162 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libibus-1.0-5 amd64 1.5.17-3ubuntu4 [134 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-ibus-1.0 amd64 1.5.17-3ubuntu4 [66.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 glee-dev amd64 5.4.0-2 [267 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libdirectfb-1.7-7 amd64 1.7.7-8 [953 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 lib++dfb-1.7-7 amd64 1.7.7-8 [29.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 liba52-0.7.4 amd64 0.7.4-19 [35.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libslang2-dev amd64 2.3.1a-3ubuntu1 [393 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcaca-dev amd64 0.99.beta19-2ubuntu0.18.04.1 [747 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcapnp-0.6.1 amd64 0.6.1-1ubuntu1 [658 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libdbus-1-dev amd64 1.12.2-1ubuntu1.1 [165 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmpeg3-2 amd64 1.8.dfsg-2.1 [87.6 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmpeg3-dev amd64 1.8.dfsg-2.1 [112 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libdirectfb-dev amd64 1.7.7-8 [199 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libglfw3 amd64 3.2.1-1 [49.2 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libibus-1.0-dev amd64 1.5.17-3ubuntu4 [145 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircore1 amd64 0.31.1-0ubuntu1 [26.5 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircommon7 amd64 0.31.1-0ubuntu1 [73.9 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libprotobuf-lite10 amd64 3.0.0-9.1ubuntu1 [97.7 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmirprotobuf3 amd64 0.31.1-0ubuntu1 [127 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmirclient9 amd64 0.31.1-0ubuntu1 [199 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircore-dev amd64 0.31.1-0ubuntu1 [21.7 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 libprotobuf-dev amd64 3.0.0-9.1ubuntu1 [959 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxkbcommon-dev amd64 0.8.0-1ubuntu0.1 [308 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircommon-dev amd64 0.31.1-0ubuntu1 [13.9 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircookie2 amd64 0.31.1-0ubuntu1 [19.7 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmircookie-dev amd64 0.31.1-0ubuntu1 [4,392 B]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmirclient-dev amd64 0.31.1-0ubuntu1 [47.8 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse-mainloop-glib0 amd64 1:11.1-1ubuntu7.2 [22.1 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse-dev amd64 1:11.1-1ubuntu7.2 [81.5 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsdl1.2debian amd64 1.2.15+dfsg2-0.1 [174 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsdl-image1.2 amd64 1.2.12-8 [29.5 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsdl-sge amd64 030809dfsg-7 [70.0 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsdl1.2-dev amd64 1.2.15+dfsg2-0.1 [706 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsdl-sge-dev amd64 030809dfsg-7 [135 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsndio-dev amd64 1.1.0-3 [13.3 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libudev-dev amd64 237-3ubuntu10.25 [19.1 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxcursor-dev amd64 1:1.1.15-1 [26.5 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-xinerama-dev all 2018.4-4 [2,628 B]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxinerama-dev amd64 2:1.1.3-1 [8,404 B]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-randr-dev all 2018.4-4 [2,620 B]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxrandr-dev amd64 2:1.5.1-1 [24.0 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxv-dev amd64 2:1.0.11-1 [32.5 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsdl2-dev amd64 2.0.8+dfsg1-1ubuntu1.18.04.3 [684 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsdl2-gfx-1.0-0 amd64 1.0.4+dfsg-1 [29.9 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsdl2-gfx-dev amd64 1.0.4+dfsg-1 [29.8 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsdl2-image-2.0-0 amd64 2.0.3+dfsg1-1 [63.0 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsdl2-image-dev amd64 2.0.3+dfsg1-1 [68.6 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsdl2-ttf-2.0-0 amd64 2.0.14+dfsg1-2 [14.8 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsdl2-ttf-dev amd64 2.0.14+dfsg1-2 [19.7 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsqlite3-dev amd64 3.22.0-1ubuntu0.1 [633 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libst1 amd64 1.9-3.1ubuntu1 [20.9 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libst-dev amd64 1.9-3.1ubuntu1 [63.4 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libvncclient1 amd64 0.9.11+dfsg-1ubuntu1.1 [54.8 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libvncserver1 amd64 0.9.11+dfsg-1ubuntu1.1 [122 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip-whl all 9.0.1-2.3~ubuntu1.18.04.1 [1,653 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-asn1crypto all 0.24.0-1 [72.8 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-cffi-backend amd64 1.11.5-1 [64.6 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-crypto amd64 2.6.1-8ubuntu2 [244 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-idna all 2.6-1 [32.5 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-six all 1.11.0-2 [11.4 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-cryptography amd64 2.1.4-1ubuntu1.3 [221 kB]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-secretstorage all 2.3.1-2 [12.1 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyring all 10.6.0-1 [26.7 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyrings.alt all 3.0-1 [16.6 kB]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3-pip all 9.0.1-2.3~ubuntu1.18.04.1 [114 kB]\n",
            "Get:68 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-pkg-resources all 39.0.1-2 [98.8 kB]\n",
            "Get:69 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-setuptools all 39.0.1-2 [248 kB]\n",
            "Get:70 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-wheel all 0.30.0-0.2 [36.5 kB]\n",
            "Get:71 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-xdg all 0.25-4ubuntu1 [31.4 kB]\n",
            "Get:72 http://archive.ubuntu.com/ubuntu bionic/universe amd64 x11vnc-data all 0.9.13-3 [214 kB]\n",
            "Get:73 http://archive.ubuntu.com/ubuntu bionic/universe amd64 x11vnc amd64 0.9.13-3 [967 kB]\n",
            "Get:74 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.3 [783 kB]\n",
            "Get:75 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libosmesa6 amd64 19.0.8-0ubuntu0~18.04.1 [2,491 kB]\n",
            "Get:76 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libosmesa6-dev amd64 19.0.8-0ubuntu0~18.04.1 [8,688 B]\n",
            "Get:77 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mesa-utils amd64 8.4.0-1 [34.3 kB]\n",
            "Fetched 16.6 MB in 6s (2,607 kB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "(Reading database ... 160623 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libgl1-mesa-glx_19.0.8-0ubuntu0~18.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (19.0.8-0ubuntu0~18.04.1) ...\n",
            "Selecting previously unselected package libglee0d1:amd64.\n",
            "Preparing to unpack .../01-libglee0d1_5.4.0-2_amd64.deb ...\n",
            "Unpacking libglee0d1:amd64 (5.4.0-2) ...\n",
            "Selecting previously unselected package libibus-1.0-5:amd64.\n",
            "Preparing to unpack .../02-libibus-1.0-5_1.5.17-3ubuntu4_amd64.deb ...\n",
            "Unpacking libibus-1.0-5:amd64 (1.5.17-3ubuntu4) ...\n",
            "Selecting previously unselected package gir1.2-ibus-1.0:amd64.\n",
            "Preparing to unpack .../03-gir1.2-ibus-1.0_1.5.17-3ubuntu4_amd64.deb ...\n",
            "Unpacking gir1.2-ibus-1.0:amd64 (1.5.17-3ubuntu4) ...\n",
            "Selecting previously unselected package glee-dev:amd64.\n",
            "Preparing to unpack .../04-glee-dev_5.4.0-2_amd64.deb ...\n",
            "Unpacking glee-dev:amd64 (5.4.0-2) ...\n",
            "Selecting previously unselected package libdirectfb-1.7-7:amd64.\n",
            "Preparing to unpack .../05-libdirectfb-1.7-7_1.7.7-8_amd64.deb ...\n",
            "Unpacking libdirectfb-1.7-7:amd64 (1.7.7-8) ...\n",
            "Selecting previously unselected package lib++dfb-1.7-7:amd64.\n",
            "Preparing to unpack .../06-lib++dfb-1.7-7_1.7.7-8_amd64.deb ...\n",
            "Unpacking lib++dfb-1.7-7:amd64 (1.7.7-8) ...\n",
            "Selecting previously unselected package liba52-0.7.4:amd64.\n",
            "Preparing to unpack .../07-liba52-0.7.4_0.7.4-19_amd64.deb ...\n",
            "Unpacking liba52-0.7.4:amd64 (0.7.4-19) ...\n",
            "Selecting previously unselected package libslang2-dev:amd64.\n",
            "Preparing to unpack .../08-libslang2-dev_2.3.1a-3ubuntu1_amd64.deb ...\n",
            "Unpacking libslang2-dev:amd64 (2.3.1a-3ubuntu1) ...\n",
            "Selecting previously unselected package libcaca-dev.\n",
            "Preparing to unpack .../09-libcaca-dev_0.99.beta19-2ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libcaca-dev (0.99.beta19-2ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libcapnp-0.6.1:amd64.\n",
            "Preparing to unpack .../10-libcapnp-0.6.1_0.6.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libcapnp-0.6.1:amd64 (0.6.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libdbus-1-dev:amd64.\n",
            "Preparing to unpack .../11-libdbus-1-dev_1.12.2-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libdbus-1-dev:amd64 (1.12.2-1ubuntu1.1) ...\n",
            "Selecting previously unselected package libmpeg3-2.\n",
            "Preparing to unpack .../12-libmpeg3-2_1.8.dfsg-2.1_amd64.deb ...\n",
            "Unpacking libmpeg3-2 (1.8.dfsg-2.1) ...\n",
            "Selecting previously unselected package libmpeg3-dev.\n",
            "Preparing to unpack .../13-libmpeg3-dev_1.8.dfsg-2.1_amd64.deb ...\n",
            "Unpacking libmpeg3-dev (1.8.dfsg-2.1) ...\n",
            "Selecting previously unselected package libdirectfb-dev:amd64.\n",
            "Preparing to unpack .../14-libdirectfb-dev_1.7.7-8_amd64.deb ...\n",
            "Unpacking libdirectfb-dev:amd64 (1.7.7-8) ...\n",
            "Selecting previously unselected package libglfw3:amd64.\n",
            "Preparing to unpack .../15-libglfw3_3.2.1-1_amd64.deb ...\n",
            "Unpacking libglfw3:amd64 (3.2.1-1) ...\n",
            "Selecting previously unselected package libibus-1.0-dev:amd64.\n",
            "Preparing to unpack .../16-libibus-1.0-dev_1.5.17-3ubuntu4_amd64.deb ...\n",
            "Unpacking libibus-1.0-dev:amd64 (1.5.17-3ubuntu4) ...\n",
            "Selecting previously unselected package libmircore1:amd64.\n",
            "Preparing to unpack .../17-libmircore1_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmircore1:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libmircommon7:amd64.\n",
            "Preparing to unpack .../18-libmircommon7_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmircommon7:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libprotobuf-lite10:amd64.\n",
            "Preparing to unpack .../19-libprotobuf-lite10_3.0.0-9.1ubuntu1_amd64.deb ...\n",
            "Unpacking libprotobuf-lite10:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Selecting previously unselected package libmirprotobuf3:amd64.\n",
            "Preparing to unpack .../20-libmirprotobuf3_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmirprotobuf3:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libmirclient9:amd64.\n",
            "Preparing to unpack .../21-libmirclient9_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmirclient9:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libmircore-dev:amd64.\n",
            "Preparing to unpack .../22-libmircore-dev_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmircore-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libprotobuf-dev:amd64.\n",
            "Preparing to unpack .../23-libprotobuf-dev_3.0.0-9.1ubuntu1_amd64.deb ...\n",
            "Unpacking libprotobuf-dev:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Selecting previously unselected package libxkbcommon-dev.\n",
            "Preparing to unpack .../24-libxkbcommon-dev_0.8.0-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libxkbcommon-dev (0.8.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libmircommon-dev:amd64.\n",
            "Preparing to unpack .../25-libmircommon-dev_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmircommon-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libmircookie2:amd64.\n",
            "Preparing to unpack .../26-libmircookie2_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmircookie2:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libmircookie-dev:amd64.\n",
            "Preparing to unpack .../27-libmircookie-dev_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmircookie-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libmirclient-dev:amd64.\n",
            "Preparing to unpack .../28-libmirclient-dev_0.31.1-0ubuntu1_amd64.deb ...\n",
            "Unpacking libmirclient-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Selecting previously unselected package libpulse-mainloop-glib0:amd64.\n",
            "Preparing to unpack .../29-libpulse-mainloop-glib0_1%3a11.1-1ubuntu7.2_amd64.deb ...\n",
            "Unpacking libpulse-mainloop-glib0:amd64 (1:11.1-1ubuntu7.2) ...\n",
            "Selecting previously unselected package libpulse-dev:amd64.\n",
            "Preparing to unpack .../30-libpulse-dev_1%3a11.1-1ubuntu7.2_amd64.deb ...\n",
            "Unpacking libpulse-dev:amd64 (1:11.1-1ubuntu7.2) ...\n",
            "Selecting previously unselected package libsdl1.2debian:amd64.\n",
            "Preparing to unpack .../31-libsdl1.2debian_1.2.15+dfsg2-0.1_amd64.deb ...\n",
            "Unpacking libsdl1.2debian:amd64 (1.2.15+dfsg2-0.1) ...\n",
            "Selecting previously unselected package libsdl-image1.2:amd64.\n",
            "Preparing to unpack .../32-libsdl-image1.2_1.2.12-8_amd64.deb ...\n",
            "Unpacking libsdl-image1.2:amd64 (1.2.12-8) ...\n",
            "Selecting previously unselected package libsdl-sge:amd64.\n",
            "Preparing to unpack .../33-libsdl-sge_030809dfsg-7_amd64.deb ...\n",
            "Unpacking libsdl-sge:amd64 (030809dfsg-7) ...\n",
            "Selecting previously unselected package libsdl1.2-dev.\n",
            "Preparing to unpack .../34-libsdl1.2-dev_1.2.15+dfsg2-0.1_amd64.deb ...\n",
            "Unpacking libsdl1.2-dev (1.2.15+dfsg2-0.1) ...\n",
            "Selecting previously unselected package libsdl-sge-dev:amd64.\n",
            "Preparing to unpack .../35-libsdl-sge-dev_030809dfsg-7_amd64.deb ...\n",
            "Unpacking libsdl-sge-dev:amd64 (030809dfsg-7) ...\n",
            "Selecting previously unselected package libsndio-dev:amd64.\n",
            "Preparing to unpack .../36-libsndio-dev_1.1.0-3_amd64.deb ...\n",
            "Unpacking libsndio-dev:amd64 (1.1.0-3) ...\n",
            "Selecting previously unselected package libudev-dev:amd64.\n",
            "Preparing to unpack .../37-libudev-dev_237-3ubuntu10.25_amd64.deb ...\n",
            "Unpacking libudev-dev:amd64 (237-3ubuntu10.25) ...\n",
            "Selecting previously unselected package libxcursor-dev:amd64.\n",
            "Preparing to unpack .../38-libxcursor-dev_1%3a1.1.15-1_amd64.deb ...\n",
            "Unpacking libxcursor-dev:amd64 (1:1.1.15-1) ...\n",
            "Selecting previously unselected package x11proto-xinerama-dev.\n",
            "Preparing to unpack .../39-x11proto-xinerama-dev_2018.4-4_all.deb ...\n",
            "Unpacking x11proto-xinerama-dev (2018.4-4) ...\n",
            "Selecting previously unselected package libxinerama-dev:amd64.\n",
            "Preparing to unpack .../40-libxinerama-dev_2%3a1.1.3-1_amd64.deb ...\n",
            "Unpacking libxinerama-dev:amd64 (2:1.1.3-1) ...\n",
            "Selecting previously unselected package x11proto-randr-dev.\n",
            "Preparing to unpack .../41-x11proto-randr-dev_2018.4-4_all.deb ...\n",
            "Unpacking x11proto-randr-dev (2018.4-4) ...\n",
            "Selecting previously unselected package libxrandr-dev:amd64.\n",
            "Preparing to unpack .../42-libxrandr-dev_2%3a1.5.1-1_amd64.deb ...\n",
            "Unpacking libxrandr-dev:amd64 (2:1.5.1-1) ...\n",
            "Selecting previously unselected package libxv-dev:amd64.\n",
            "Preparing to unpack .../43-libxv-dev_2%3a1.0.11-1_amd64.deb ...\n",
            "Unpacking libxv-dev:amd64 (2:1.0.11-1) ...\n",
            "Selecting previously unselected package libsdl2-dev:amd64.\n",
            "Preparing to unpack .../44-libsdl2-dev_2.0.8+dfsg1-1ubuntu1.18.04.3_amd64.deb ...\n",
            "Unpacking libsdl2-dev:amd64 (2.0.8+dfsg1-1ubuntu1.18.04.3) ...\n",
            "Selecting previously unselected package libsdl2-gfx-1.0-0:amd64.\n",
            "Preparing to unpack .../45-libsdl2-gfx-1.0-0_1.0.4+dfsg-1_amd64.deb ...\n",
            "Unpacking libsdl2-gfx-1.0-0:amd64 (1.0.4+dfsg-1) ...\n",
            "Selecting previously unselected package libsdl2-gfx-dev:amd64.\n",
            "Preparing to unpack .../46-libsdl2-gfx-dev_1.0.4+dfsg-1_amd64.deb ...\n",
            "Unpacking libsdl2-gfx-dev:amd64 (1.0.4+dfsg-1) ...\n",
            "Selecting previously unselected package libsdl2-image-2.0-0:amd64.\n",
            "Preparing to unpack .../47-libsdl2-image-2.0-0_2.0.3+dfsg1-1_amd64.deb ...\n",
            "Unpacking libsdl2-image-2.0-0:amd64 (2.0.3+dfsg1-1) ...\n",
            "Selecting previously unselected package libsdl2-image-dev:amd64.\n",
            "Preparing to unpack .../48-libsdl2-image-dev_2.0.3+dfsg1-1_amd64.deb ...\n",
            "Unpacking libsdl2-image-dev:amd64 (2.0.3+dfsg1-1) ...\n",
            "Selecting previously unselected package libsdl2-ttf-2.0-0:amd64.\n",
            "Preparing to unpack .../49-libsdl2-ttf-2.0-0_2.0.14+dfsg1-2_amd64.deb ...\n",
            "Unpacking libsdl2-ttf-2.0-0:amd64 (2.0.14+dfsg1-2) ...\n",
            "Selecting previously unselected package libsdl2-ttf-dev:amd64.\n",
            "Preparing to unpack .../50-libsdl2-ttf-dev_2.0.14+dfsg1-2_amd64.deb ...\n",
            "Unpacking libsdl2-ttf-dev:amd64 (2.0.14+dfsg1-2) ...\n",
            "Selecting previously unselected package libsqlite3-dev:amd64.\n",
            "Preparing to unpack .../51-libsqlite3-dev_3.22.0-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libsqlite3-dev:amd64 (3.22.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libst1.\n",
            "Preparing to unpack .../52-libst1_1.9-3.1ubuntu1_amd64.deb ...\n",
            "Unpacking libst1 (1.9-3.1ubuntu1) ...\n",
            "Selecting previously unselected package libst-dev.\n",
            "Preparing to unpack .../53-libst-dev_1.9-3.1ubuntu1_amd64.deb ...\n",
            "Unpacking libst-dev (1.9-3.1ubuntu1) ...\n",
            "Selecting previously unselected package libvncclient1:amd64.\n",
            "Preparing to unpack .../54-libvncclient1_0.9.11+dfsg-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libvncclient1:amd64 (0.9.11+dfsg-1ubuntu1.1) ...\n",
            "Selecting previously unselected package libvncserver1:amd64.\n",
            "Preparing to unpack .../55-libvncserver1_0.9.11+dfsg-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libvncserver1:amd64 (0.9.11+dfsg-1ubuntu1.1) ...\n",
            "Selecting previously unselected package python-pip-whl.\n",
            "Preparing to unpack .../56-python-pip-whl_9.0.1-2.3~ubuntu1.18.04.1_all.deb ...\n",
            "Unpacking python-pip-whl (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
            "Selecting previously unselected package python3-asn1crypto.\n",
            "Preparing to unpack .../57-python3-asn1crypto_0.24.0-1_all.deb ...\n",
            "Unpacking python3-asn1crypto (0.24.0-1) ...\n",
            "Selecting previously unselected package python3-cffi-backend.\n",
            "Preparing to unpack .../58-python3-cffi-backend_1.11.5-1_amd64.deb ...\n",
            "Unpacking python3-cffi-backend (1.11.5-1) ...\n",
            "Selecting previously unselected package python3-crypto.\n",
            "Preparing to unpack .../59-python3-crypto_2.6.1-8ubuntu2_amd64.deb ...\n",
            "Unpacking python3-crypto (2.6.1-8ubuntu2) ...\n",
            "Selecting previously unselected package python3-idna.\n",
            "Preparing to unpack .../60-python3-idna_2.6-1_all.deb ...\n",
            "Unpacking python3-idna (2.6-1) ...\n",
            "Selecting previously unselected package python3-six.\n",
            "Preparing to unpack .../61-python3-six_1.11.0-2_all.deb ...\n",
            "Unpacking python3-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python3-cryptography.\n",
            "Preparing to unpack .../62-python3-cryptography_2.1.4-1ubuntu1.3_amd64.deb ...\n",
            "Unpacking python3-cryptography (2.1.4-1ubuntu1.3) ...\n",
            "Selecting previously unselected package python3-secretstorage.\n",
            "Preparing to unpack .../63-python3-secretstorage_2.3.1-2_all.deb ...\n",
            "Unpacking python3-secretstorage (2.3.1-2) ...\n",
            "Selecting previously unselected package python3-keyring.\n",
            "Preparing to unpack .../64-python3-keyring_10.6.0-1_all.deb ...\n",
            "Unpacking python3-keyring (10.6.0-1) ...\n",
            "Selecting previously unselected package python3-keyrings.alt.\n",
            "Preparing to unpack .../65-python3-keyrings.alt_3.0-1_all.deb ...\n",
            "Unpacking python3-keyrings.alt (3.0-1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../66-python3-pip_9.0.1-2.3~ubuntu1.18.04.1_all.deb ...\n",
            "Unpacking python3-pip (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
            "Selecting previously unselected package python3-pkg-resources.\n",
            "Preparing to unpack .../67-python3-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python3-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../68-python3-setuptools_39.0.1-2_all.deb ...\n",
            "Unpacking python3-setuptools (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../69-python3-wheel_0.30.0-0.2_all.deb ...\n",
            "Unpacking python3-wheel (0.30.0-0.2) ...\n",
            "Selecting previously unselected package python3-xdg.\n",
            "Preparing to unpack .../70-python3-xdg_0.25-4ubuntu1_all.deb ...\n",
            "Unpacking python3-xdg (0.25-4ubuntu1) ...\n",
            "Selecting previously unselected package x11vnc-data.\n",
            "Preparing to unpack .../71-x11vnc-data_0.9.13-3_all.deb ...\n",
            "Unpacking x11vnc-data (0.9.13-3) ...\n",
            "Selecting previously unselected package x11vnc.\n",
            "Preparing to unpack .../72-x11vnc_0.9.13-3_amd64.deb ...\n",
            "Unpacking x11vnc (0.9.13-3) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../73-xvfb_2%3a1.19.6-1ubuntu4.3_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.3) ...\n",
            "Selecting previously unselected package libosmesa6:amd64.\n",
            "Preparing to unpack .../74-libosmesa6_19.0.8-0ubuntu0~18.04.1_amd64.deb ...\n",
            "Unpacking libosmesa6:amd64 (19.0.8-0ubuntu0~18.04.1) ...\n",
            "Selecting previously unselected package libosmesa6-dev:amd64.\n",
            "Preparing to unpack .../75-libosmesa6-dev_19.0.8-0ubuntu0~18.04.1_amd64.deb ...\n",
            "Unpacking libosmesa6-dev:amd64 (19.0.8-0ubuntu0~18.04.1) ...\n",
            "Selecting previously unselected package mesa-utils.\n",
            "Preparing to unpack .../76-mesa-utils_8.4.0-1_amd64.deb ...\n",
            "Unpacking mesa-utils (8.4.0-1) ...\n",
            "Setting up x11vnc-data (0.9.13-3) ...\n",
            "Setting up python-pip-whl (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
            "Setting up libdbus-1-dev:amd64 (1.12.2-1ubuntu1.1) ...\n",
            "Setting up libxcursor-dev:amd64 (1:1.1.15-1) ...\n",
            "Setting up libxkbcommon-dev (0.8.0-1ubuntu0.1) ...\n",
            "Setting up libsdl2-gfx-1.0-0:amd64 (1.0.4+dfsg-1) ...\n",
            "Setting up libpulse-mainloop-glib0:amd64 (1:11.1-1ubuntu7.2) ...\n",
            "Setting up libpulse-dev:amd64 (1:11.1-1ubuntu7.2) ...\n",
            "Setting up libvncserver1:amd64 (0.9.11+dfsg-1ubuntu1.1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Setting up libmircore-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up python3-cffi-backend (1.11.5-1) ...\n",
            "Setting up libsqlite3-dev:amd64 (3.22.0-1ubuntu0.1) ...\n",
            "Setting up python3-crypto (2.6.1-8ubuntu2) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.3) ...\n",
            "Setting up libosmesa6:amd64 (19.0.8-0ubuntu0~18.04.1) ...\n",
            "Setting up libsndio-dev:amd64 (1.1.0-3) ...\n",
            "Setting up libglee0d1:amd64 (5.4.0-2) ...\n",
            "Setting up python3-idna (2.6-1) ...\n",
            "Setting up python3-xdg (0.25-4ubuntu1) ...\n",
            "Setting up libglfw3:amd64 (3.2.1-1) ...\n",
            "Setting up python3-six (1.11.0-2) ...\n",
            "Setting up libvncclient1:amd64 (0.9.11+dfsg-1ubuntu1.1) ...\n",
            "Setting up libsdl1.2debian:amd64 (1.2.15+dfsg2-0.1) ...\n",
            "Setting up python3-wheel (0.30.0-0.2) ...\n",
            "Setting up python3-pkg-resources (39.0.1-2) ...\n",
            "Setting up glee-dev:amd64 (5.4.0-2) ...\n",
            "Setting up python3-asn1crypto (0.24.0-1) ...\n",
            "Setting up liba52-0.7.4:amd64 (0.7.4-19) ...\n",
            "Setting up libmircookie2:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up libst1 (1.9-3.1ubuntu1) ...\n",
            "Setting up libmpeg3-2 (1.8.dfsg-2.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (19.0.8-0ubuntu0~18.04.1) ...\n",
            "Setting up x11proto-xinerama-dev (2018.4-4) ...\n",
            "Setting up x11proto-randr-dev (2018.4-4) ...\n",
            "Setting up libxinerama-dev:amd64 (2:1.1.3-1) ...\n",
            "Setting up libxv-dev:amd64 (2:1.0.11-1) ...\n",
            "Setting up libdirectfb-1.7-7:amd64 (1.7.7-8) ...\n",
            "Setting up python3-pip (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
            "Setting up libcapnp-0.6.1:amd64 (0.6.1-1ubuntu1) ...\n",
            "Setting up libsdl2-image-2.0-0:amd64 (2.0.3+dfsg1-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up libibus-1.0-5:amd64 (1.5.17-3ubuntu4) ...\n",
            "Setting up libsdl-image1.2:amd64 (1.2.12-8) ...\n",
            "Setting up libsdl2-ttf-2.0-0:amd64 (2.0.14+dfsg1-2) ...\n",
            "Setting up libmpeg3-dev (1.8.dfsg-2.1) ...\n",
            "Setting up python3-setuptools (39.0.1-2) ...\n",
            "Setting up libmircore1:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up libprotobuf-lite10:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Setting up libslang2-dev:amd64 (2.3.1a-3ubuntu1) ...\n",
            "Setting up libudev-dev:amd64 (237-3ubuntu10.25) ...\n",
            "Setting up mesa-utils (8.4.0-1) ...\n",
            "Setting up python3-cryptography (2.1.4-1ubuntu1.3) ...\n",
            "Setting up gir1.2-ibus-1.0:amd64 (1.5.17-3ubuntu4) ...\n",
            "Setting up libosmesa6-dev:amd64 (19.0.8-0ubuntu0~18.04.1) ...\n",
            "Setting up libxrandr-dev:amd64 (2:1.5.1-1) ...\n",
            "Setting up x11vnc (0.9.13-3) ...\n",
            "Setting up libmirprotobuf3:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up libprotobuf-dev:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Setting up python3-keyrings.alt (3.0-1) ...\n",
            "Setting up lib++dfb-1.7-7:amd64 (1.7.7-8) ...\n",
            "Setting up libmircookie-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up libst-dev (1.9-3.1ubuntu1) ...\n",
            "Setting up libcaca-dev (0.99.beta19-2ubuntu0.18.04.1) ...\n",
            "Setting up libibus-1.0-dev:amd64 (1.5.17-3ubuntu4) ...\n",
            "Setting up libsdl-sge:amd64 (030809dfsg-7) ...\n",
            "Setting up libsdl1.2-dev (1.2.15+dfsg2-0.1) ...\n",
            "Setting up libmircommon7:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up python3-secretstorage (2.3.1-2) ...\n",
            "Setting up python3-keyring (10.6.0-1) ...\n",
            "Setting up libdirectfb-dev:amd64 (1.7.7-8) ...\n",
            "Setting up libmirclient9:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up libsdl-sge-dev:amd64 (030809dfsg-7) ...\n",
            "Setting up libmircommon-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up libmirclient-dev:amd64 (0.31.1-0ubuntu1) ...\n",
            "Setting up libsdl2-dev:amd64 (2.0.8+dfsg1-1ubuntu1.18.04.3) ...\n",
            "Setting up libsdl2-ttf-dev:amd64 (2.0.14+dfsg1-2) ...\n",
            "Setting up libsdl2-image-dev:amd64 (2.0.3+dfsg1-1) ...\n",
            "Setting up libsdl2-gfx-dev:amd64 (1.0.4+dfsg-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZIOtmtx1fde",
        "colab_type": "text"
      },
      "source": [
        "### Instalar bibliotecas Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erT5aDew1fdf",
        "colab_type": "text"
      },
      "source": [
        "#### Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0qzSyI21fdh",
        "colab_type": "code",
        "outputId": "b5d5eebc-9144-4581-fe46-4758841a1c58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 60kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.16.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.4)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.7)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (0.15.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSVYWPUy1fdk",
        "colab_type": "text"
      },
      "source": [
        "#### OpenAI baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WwkBNkM1fdl",
        "colab_type": "code",
        "outputId": "1bcbfafc-57f4-4b4b-fd1d-d2a8b7a52984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "source": [
        "!git clone https://github.com/openai/baselines.git\n",
        "%cd baselines\n",
        "!pip install -e .\n",
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'baselines'...\n",
            "remote: Enumerating objects: 3531, done.\u001b[K\n",
            "remote: Total 3531 (delta 0), reused 0 (delta 0), pack-reused 3531\u001b[K\n",
            "Receiving objects: 100% (3531/3531), 6.47 MiB | 4.39 MiB/s, done.\n",
            "Resolving deltas: 100% (2342/2342), done.\n",
            "/content/baselines\n",
            "Obtaining file:///content/baselines\n",
            "Requirement already satisfied: gym<1.0.0,>=0.10.0 in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (0.10.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (1.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (4.28.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (0.13.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (7.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from baselines==0.1.6) (3.4.5.20)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.10.0->baselines==0.1.6) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.10.0->baselines==0.1.6) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.10.0->baselines==0.1.6) (1.16.4)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.10.0->baselines==0.1.6) (2.21.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym<1.0.0,>=0.10.0->baselines==0.1.6) (0.16.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym<1.0.0,>=0.10.0->baselines==0.1.6) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym<1.0.0,>=0.10.0->baselines==0.1.6) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym<1.0.0,>=0.10.0->baselines==0.1.6) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym<1.0.0,>=0.10.0->baselines==0.1.6) (2019.6.16)\n",
            "Installing collected packages: baselines\n",
            "  Running setup.py develop for baselines\n",
            "Successfully installed baselines\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYAkybmR1fdp",
        "colab_type": "text"
      },
      "source": [
        "#### Football"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ7cu7HP1fdz",
        "colab_type": "code",
        "outputId": "201eec0c-72ce-45c8-fd00-5ea84548a3e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "!git clone https://github.com/jgromero/football.git\n",
        "%cd football\n",
        "!pip install .[tf_gpu]\n",
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'football'...\n",
            "remote: Enumerating objects: 1028, done.\u001b[K\n",
            "remote: Counting objects: 100% (1028/1028), done.\u001b[K\n",
            "remote: Compressing objects: 100% (747/747), done.\u001b[K\n",
            "remote: Total 1028 (delta 259), reused 1026 (delta 259), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1028/1028), 26.28 MiB | 8.84 MiB/s, done.\n",
            "Resolving deltas: 100% (259/259), done.\n",
            "/content/football\n",
            "Processing /content/football\n",
            "Collecting pygame==1.9.6 (from gfootball==0.2)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/ede6428359f913ed9cd1643dd5533aefeb5a2699cc95bea089de50ead586/pygame-1.9.6-cp36-cp36m-manylinux1_x86_64.whl (11.4MB)\n",
            "\u001b[K     |████████████████████████████████| 11.4MB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gfootball==0.2) (3.4.5.20)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gfootball==0.2) (1.3.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gfootball==0.2) (0.10.11)\n",
            "Requirement already satisfied: tensorflow-gpu<2.0 in /usr/local/lib/python3.6/dist-packages (from gfootball==0.2) (1.14.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python->gfootball==0.2) (1.16.4)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gfootball==0.2) (2.21.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gfootball==0.2) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gfootball==0.2) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2.0->gfootball==0.2) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2.0->gfootball==0.2) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2.0->gfootball==0.2) (3.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2.0->gfootball==0.2) (1.14.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2.0->gfootball==0.2) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2.0->gfootball==0.2) (1.11.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2.0->gfootball==0.2) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2.0->gfootball==0.2) (0.1.7)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2.0->gfootball==0.2) (0.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2.0->gfootball==0.2) (0.33.4)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2.0->gfootball==0.2) (1.14.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2.0->gfootball==0.2) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2.0->gfootball==0.2) (0.7.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gfootball==0.2) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gfootball==0.2) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gfootball==0.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gfootball==0.2) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym->gfootball==0.2) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu<2.0->gfootball==0.2) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu<2.0->gfootball==0.2) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu<2.0->gfootball==0.2) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu<2.0->gfootball==0.2) (3.1.1)\n",
            "Building wheels for collected packages: gfootball\n",
            "\n",
            "  Building wheel for gfootball (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gfootball: filename=gfootball-0.2-cp36-cp36m-linux_x86_64.whl size=24534784 sha256=c1d425a86ea004060b005f57347405b7c564c067d488f4f826011e86e66e54c5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0d534r96/wheels/41/ad/ae/8cf1d92b8694b10187e5daf33e8d5c248ffa5437e234ccbbee\n",
            "Successfully built gfootball\n",
            "Installing collected packages: pygame, gfootball\n",
            "Successfully installed gfootball-0.2 pygame-1.9.6\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP9LpJgk1fd3",
        "colab_type": "text"
      },
      "source": [
        "### Test con implementación PPO de OpenAI baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlBTnGuU1fd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python3 -m gfootball.examples.run_ppo2 --level=academy_empty_goal_close"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aFAj9h01fd_",
        "colab_type": "text"
      },
      "source": [
        "### Agente aleatorio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y92dqeb1feL",
        "colab_type": "code",
        "outputId": "63f7f255-d2fd-4ac5-f865-18e9e5b35106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "'''import gfootball.env as football_env\n",
        "\n",
        "env = football_env.create_environment(\n",
        "    env_name='academy_empty_goal_close', \n",
        "    stacked=False,\n",
        "    representation='simple115',\n",
        "    with_checkpoints=True,\n",
        "    render=False)\n",
        "\n",
        "for i in range(1, 10):\n",
        "    env.reset()\n",
        "    acc_reward = 0\n",
        "\n",
        "    while True:\n",
        "        action = env.action_space.sample()\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        acc_reward += reward \n",
        "    \n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print(\"Recompensa episodio {:d}: {:.2f}\".format(i, acc_reward))\n",
        "\n",
        "env.close()'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import gfootball.env as football_env\\n\\nenv = football_env.create_environment(\\n    env_name=\\'academy_empty_goal_close\\', \\n    stacked=False,\\n    representation=\\'simple115\\',\\n    with_checkpoints=True,\\n    render=False)\\n\\nfor i in range(1, 10):\\n    env.reset()\\n    acc_reward = 0\\n\\n    while True:\\n        action = env.action_space.sample()\\n        observation, reward, done, info = env.step(action)\\n        acc_reward += reward \\n    \\n        if done:\\n            break\\n\\n    print(\"Recompensa episodio {:d}: {:.2f}\".format(i, acc_reward))\\n\\nenv.close()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR8GQgoZP8ml",
        "colab_type": "text"
      },
      "source": [
        "# DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaXb0RVxOO9J",
        "colab_type": "text"
      },
      "source": [
        "Instalo las librerias de keras-rl para implementar la DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8mP7f8nQAX9",
        "colab_type": "code",
        "outputId": "f87c4381-a4e6-40d1-cc08-83aa56141572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/matthiasplappert/keras-rl.git\n",
        "%cd /content/keras-rl\n",
        "!python setup.py install"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-rl'...\n",
            "remote: Enumerating objects: 1715, done.\u001b[K\n",
            "remote: Total 1715 (delta 0), reused 0 (delta 0), pack-reused 1715\u001b[K\n",
            "Receiving objects: 100% (1715/1715), 1.37 MiB | 1.54 MiB/s, done.\n",
            "Resolving deltas: 100% (1066/1066), done.\n",
            "/content/keras-rl\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating keras_rl.egg-info\n",
            "writing keras_rl.egg-info/PKG-INFO\n",
            "writing dependency_links to keras_rl.egg-info/dependency_links.txt\n",
            "writing requirements to keras_rl.egg-info/requires.txt\n",
            "writing top-level names to keras_rl.egg-info/top_level.txt\n",
            "writing manifest file 'keras_rl.egg-info/SOURCES.txt'\n",
            "writing manifest file 'keras_rl.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/rl\n",
            "copying rl/policy.py -> build/lib/rl\n",
            "copying rl/util.py -> build/lib/rl\n",
            "copying rl/callbacks.py -> build/lib/rl\n",
            "copying rl/__init__.py -> build/lib/rl\n",
            "copying rl/random.py -> build/lib/rl\n",
            "copying rl/processors.py -> build/lib/rl\n",
            "copying rl/memory.py -> build/lib/rl\n",
            "copying rl/core.py -> build/lib/rl\n",
            "creating build/lib/tests\n",
            "copying tests/__init__.py -> build/lib/tests\n",
            "creating build/lib/utils\n",
            "copying utils/__init__.py -> build/lib/utils\n",
            "creating build/lib/rl/common\n",
            "copying rl/common/tile_images.py -> build/lib/rl/common\n",
            "copying rl/common/cmd_util.py -> build/lib/rl/common\n",
            "copying rl/common/__init__.py -> build/lib/rl/common\n",
            "copying rl/common/misc_util.py -> build/lib/rl/common\n",
            "creating build/lib/rl/agents\n",
            "copying rl/agents/cem.py -> build/lib/rl/agents\n",
            "copying rl/agents/ddpg.py -> build/lib/rl/agents\n",
            "copying rl/agents/__init__.py -> build/lib/rl/agents\n",
            "copying rl/agents/dqn.py -> build/lib/rl/agents\n",
            "copying rl/agents/sarsa.py -> build/lib/rl/agents\n",
            "creating build/lib/rl/common/vec_env\n",
            "copying rl/common/vec_env/__init__.py -> build/lib/rl/common/vec_env\n",
            "copying rl/common/vec_env/subproc_env_vec.py -> build/lib/rl/common/vec_env\n",
            "creating build/lib/tests/rl\n",
            "copying tests/rl/test_util.py -> build/lib/tests/rl\n",
            "copying tests/rl/test_memory.py -> build/lib/tests/rl\n",
            "copying tests/rl/util.py -> build/lib/tests/rl\n",
            "copying tests/rl/__init__.py -> build/lib/tests/rl\n",
            "copying tests/rl/test_core.py -> build/lib/tests/rl\n",
            "creating build/lib/tests/rl/agents\n",
            "copying tests/rl/agents/test_cem.py -> build/lib/tests/rl/agents\n",
            "copying tests/rl/agents/test_ddpg.py -> build/lib/tests/rl/agents\n",
            "copying tests/rl/agents/test_dqn.py -> build/lib/tests/rl/agents\n",
            "copying tests/rl/agents/__init__.py -> build/lib/tests/rl/agents\n",
            "creating build/lib/utils/gym\n",
            "copying utils/gym/prng.py -> build/lib/utils/gym\n",
            "copying utils/gym/__init__.py -> build/lib/utils/gym\n",
            "creating build/lib/utils/gym/envs\n",
            "copying utils/gym/envs/__init__.py -> build/lib/utils/gym/envs\n",
            "copying utils/gym/envs/twoRoundDeterministicRewardEnv.py -> build/lib/utils/gym/envs\n",
            "creating build/lib/utils/gym/spaces\n",
            "copying utils/gym/spaces/discrete.py -> build/lib/utils/gym/spaces\n",
            "copying utils/gym/spaces/__init__.py -> build/lib/utils/gym/spaces\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/rl\n",
            "copying build/lib/rl/policy.py -> build/bdist.linux-x86_64/egg/rl\n",
            "creating build/bdist.linux-x86_64/egg/rl/common\n",
            "copying build/lib/rl/common/tile_images.py -> build/bdist.linux-x86_64/egg/rl/common\n",
            "copying build/lib/rl/common/cmd_util.py -> build/bdist.linux-x86_64/egg/rl/common\n",
            "copying build/lib/rl/common/__init__.py -> build/bdist.linux-x86_64/egg/rl/common\n",
            "copying build/lib/rl/common/misc_util.py -> build/bdist.linux-x86_64/egg/rl/common\n",
            "creating build/bdist.linux-x86_64/egg/rl/common/vec_env\n",
            "copying build/lib/rl/common/vec_env/__init__.py -> build/bdist.linux-x86_64/egg/rl/common/vec_env\n",
            "copying build/lib/rl/common/vec_env/subproc_env_vec.py -> build/bdist.linux-x86_64/egg/rl/common/vec_env\n",
            "copying build/lib/rl/util.py -> build/bdist.linux-x86_64/egg/rl\n",
            "copying build/lib/rl/callbacks.py -> build/bdist.linux-x86_64/egg/rl\n",
            "copying build/lib/rl/__init__.py -> build/bdist.linux-x86_64/egg/rl\n",
            "copying build/lib/rl/random.py -> build/bdist.linux-x86_64/egg/rl\n",
            "copying build/lib/rl/processors.py -> build/bdist.linux-x86_64/egg/rl\n",
            "copying build/lib/rl/memory.py -> build/bdist.linux-x86_64/egg/rl\n",
            "creating build/bdist.linux-x86_64/egg/rl/agents\n",
            "copying build/lib/rl/agents/cem.py -> build/bdist.linux-x86_64/egg/rl/agents\n",
            "copying build/lib/rl/agents/ddpg.py -> build/bdist.linux-x86_64/egg/rl/agents\n",
            "copying build/lib/rl/agents/__init__.py -> build/bdist.linux-x86_64/egg/rl/agents\n",
            "copying build/lib/rl/agents/dqn.py -> build/bdist.linux-x86_64/egg/rl/agents\n",
            "copying build/lib/rl/agents/sarsa.py -> build/bdist.linux-x86_64/egg/rl/agents\n",
            "copying build/lib/rl/core.py -> build/bdist.linux-x86_64/egg/rl\n",
            "creating build/bdist.linux-x86_64/egg/tests\n",
            "creating build/bdist.linux-x86_64/egg/tests/rl\n",
            "copying build/lib/tests/rl/test_util.py -> build/bdist.linux-x86_64/egg/tests/rl\n",
            "copying build/lib/tests/rl/test_memory.py -> build/bdist.linux-x86_64/egg/tests/rl\n",
            "copying build/lib/tests/rl/util.py -> build/bdist.linux-x86_64/egg/tests/rl\n",
            "copying build/lib/tests/rl/__init__.py -> build/bdist.linux-x86_64/egg/tests/rl\n",
            "creating build/bdist.linux-x86_64/egg/tests/rl/agents\n",
            "copying build/lib/tests/rl/agents/test_cem.py -> build/bdist.linux-x86_64/egg/tests/rl/agents\n",
            "copying build/lib/tests/rl/agents/test_ddpg.py -> build/bdist.linux-x86_64/egg/tests/rl/agents\n",
            "copying build/lib/tests/rl/agents/test_dqn.py -> build/bdist.linux-x86_64/egg/tests/rl/agents\n",
            "copying build/lib/tests/rl/agents/__init__.py -> build/bdist.linux-x86_64/egg/tests/rl/agents\n",
            "copying build/lib/tests/rl/test_core.py -> build/bdist.linux-x86_64/egg/tests/rl\n",
            "copying build/lib/tests/__init__.py -> build/bdist.linux-x86_64/egg/tests\n",
            "creating build/bdist.linux-x86_64/egg/utils\n",
            "copying build/lib/utils/__init__.py -> build/bdist.linux-x86_64/egg/utils\n",
            "creating build/bdist.linux-x86_64/egg/utils/gym\n",
            "copying build/lib/utils/gym/prng.py -> build/bdist.linux-x86_64/egg/utils/gym\n",
            "copying build/lib/utils/gym/__init__.py -> build/bdist.linux-x86_64/egg/utils/gym\n",
            "creating build/bdist.linux-x86_64/egg/utils/gym/envs\n",
            "copying build/lib/utils/gym/envs/__init__.py -> build/bdist.linux-x86_64/egg/utils/gym/envs\n",
            "copying build/lib/utils/gym/envs/twoRoundDeterministicRewardEnv.py -> build/bdist.linux-x86_64/egg/utils/gym/envs\n",
            "creating build/bdist.linux-x86_64/egg/utils/gym/spaces\n",
            "copying build/lib/utils/gym/spaces/discrete.py -> build/bdist.linux-x86_64/egg/utils/gym/spaces\n",
            "copying build/lib/utils/gym/spaces/__init__.py -> build/bdist.linux-x86_64/egg/utils/gym/spaces\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/policy.py to policy.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/common/tile_images.py to tile_images.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/common/cmd_util.py to cmd_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/common/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/common/misc_util.py to misc_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/common/vec_env/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/common/vec_env/subproc_env_vec.py to subproc_env_vec.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/util.py to util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/callbacks.py to callbacks.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/random.py to random.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/processors.py to processors.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/memory.py to memory.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/agents/cem.py to cem.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/agents/ddpg.py to ddpg.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/agents/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/agents/dqn.py to dqn.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/agents/sarsa.py to sarsa.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/rl/core.py to core.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/test_util.py to test_util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/test_memory.py to test_memory.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/util.py to util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/agents/test_cem.py to test_cem.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/agents/test_ddpg.py to test_ddpg.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/agents/test_dqn.py to test_dqn.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/agents/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/rl/test_core.py to test_core.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tests/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/utils/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/utils/gym/prng.py to prng.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/utils/gym/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/utils/gym/envs/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/utils/gym/envs/twoRoundDeterministicRewardEnv.py to twoRoundDeterministicRewardEnv.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/utils/gym/spaces/discrete.py to discrete.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/utils/gym/spaces/__init__.py to __init__.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying keras_rl.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying keras_rl.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying keras_rl.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying keras_rl.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying keras_rl.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "tests.rl.__pycache__.test_core.cpython-36: module references __file__\n",
            "tests.rl.__pycache__.test_memory.cpython-36: module references __file__\n",
            "tests.rl.__pycache__.test_util.cpython-36: module references __file__\n",
            "tests.rl.agents.__pycache__.test_dqn.cpython-36: module references __file__\n",
            "creating dist\n",
            "creating 'dist/keras_rl-0.4.2-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing keras_rl-0.4.2-py3.6.egg\n",
            "creating /usr/local/lib/python3.6/dist-packages/keras_rl-0.4.2-py3.6.egg\n",
            "Extracting keras_rl-0.4.2-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding keras-rl 0.4.2 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/keras_rl-0.4.2-py3.6.egg\n",
            "Processing dependencies for keras-rl==0.4.2\n",
            "Searching for Keras==2.2.4\n",
            "Best match: Keras 2.2.4\n",
            "Adding Keras 2.2.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras-Applications==1.0.8\n",
            "Best match: Keras-Applications 1.0.8\n",
            "Adding Keras-Applications 1.0.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for six==1.12.0\n",
            "Best match: six 1.12.0\n",
            "Adding six 1.12.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for scipy==1.3.1\n",
            "Best match: scipy 1.3.1\n",
            "Adding scipy 1.3.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for PyYAML==3.13\n",
            "Best match: PyYAML 3.13\n",
            "Adding PyYAML 3.13 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for h5py==2.8.0\n",
            "Best match: h5py 2.8.0\n",
            "Adding h5py 2.8.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.16.4\n",
            "Best match: numpy 1.16.4\n",
            "Adding numpy 1.16.4 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras-Preprocessing==1.1.0\n",
            "Best match: Keras-Preprocessing 1.1.0\n",
            "Adding Keras-Preprocessing 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for keras-rl==0.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQUmd42CIP_S",
        "colab_type": "text"
      },
      "source": [
        "Importo las librerias correspondientes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlwQXuxVL75c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense, Input, Flatten, Dropout\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNxVBm1hr_mx",
        "colab_type": "text"
      },
      "source": [
        "Veo:\n",
        "\n",
        "           - Estado\n",
        "           - Accion\n",
        "           - Rango de recompenza"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en3zbHf0SKsy",
        "colab_type": "code",
        "outputId": "d7499ff3-8111-4acc-ba14-1e1506db2cc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import gfootball.env as football_env\n",
        "\n",
        "env = football_env.create_environment(\n",
        "    env_name='academy_empty_goal_close', \n",
        "    stacked=False,\n",
        "    representation='simple115',\n",
        "    with_checkpoints=True,\n",
        "    render=False)\n",
        "\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "print(env.action_space.n)\n",
        "print(env.reward_range)\n",
        "actions = env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box(115,)\n",
            "Discrete(21)\n",
            "21\n",
            "(-inf, inf)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX_SwEcVsJdf",
        "colab_type": "text"
      },
      "source": [
        "# Ahora implemento la DQN con Keras-lr!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96lCyY56tGbW",
        "colab_type": "text"
      },
      "source": [
        "## Construyo el modelo (con Keras) que se utilizará en la implementación de la DQN.\n",
        "\n",
        "El input es de tamaño del state (115) x 3, aplanado (con el flatten), ya que estoy pasando 3 frames de estado a la red para que, ademas de los datos que tiene de cada estado, tenga los datos de los movimientos que se realizan en el juego para aprender mejor peculiaridades que surgen por los movimientos.\n",
        "\n",
        "La capa flatten es para poner en una sola dimension lo del input y de este modo se pueda conectar con la capa densa.\n",
        "\n",
        "Capas densas ocultas, con activación relu.\n",
        "\n",
        "La capa densa final es lineal y tiene ese tam ya que es la salida de la red, la cual representaria el valor Q para cada accion dado el estado de entrada.\n",
        "\n",
        "\n",
        "Utilizo dropout para mejorar el entrenamiento. Aunque no debería de existir el overfitting ya que cada episodio inicia de manera diferente y por lo tanto se resuelve de otra manera, aplicar dropout beneficio enormemente el entrenamiento de la red. Yo supongo que es porque le da oportunidad a otras neuronas de modificar sus pesos, siendo por ejemplo el caso de que casi siempre que se inicia el episodio el jugador tiene que ir contra un mismo lugar y eso le dificulta generalizar cuando inicia en otra posición o con otro estado muy distinto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU2lO4y0Oidl",
        "colab_type": "code",
        "outputId": "91c9b530-c1be-4105-e54c-c6352ae618d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "source": [
        "WINDOW_LENGTH = 3\n",
        "\n",
        "i = Input(shape=(WINDOW_LENGTH,) + env.observation_space.shape)\n",
        "d = Flatten()(i)\n",
        "d = Dense(100, activation='relu')(d)\n",
        "d = Dropout(0.4)(d)\n",
        "d = Dense(200, activation='relu')(d)\n",
        "d = Dropout(0.4)(d) #La que salio tdo obien tenia dropout\n",
        "d = Dense(100, activation='relu')(d)\n",
        "d = Dense(21, activation='linear')(d)\n",
        "\n",
        "model = Model(inputs=i, outputs=d)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0815 14:23:22.701970 140310373406592 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 3, 115)            0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 345)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100)               34600     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 21)                2121      \n",
            "=================================================================\n",
            "Total params: 77,021\n",
            "Trainable params: 77,021\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upqmUuqytzCL",
        "colab_type": "text"
      },
      "source": [
        "Se tendrá una politica epsilon greedy con epsilon = 0.1.\n",
        "\n",
        "\n",
        "Además se utilizará una memoria secuencial, la cual guarda en la memoria cada jugada y luego obtiene un batch, con el cual entrena. Al tener tamaño de ventana 3 significa que voy a proveer a la red de 3 frames seguidos dentro de la memoria para entrenar (relacion directa con el input de la red, el cual tambien es 3), para que tenga una pequeña \"pelicula\" la cual podrá observar para medir, por ejemplo, la velocidad de la pelota, el movimiento del jugador, entre otras cosas*. El limite representa cuantos steps puede guardar la memoria antes de que se  deban reemplazar los mas viejos para agregar otros nuevos ejemplos.\n",
        "\n",
        "\n",
        "DQNAgent utiliza el modelo creado anteriormente como Q y Q_target, la cantidad de acciones que puede realizar el agente, el tipo de memoria que va a utilizar, el nb_steps_warmup significa que cantidad de steps se realizan (para agregar jugadas a la mem) antes de iniciar el entrenamiento. En este caso se guardan 10.000 steps a la memoria de jugadas, para luego empezar a entrenar. target_model_update representa que tanto se va a modificar la red target cada vez que se actualice (en este caso se actualiza cada step). La formula utilizada para este update a la red target es: \n",
        "        \n",
        "         (1 - target_model_update) * oldθ + target_model_update * newθ\n",
        "\n",
        "En caso de aplicar un target_model_update mayor a 1 se copiaran la totalidad de los parámetros de la red Q a la de Q_target cada target_model_update cantidad de steps. policy es la politica que se utiliza para seleccionar la acción a realizar, la cual, como se dijo anteriormente, es e-greedy (e=0.1).\n",
        "\n",
        "\n",
        "Se utilizará como optimizador Adam con lr de 0.001 y como metrica el mean absolute error (el promedio del error en su valor absoluto).\n",
        "\n",
        "La idea del lr bajo y de actualizar poco la red target es para que la red no se modifique enormemente por buenas o malas jugadas, ya que puede tener problemas con el checkpoint (contaria como mejor una jugada de acercarse mucho al area enemigo en vez de una de acercarse un poco y pegarle aunque erre por poco, ya que en este ultimo caso es relativamente facil corregir el disparo en otros episodios).\n",
        "\n",
        "\n",
        "**Esto ya se encuentra incluido en el espacio observado (lo que se le pasa a la red), pero me parecio un buen agregado incluir una ventana de 3 frames para que aprenda mejor los movimientos del juego.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_SjcINPt7YT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = EpsGreedyQPolicy() # Eps = 0.1 \n",
        "memory = SequentialMemory(limit=50000, window_length=WINDOW_LENGTH)\n",
        "dqn = DQNAgent(model=model, nb_actions=actions, memory=memory, nb_steps_warmup=10000, target_model_update=0.02, policy=policy)\n",
        "dqn.compile(optimizer=Adam(lr=0.001), metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxvgCXf0QBew",
        "colab_type": "text"
      },
      "source": [
        "# Train!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX_bkhqlP5FA",
        "colab_type": "text"
      },
      "source": [
        "Entreno al agente por 150.000 steps. Puede parecer poco pero para cumplir la tarea, dada la topología que se tiene, es suficiente.\n",
        "\n",
        "Tiempo de demora de entrenamiento: 37min."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be1da7btuB7v",
        "colab_type": "code",
        "outputId": "825bcb4a-74ca-459a-83d8-83a08b49082f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "h_train = dqn.fit(env, nb_steps=150000, visualize=False, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 150000 steps ...\n",
            "     12/150000: episode: 1, duration: 0.172s, episode steps: 12, steps per second: 70, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 13.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "     25/150000: episode: 2, duration: 0.160s, episode steps: 13, steps per second: 81, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 10.846 [4.000, 20.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "     40/150000: episode: 3, duration: 0.170s, episode steps: 15, steps per second: 88, episode reward: 2.000, mean reward: 0.133 [0.000, 1.100], mean action: 11.267 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.006], loss: --, mean_absolute_error: --, mean_q: --\n",
            "     49/150000: episode: 4, duration: 0.154s, episode steps: 9, steps per second: 59, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 12.556 [5.000, 19.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "     61/150000: episode: 5, duration: 0.148s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.491], loss: --, mean_absolute_error: --, mean_q: --\n",
            "     71/150000: episode: 6, duration: 0.154s, episode steps: 10, steps per second: 65, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.800 [5.000, 17.000], mean observation: -0.636 [-1.011, 1.017], loss: --, mean_absolute_error: --, mean_q: --\n",
            "     83/150000: episode: 7, duration: 0.165s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.083 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "     98/150000: episode: 8, duration: 0.211s, episode steps: 15, steps per second: 71, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 12.133 [10.000, 16.000], mean observation: -0.629 [-1.011, 2.625], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    111/150000: episode: 9, duration: 0.161s, episode steps: 13, steps per second: 81, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.385 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    123/150000: episode: 10, duration: 0.159s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.465], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    135/150000: episode: 11, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    147/150000: episode: 12, duration: 0.163s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    159/150000: episode: 13, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.250 [2.000, 20.000], mean observation: -0.635 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    171/150000: episode: 14, duration: 0.163s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.156], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    186/150000: episode: 15, duration: 0.200s, episode steps: 15, steps per second: 75, episode reward: 0.900, mean reward: 0.060 [0.000, 0.900], mean action: 11.200 [4.000, 12.000], mean observation: -0.634 [-1.011, 1.012], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    198/150000: episode: 16, duration: 0.163s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.715], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    210/150000: episode: 17, duration: 0.155s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    222/150000: episode: 18, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.000 [5.000, 12.000], mean observation: -0.636 [-1.041, 1.010], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    232/150000: episode: 19, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.800 [2.000, 20.000], mean observation: -0.632 [-1.037, 1.492], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    242/150000: episode: 20, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.019], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    254/150000: episode: 21, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.667 [5.000, 15.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    266/150000: episode: 22, duration: 0.159s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.004], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    275/150000: episode: 23, duration: 0.121s, episode steps: 9, steps per second: 74, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 10.111 [2.000, 12.000], mean observation: -0.633 [-1.009, 1.019], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    287/150000: episode: 24, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.333 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.031], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    297/150000: episode: 25, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.800 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.057], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    307/150000: episode: 26, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.600 [5.000, 15.000], mean observation: -0.634 [-1.041, 1.008], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    319/150000: episode: 27, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    390/150000: episode: 28, duration: 0.364s, episode steps: 71, steps per second: 195, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 11.704 [2.000, 20.000], mean observation: -0.636 [-1.009, 1.077], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    403/150000: episode: 29, duration: 0.160s, episode steps: 13, steps per second: 81, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.769 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    416/150000: episode: 30, duration: 0.162s, episode steps: 13, steps per second: 80, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 12.615 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    428/150000: episode: 31, duration: 0.148s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.917 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.014], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    440/150000: episode: 32, duration: 0.169s, episode steps: 12, steps per second: 71, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 13.000], mean observation: -0.633 [-1.011, 1.366], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    453/150000: episode: 33, duration: 0.154s, episode steps: 13, steps per second: 84, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.077 [5.000, 13.000], mean observation: -0.636 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    465/150000: episode: 34, duration: 0.159s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    477/150000: episode: 35, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    489/150000: episode: 36, duration: 0.155s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    499/150000: episode: 37, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.700 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.018], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    640/150000: episode: 38, duration: 0.651s, episode steps: 141, steps per second: 217, episode reward: 0.900, mean reward: 0.006 [0.000, 0.900], mean action: 11.823 [4.000, 19.000], mean observation: -0.629 [-1.009, 1.336], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    650/150000: episode: 39, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 9.900 [1.000, 12.000], mean observation: -0.636 [-1.011, 1.014], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    663/150000: episode: 40, duration: 0.162s, episode steps: 13, steps per second: 80, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.462 [0.000, 19.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    675/150000: episode: 41, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.750 [5.000, 16.000], mean observation: -0.636 [-1.011, 1.012], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    687/150000: episode: 42, duration: 0.169s, episode steps: 12, steps per second: 71, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.667 [5.000, 15.000], mean observation: -0.636 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    699/150000: episode: 43, duration: 0.155s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    711/150000: episode: 44, duration: 0.155s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.368], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    721/150000: episode: 45, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.400 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.017], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    734/150000: episode: 46, duration: 0.167s, episode steps: 13, steps per second: 78, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.769 [5.000, 16.000], mean observation: -0.635 [-1.011, 1.012], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    751/150000: episode: 47, duration: 0.203s, episode steps: 17, steps per second: 84, episode reward: 0.900, mean reward: 0.053 [0.000, 0.900], mean action: 11.353 [1.000, 12.000], mean observation: -0.635 [-1.009, 1.009], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    763/150000: episode: 48, duration: 0.161s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.181], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    813/150000: episode: 49, duration: 0.307s, episode steps: 50, steps per second: 163, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 11.640 [4.000, 17.000], mean observation: -0.611 [-1.011, 7.662], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    825/150000: episode: 50, duration: 0.153s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.034], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    837/150000: episode: 51, duration: 0.161s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    854/150000: episode: 52, duration: 0.199s, episode steps: 17, steps per second: 85, episode reward: 0.900, mean reward: 0.053 [0.000, 0.900], mean action: 11.000 [2.000, 12.000], mean observation: -0.627 [-1.011, 3.492], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    866/150000: episode: 53, duration: 0.163s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.333 [2.000, 13.000], mean observation: -0.633 [-1.011, 1.206], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    878/150000: episode: 54, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.041, 1.009], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    890/150000: episode: 55, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.000 [2.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    987/150000: episode: 56, duration: 0.501s, episode steps: 97, steps per second: 194, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 11.619 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.374], loss: --, mean_absolute_error: --, mean_q: --\n",
            "    999/150000: episode: 57, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1012/150000: episode: 58, duration: 0.175s, episode steps: 13, steps per second: 74, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 12.077 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.337], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1117/150000: episode: 59, duration: 0.514s, episode steps: 105, steps per second: 204, episode reward: 0.200, mean reward: 0.002 [0.000, 0.200], mean action: 12.371 [3.000, 20.000], mean observation: -0.635 [-1.011, 1.550], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1129/150000: episode: 60, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1141/150000: episode: 61, duration: 0.164s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.917 [2.000, 12.000], mean observation: -0.636 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1153/150000: episode: 62, duration: 0.164s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.833 [5.000, 17.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1165/150000: episode: 63, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.041, 1.009], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1178/150000: episode: 64, duration: 0.172s, episode steps: 13, steps per second: 76, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 10.692 [0.000, 14.000], mean observation: -0.635 [-1.011, 1.012], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1190/150000: episode: 65, duration: 0.162s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.000 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.199], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1203/150000: episode: 66, duration: 0.160s, episode steps: 13, steps per second: 81, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 10.692 [4.000, 12.000], mean observation: -0.635 [-1.011, 1.008], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1213/150000: episode: 67, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.700 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.014], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1225/150000: episode: 68, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.282], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1237/150000: episode: 69, duration: 0.165s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.750 [5.000, 16.000], mean observation: -0.636 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1249/150000: episode: 70, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1261/150000: episode: 71, duration: 0.161s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [1.000, 15.000], mean observation: -0.635 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1273/150000: episode: 72, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1286/150000: episode: 73, duration: 0.169s, episode steps: 13, steps per second: 77, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 10.846 [4.000, 12.000], mean observation: -0.636 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1301/150000: episode: 74, duration: 0.162s, episode steps: 15, steps per second: 92, episode reward: 2.000, mean reward: 0.133 [0.000, 1.100], mean action: 11.333 [3.000, 18.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1313/150000: episode: 75, duration: 0.169s, episode steps: 12, steps per second: 71, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1417/150000: episode: 76, duration: 0.524s, episode steps: 104, steps per second: 198, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 11.971 [1.000, 20.000], mean observation: -0.633 [-1.011, 1.013], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1426/150000: episode: 77, duration: 0.143s, episode steps: 9, steps per second: 63, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.000 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1438/150000: episode: 78, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.583 [5.000, 14.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1450/150000: episode: 79, duration: 0.170s, episode steps: 12, steps per second: 71, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.637 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1462/150000: episode: 80, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.083 [0.000, 12.000], mean observation: -0.636 [-1.040, 1.011], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1474/150000: episode: 81, duration: 0.155s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1487/150000: episode: 82, duration: 0.166s, episode steps: 13, steps per second: 78, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 10.769 [3.000, 12.000], mean observation: -0.636 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1499/150000: episode: 83, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.041, 1.010], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1511/150000: episode: 84, duration: 0.163s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.250 [4.000, 18.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1523/150000: episode: 85, duration: 0.149s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.167 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.206], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1536/150000: episode: 86, duration: 0.168s, episode steps: 13, steps per second: 77, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.308 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.014], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1546/150000: episode: 87, duration: 0.150s, episode steps: 10, steps per second: 67, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.200 [3.000, 12.000], mean observation: -0.635 [-1.011, 1.017], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1558/150000: episode: 88, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1570/150000: episode: 89, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1582/150000: episode: 90, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.833 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1594/150000: episode: 91, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1606/150000: episode: 92, duration: 0.175s, episode steps: 12, steps per second: 69, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.167 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1616/150000: episode: 93, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.200 [5.000, 15.000], mean observation: -0.633 [-1.011, 1.064], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1626/150000: episode: 94, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.300 [5.000, 15.000], mean observation: -0.634 [-1.011, 1.015], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1638/150000: episode: 95, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1650/150000: episode: 96, duration: 0.159s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1662/150000: episode: 97, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.375], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1674/150000: episode: 98, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1684/150000: episode: 99, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.800 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.019], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1696/150000: episode: 100, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.041, 1.009], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1708/150000: episode: 101, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.006], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1721/150000: episode: 102, duration: 0.161s, episode steps: 13, steps per second: 81, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1733/150000: episode: 103, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.917 [5.000, 12.000], mean observation: -0.635 [-1.035, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1748/150000: episode: 104, duration: 0.173s, episode steps: 15, steps per second: 87, episode reward: 2.000, mean reward: 0.133 [0.000, 1.100], mean action: 11.267 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.543], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1760/150000: episode: 105, duration: 0.161s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1772/150000: episode: 106, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.500 [1.000, 12.000], mean observation: -0.634 [-1.011, 1.316], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1784/150000: episode: 107, duration: 0.155s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1801/150000: episode: 108, duration: 0.202s, episode steps: 17, steps per second: 84, episode reward: 0.900, mean reward: 0.053 [0.000, 0.900], mean action: 11.941 [5.000, 20.000], mean observation: -0.632 [-1.009, 1.397], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1814/150000: episode: 109, duration: 0.157s, episode steps: 13, steps per second: 83, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 12.077 [4.000, 19.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1826/150000: episode: 110, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1863/150000: episode: 111, duration: 0.294s, episode steps: 37, steps per second: 126, episode reward: 0.900, mean reward: 0.024 [0.000, 0.900], mean action: 11.865 [2.000, 16.000], mean observation: -0.634 [-1.009, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1875/150000: episode: 112, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.417 [5.000, 20.000], mean observation: -0.635 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1888/150000: episode: 113, duration: 0.166s, episode steps: 13, steps per second: 79, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.923 [5.000, 18.000], mean observation: -0.635 [-1.011, 1.013], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1900/150000: episode: 114, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.006], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1913/150000: episode: 115, duration: 0.158s, episode steps: 13, steps per second: 83, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 10.923 [2.000, 15.000], mean observation: -0.635 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1925/150000: episode: 116, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1938/150000: episode: 117, duration: 0.175s, episode steps: 13, steps per second: 74, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.077 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   1950/150000: episode: 118, duration: 0.153s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2082/150000: episode: 119, duration: 0.625s, episode steps: 132, steps per second: 211, episode reward: 2.000, mean reward: 0.015 [0.000, 1.100], mean action: 11.652 [0.000, 20.000], mean observation: -0.631 [-1.011, 1.151], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2094/150000: episode: 120, duration: 0.148s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2168/150000: episode: 121, duration: 0.392s, episode steps: 74, steps per second: 189, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 11.446 [1.000, 15.000], mean observation: -0.635 [-1.011, 1.125], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2177/150000: episode: 122, duration: 0.152s, episode steps: 9, steps per second: 59, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.333 [5.000, 13.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2189/150000: episode: 123, duration: 0.161s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.333 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2201/150000: episode: 124, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.083 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.302], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2213/150000: episode: 125, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.082], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2225/150000: episode: 126, duration: 0.165s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [0.000, 12.000], mean observation: -0.634 [-1.011, 1.189], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2237/150000: episode: 127, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2249/150000: episode: 128, duration: 0.155s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2261/150000: episode: 129, duration: 0.157s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [0.000, 12.000], mean observation: -0.634 [-1.011, 1.333], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2273/150000: episode: 130, duration: 0.148s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.000 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.091], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2285/150000: episode: 131, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.167 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.006], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2295/150000: episode: 132, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.600 [5.000, 15.000], mean observation: -0.635 [-1.011, 1.017], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2307/150000: episode: 133, duration: 0.153s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.062], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2319/150000: episode: 134, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.000 [5.000, 18.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2331/150000: episode: 135, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.062], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2343/150000: episode: 136, duration: 0.159s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2353/150000: episode: 137, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.800 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.014], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2365/150000: episode: 138, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2375/150000: episode: 139, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.000 [5.000, 15.000], mean observation: -0.636 [-1.011, 1.015], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2387/150000: episode: 140, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2463/150000: episode: 141, duration: 0.387s, episode steps: 76, steps per second: 196, episode reward: 0.100, mean reward: 0.001 [0.000, 0.100], mean action: 12.039 [1.000, 20.000], mean observation: -0.635 [-1.009, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2475/150000: episode: 142, duration: 0.157s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.583 [5.000, 14.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2487/150000: episode: 143, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.250], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2499/150000: episode: 144, duration: 0.165s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.637 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2511/150000: episode: 145, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.051], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2523/150000: episode: 146, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.307], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2538/150000: episode: 147, duration: 0.167s, episode steps: 15, steps per second: 90, episode reward: 2.000, mean reward: 0.133 [0.000, 2.000], mean action: 11.933 [6.000, 17.000], mean observation: -0.634 [-1.011, 1.168], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2641/150000: episode: 148, duration: 0.538s, episode steps: 103, steps per second: 191, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 11.641 [2.000, 20.000], mean observation: -0.633 [-1.009, 1.210], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2653/150000: episode: 149, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.333 [5.000, 19.000], mean observation: -0.636 [-1.041, 1.012], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2663/150000: episode: 150, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.500 [5.000, 14.000], mean observation: -0.633 [-1.042, 1.267], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2672/150000: episode: 151, duration: 0.147s, episode steps: 9, steps per second: 61, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.444 [5.000, 14.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2684/150000: episode: 152, duration: 0.163s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [4.000, 12.000], mean observation: -0.635 [-1.011, 1.004], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2696/150000: episode: 153, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2708/150000: episode: 154, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [4.000, 12.000], mean observation: -0.634 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2720/150000: episode: 155, duration: 0.164s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.167 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2732/150000: episode: 156, duration: 0.161s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.215], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2742/150000: episode: 157, duration: 0.123s, episode steps: 10, steps per second: 82, episode reward: 2.000, mean reward: 0.200 [0.000, 2.000], mean action: 9.200 [0.000, 17.000], mean observation: -0.633 [-1.009, 1.019], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2754/150000: episode: 158, duration: 0.161s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.006], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2766/150000: episode: 159, duration: 0.153s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2775/150000: episode: 160, duration: 0.144s, episode steps: 9, steps per second: 63, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 9.667 [0.000, 12.000], mean observation: -0.636 [-1.039, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2787/150000: episode: 161, duration: 0.170s, episode steps: 12, steps per second: 71, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.083 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2799/150000: episode: 162, duration: 0.157s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2811/150000: episode: 163, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.297], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2823/150000: episode: 164, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [0.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2833/150000: episode: 165, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.900 [5.000, 18.000], mean observation: -0.636 [-1.011, 1.018], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2845/150000: episode: 166, duration: 0.162s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2857/150000: episode: 167, duration: 0.153s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.917 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.080], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2866/150000: episode: 168, duration: 0.157s, episode steps: 9, steps per second: 57, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 12.111 [5.000, 20.000], mean observation: -0.636 [-1.039, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2878/150000: episode: 169, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.083 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.353], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2890/150000: episode: 170, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   2902/150000: episode: 171, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.250 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.418], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3041/150000: episode: 172, duration: 0.645s, episode steps: 139, steps per second: 216, episode reward: 2.000, mean reward: 0.014 [0.000, 1.000], mean action: 11.856 [5.000, 15.000], mean observation: -0.629 [-1.011, 1.187], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3054/150000: episode: 173, duration: 0.155s, episode steps: 13, steps per second: 84, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.462 [4.000, 15.000], mean observation: -0.635 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3066/150000: episode: 174, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.105], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3078/150000: episode: 175, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3087/150000: episode: 176, duration: 0.145s, episode steps: 9, steps per second: 62, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.000 [5.000, 12.000], mean observation: -0.637 [-1.039, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3099/150000: episode: 177, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.833 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.098], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3111/150000: episode: 178, duration: 0.170s, episode steps: 12, steps per second: 71, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.000 [3.000, 16.000], mean observation: -0.635 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3123/150000: episode: 179, duration: 0.149s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.833 [5.000, 17.000], mean observation: -0.633 [-1.011, 1.495], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3135/150000: episode: 180, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3148/150000: episode: 181, duration: 0.178s, episode steps: 13, steps per second: 73, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.000 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.093], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3160/150000: episode: 182, duration: 0.162s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [0.000, 12.000], mean observation: -0.636 [-1.041, 1.013], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3169/150000: episode: 183, duration: 0.121s, episode steps: 9, steps per second: 74, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.000 [5.000, 12.000], mean observation: -0.635 [-1.009, 1.021], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3181/150000: episode: 184, duration: 0.155s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.083 [5.000, 12.000], mean observation: -0.636 [-1.041, 1.010], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3193/150000: episode: 185, duration: 0.169s, episode steps: 12, steps per second: 71, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3205/150000: episode: 186, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3217/150000: episode: 187, duration: 0.155s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.000 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3226/150000: episode: 188, duration: 0.130s, episode steps: 9, steps per second: 69, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.444 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.024], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3239/150000: episode: 189, duration: 0.154s, episode steps: 13, steps per second: 84, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 11.308 [5.000, 15.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3251/150000: episode: 190, duration: 0.159s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.583 [5.000, 14.000], mean observation: -0.635 [-1.011, 1.006], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3263/150000: episode: 191, duration: 0.163s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.004], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3275/150000: episode: 192, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.339], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3285/150000: episode: 193, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.500 [5.000, 19.000], mean observation: -0.636 [-1.011, 1.016], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3300/150000: episode: 194, duration: 0.166s, episode steps: 15, steps per second: 90, episode reward: 2.000, mean reward: 0.133 [0.000, 2.000], mean action: 12.867 [12.000, 19.000], mean observation: -0.631 [-1.011, 2.037], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3312/150000: episode: 195, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 14.000], mean observation: -0.635 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3324/150000: episode: 196, duration: 0.161s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.667 [3.000, 12.000], mean observation: -0.636 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3336/150000: episode: 197, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.583 [5.000, 18.000], mean observation: -0.636 [-1.041, 1.013], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3348/150000: episode: 198, duration: 0.168s, episode steps: 12, steps per second: 72, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.637 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3360/150000: episode: 199, duration: 0.149s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3369/150000: episode: 200, duration: 0.144s, episode steps: 9, steps per second: 62, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 10.111 [0.000, 14.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3381/150000: episode: 201, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3393/150000: episode: 202, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3405/150000: episode: 203, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3417/150000: episode: 204, duration: 0.169s, episode steps: 12, steps per second: 71, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3429/150000: episode: 205, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [0.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3443/150000: episode: 206, duration: 0.165s, episode steps: 14, steps per second: 85, episode reward: 2.000, mean reward: 0.143 [0.000, 2.000], mean action: 11.786 [4.000, 19.000], mean observation: -0.632 [-1.040, 1.863], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3455/150000: episode: 207, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.667 [5.000, 15.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3588/150000: episode: 208, duration: 0.627s, episode steps: 133, steps per second: 212, episode reward: 2.000, mean reward: 0.015 [0.000, 1.100], mean action: 11.617 [1.000, 18.000], mean observation: -0.631 [-1.011, 1.207], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3598/150000: episode: 209, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.400 [4.000, 20.000], mean observation: -0.634 [-1.011, 1.015], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3609/150000: episode: 210, duration: 0.136s, episode steps: 11, steps per second: 81, episode reward: 2.000, mean reward: 0.182 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.635 [-1.009, 1.020], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3621/150000: episode: 211, duration: 0.153s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.917 [3.000, 15.000], mean observation: -0.635 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3633/150000: episode: 212, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.083 [5.000, 14.000], mean observation: -0.635 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3645/150000: episode: 213, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [0.000, 12.000], mean observation: -0.634 [-1.011, 1.068], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3658/150000: episode: 214, duration: 0.166s, episode steps: 13, steps per second: 78, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.077 [2.000, 17.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3670/150000: episode: 215, duration: 0.159s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [4.000, 12.000], mean observation: -0.634 [-1.011, 1.004], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3682/150000: episode: 216, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.667 [0.000, 16.000], mean observation: -0.636 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3695/150000: episode: 217, duration: 0.152s, episode steps: 13, steps per second: 86, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 10.923 [5.000, 12.000], mean observation: -0.635 [-1.041, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3704/150000: episode: 218, duration: 0.132s, episode steps: 9, steps per second: 68, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.021], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3714/150000: episode: 219, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.400 [3.000, 12.000], mean observation: -0.636 [-1.011, 1.018], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3726/150000: episode: 220, duration: 0.159s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [3.000, 15.000], mean observation: -0.636 [-1.011, 1.008], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3739/150000: episode: 221, duration: 0.172s, episode steps: 13, steps per second: 75, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.000 [0.000, 18.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3751/150000: episode: 222, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3763/150000: episode: 223, duration: 0.153s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.833 [1.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3775/150000: episode: 224, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 13.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3787/150000: episode: 225, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.500 [1.000, 12.000], mean observation: -0.634 [-1.011, 1.113], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3799/150000: episode: 226, duration: 0.161s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3811/150000: episode: 227, duration: 0.151s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.667 [5.000, 15.000], mean observation: -0.634 [-1.011, 1.142], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3823/150000: episode: 228, duration: 0.167s, episode steps: 12, steps per second: 72, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.083 [0.000, 20.000], mean observation: -0.636 [-1.041, 1.014], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3837/150000: episode: 229, duration: 0.159s, episode steps: 14, steps per second: 88, episode reward: 2.000, mean reward: 0.143 [0.000, 2.000], mean action: 12.714 [12.000, 19.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3849/150000: episode: 230, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [0.000, 12.000], mean observation: -0.636 [-1.041, 1.009], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3869/150000: episode: 231, duration: 0.183s, episode steps: 20, steps per second: 109, episode reward: 2.000, mean reward: 0.100 [0.000, 1.100], mean action: 11.100 [1.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3883/150000: episode: 232, duration: 0.162s, episode steps: 14, steps per second: 86, episode reward: 2.000, mean reward: 0.143 [0.000, 2.000], mean action: 11.429 [0.000, 16.000], mean observation: -0.635 [-1.011, 1.013], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3895/150000: episode: 233, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3907/150000: episode: 234, duration: 0.186s, episode steps: 12, steps per second: 64, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.083 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3917/150000: episode: 235, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.500 [4.000, 12.000], mean observation: -0.635 [-1.040, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3929/150000: episode: 236, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3946/150000: episode: 237, duration: 0.205s, episode steps: 17, steps per second: 83, episode reward: 0.900, mean reward: 0.053 [0.000, 0.900], mean action: 11.176 [3.000, 14.000], mean observation: -0.635 [-1.009, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3958/150000: episode: 238, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.167 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3970/150000: episode: 239, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.833 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.288], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3982/150000: episode: 240, duration: 0.163s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.833 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   3994/150000: episode: 241, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.124], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4006/150000: episode: 242, duration: 0.149s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [2.000, 14.000], mean observation: -0.636 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4020/150000: episode: 243, duration: 0.163s, episode steps: 14, steps per second: 86, episode reward: 2.000, mean reward: 0.143 [0.000, 1.000], mean action: 12.786 [5.000, 20.000], mean observation: -0.635 [-1.041, 1.017], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4032/150000: episode: 244, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4101/150000: episode: 245, duration: 0.391s, episode steps: 69, steps per second: 177, episode reward: 0.200, mean reward: 0.003 [0.000, 0.200], mean action: 11.681 [0.000, 18.000], mean observation: -0.637 [-1.009, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4113/150000: episode: 246, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.286], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4125/150000: episode: 247, duration: 0.159s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [4.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4137/150000: episode: 248, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.167 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4149/150000: episode: 249, duration: 0.153s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4161/150000: episode: 250, duration: 0.155s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4173/150000: episode: 251, duration: 0.159s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.636 [-1.041, 1.009], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4186/150000: episode: 252, duration: 0.165s, episode steps: 13, steps per second: 79, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.615 [3.000, 19.000], mean observation: -0.635 [-1.045, 1.072], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4198/150000: episode: 253, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.583 [2.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4210/150000: episode: 254, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.083 [5.000, 20.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4219/150000: episode: 255, duration: 0.128s, episode steps: 9, steps per second: 70, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 10.111 [5.000, 12.000], mean observation: -0.635 [-1.009, 1.021], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4234/150000: episode: 256, duration: 0.163s, episode steps: 15, steps per second: 92, episode reward: 2.000, mean reward: 0.133 [0.000, 1.100], mean action: 10.867 [2.000, 12.000], mean observation: -0.635 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4246/150000: episode: 257, duration: 0.155s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.833 [0.000, 12.000], mean observation: -0.635 [-1.041, 1.009], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4258/150000: episode: 258, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.004], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4268/150000: episode: 259, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.700 [5.000, 15.000], mean observation: -0.632 [-1.011, 1.449], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4280/150000: episode: 260, duration: 0.149s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.833 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4292/150000: episode: 261, duration: 0.161s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [0.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4304/150000: episode: 262, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.917 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4316/150000: episode: 263, duration: 0.159s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.833 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4329/150000: episode: 264, duration: 0.154s, episode steps: 13, steps per second: 84, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.000 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4341/150000: episode: 265, duration: 0.167s, episode steps: 12, steps per second: 72, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.212], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4353/150000: episode: 266, duration: 0.155s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.750 [1.000, 12.000], mean observation: -0.633 [-1.011, 1.372], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4365/150000: episode: 267, duration: 0.159s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [4.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4377/150000: episode: 268, duration: 0.149s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.282], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4389/150000: episode: 269, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.083 [5.000, 20.000], mean observation: -0.633 [-1.011, 1.381], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4398/150000: episode: 270, duration: 0.122s, episode steps: 9, steps per second: 74, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.000 [5.000, 16.000], mean observation: -0.633 [-1.011, 1.020], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4522/150000: episode: 271, duration: 0.639s, episode steps: 124, steps per second: 194, episode reward: 0.900, mean reward: 0.007 [0.000, 0.900], mean action: 11.363 [1.000, 20.000], mean observation: -0.632 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4534/150000: episode: 272, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4545/150000: episode: 273, duration: 0.154s, episode steps: 11, steps per second: 72, episode reward: 2.000, mean reward: 0.182 [0.000, 2.000], mean action: 10.545 [1.000, 17.000], mean observation: -0.636 [-1.011, 1.008], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4558/150000: episode: 274, duration: 0.155s, episode steps: 13, steps per second: 84, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 12.077 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.009], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4573/150000: episode: 275, duration: 0.182s, episode steps: 15, steps per second: 82, episode reward: 2.000, mean reward: 0.133 [0.000, 2.000], mean action: 12.133 [8.000, 18.000], mean observation: -0.632 [-1.036, 1.744], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4586/150000: episode: 276, duration: 0.157s, episode steps: 13, steps per second: 83, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 10.846 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.015], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4598/150000: episode: 277, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.250 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4610/150000: episode: 278, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.917 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.491], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4622/150000: episode: 279, duration: 0.157s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.333 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4634/150000: episode: 280, duration: 0.155s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4646/150000: episode: 281, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.333 [5.000, 18.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4656/150000: episode: 282, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.000 [3.000, 18.000], mean observation: -0.636 [-1.011, 1.017], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4668/150000: episode: 283, duration: 0.159s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.212], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4680/150000: episode: 284, duration: 0.153s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.833 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4690/150000: episode: 285, duration: 0.151s, episode steps: 10, steps per second: 66, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.100 [5.000, 17.000], mean observation: -0.636 [-1.041, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4702/150000: episode: 286, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4714/150000: episode: 287, duration: 0.159s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4840/150000: episode: 288, duration: 0.608s, episode steps: 126, steps per second: 207, episode reward: 2.000, mean reward: 0.016 [0.000, 1.000], mean action: 11.690 [0.000, 20.000], mean observation: -0.631 [-1.009, 1.021], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4852/150000: episode: 289, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.004], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4864/150000: episode: 290, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.583 [5.000, 14.000], mean observation: -0.636 [-1.041, 1.016], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4876/150000: episode: 291, duration: 0.149s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.750 [5.000, 16.000], mean observation: -0.636 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4886/150000: episode: 292, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.800 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.014], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4898/150000: episode: 293, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [0.000, 12.000], mean observation: -0.636 [-1.041, 1.012], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4910/150000: episode: 294, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [4.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4922/150000: episode: 295, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.667 [4.000, 12.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4934/150000: episode: 296, duration: 0.155s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4946/150000: episode: 297, duration: 0.153s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.311], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4958/150000: episode: 298, duration: 0.153s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4968/150000: episode: 299, duration: 0.150s, episode steps: 10, steps per second: 66, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.100 [4.000, 12.000], mean observation: -0.632 [-1.011, 1.452], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4980/150000: episode: 300, duration: 0.162s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   4992/150000: episode: 301, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5004/150000: episode: 302, duration: 0.159s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.750 [5.000, 16.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5014/150000: episode: 303, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 12.100 [5.000, 20.000], mean observation: -0.633 [-1.011, 1.119], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5031/150000: episode: 304, duration: 0.193s, episode steps: 17, steps per second: 88, episode reward: 2.000, mean reward: 0.118 [0.000, 1.100], mean action: 11.765 [8.000, 12.000], mean observation: -0.632 [-1.011, 2.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5043/150000: episode: 305, duration: 0.166s, episode steps: 12, steps per second: 72, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.667 [5.000, 15.000], mean observation: -0.636 [-1.041, 1.011], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5153/150000: episode: 306, duration: 0.498s, episode steps: 110, steps per second: 221, episode reward: 2.000, mean reward: 0.018 [0.000, 1.000], mean action: 11.700 [1.000, 18.000], mean observation: -0.631 [-1.011, 1.493], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5165/150000: episode: 307, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.750 [5.000, 19.000], mean observation: -0.636 [-1.011, 1.006], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5177/150000: episode: 308, duration: 0.159s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.583 [5.000, 20.000], mean observation: -0.637 [-1.041, 1.013], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5189/150000: episode: 309, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5202/150000: episode: 310, duration: 0.161s, episode steps: 13, steps per second: 81, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.154 [4.000, 16.000], mean observation: -0.634 [-1.011, 1.381], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5214/150000: episode: 311, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.041, 1.010], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5226/150000: episode: 312, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5238/150000: episode: 313, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5251/150000: episode: 314, duration: 0.161s, episode steps: 13, steps per second: 81, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.308 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.004], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5265/150000: episode: 315, duration: 0.183s, episode steps: 14, steps per second: 76, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 12.286 [12.000, 16.000], mean observation: -0.628 [-1.011, 2.844], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5277/150000: episode: 316, duration: 0.168s, episode steps: 12, steps per second: 72, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.500 [4.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5289/150000: episode: 317, duration: 0.153s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.083 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5301/150000: episode: 318, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.333 [3.000, 12.000], mean observation: -0.635 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5313/150000: episode: 319, duration: 0.155s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.917 [2.000, 14.000], mean observation: -0.634 [-1.011, 1.351], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5325/150000: episode: 320, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.637 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5337/150000: episode: 321, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.333 [0.000, 14.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5347/150000: episode: 322, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.900 [5.000, 18.000], mean observation: -0.633 [-1.011, 1.044], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5359/150000: episode: 323, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5371/150000: episode: 324, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 20.000], mean observation: -0.637 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5380/150000: episode: 325, duration: 0.120s, episode steps: 9, steps per second: 75, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 9.111 [0.000, 12.000], mean observation: -0.635 [-1.011, 1.022], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5392/150000: episode: 326, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.500 [1.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5404/150000: episode: 327, duration: 0.155s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.667 [3.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5416/150000: episode: 328, duration: 0.159s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.583 [5.000, 14.000], mean observation: -0.636 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5428/150000: episode: 329, duration: 0.164s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5440/150000: episode: 330, duration: 0.165s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.583 [5.000, 14.000], mean observation: -0.636 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5450/150000: episode: 331, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.200 [1.000, 12.000], mean observation: -0.635 [-1.038, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5465/150000: episode: 332, duration: 0.167s, episode steps: 15, steps per second: 90, episode reward: 2.000, mean reward: 0.133 [0.000, 2.000], mean action: 12.333 [12.000, 17.000], mean observation: -0.635 [-1.036, 1.006], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5474/150000: episode: 333, duration: 0.120s, episode steps: 9, steps per second: 75, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.333 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.020], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5487/150000: episode: 334, duration: 0.159s, episode steps: 13, steps per second: 82, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 10.462 [0.000, 12.000], mean observation: -0.634 [-1.011, 1.027], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5499/150000: episode: 335, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5549/150000: episode: 336, duration: 0.322s, episode steps: 50, steps per second: 155, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 11.820 [5.000, 12.000], mean observation: -0.611 [-1.009, 7.668], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5559/150000: episode: 337, duration: 0.143s, episode steps: 10, steps per second: 70, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.200 [1.000, 12.000], mean observation: -0.634 [-1.011, 1.012], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5569/150000: episode: 338, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.700 [5.000, 16.000], mean observation: -0.631 [-1.041, 1.991], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5581/150000: episode: 339, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5593/150000: episode: 340, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5737/150000: episode: 341, duration: 0.651s, episode steps: 144, steps per second: 221, episode reward: 0.900, mean reward: 0.006 [0.000, 0.900], mean action: 11.708 [0.000, 19.000], mean observation: -0.631 [-1.009, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5749/150000: episode: 342, duration: 0.145s, episode steps: 12, steps per second: 83, episode reward: 2.000, mean reward: 0.167 [0.000, 2.000], mean action: 12.833 [12.000, 18.000], mean observation: -0.631 [-1.041, 1.748], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5761/150000: episode: 343, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5773/150000: episode: 344, duration: 0.151s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [0.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5785/150000: episode: 345, duration: 0.169s, episode steps: 12, steps per second: 71, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5795/150000: episode: 346, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.800 [1.000, 18.000], mean observation: -0.634 [-1.011, 1.009], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5808/150000: episode: 347, duration: 0.165s, episode steps: 13, steps per second: 79, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.231 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.013], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5820/150000: episode: 348, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5833/150000: episode: 349, duration: 0.163s, episode steps: 13, steps per second: 80, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 10.769 [3.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5845/150000: episode: 350, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.079], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5854/150000: episode: 351, duration: 0.153s, episode steps: 9, steps per second: 59, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 9.667 [2.000, 12.000], mean observation: -0.637 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5866/150000: episode: 352, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.750 [5.000, 16.000], mean observation: -0.636 [-1.040, 1.008], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5878/150000: episode: 353, duration: 0.157s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5891/150000: episode: 354, duration: 0.159s, episode steps: 13, steps per second: 82, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 9.923 [1.000, 12.000], mean observation: -0.637 [-1.040, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5903/150000: episode: 355, duration: 0.165s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.637 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5915/150000: episode: 356, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5927/150000: episode: 357, duration: 0.161s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5939/150000: episode: 358, duration: 0.166s, episode steps: 12, steps per second: 72, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.833 [5.000, 17.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5951/150000: episode: 359, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.249], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   5963/150000: episode: 360, duration: 0.155s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.008], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6094/150000: episode: 361, duration: 0.617s, episode steps: 131, steps per second: 212, episode reward: 2.000, mean reward: 0.015 [0.000, 1.100], mean action: 11.641 [0.000, 19.000], mean observation: -0.631 [-1.011, 1.147], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6106/150000: episode: 362, duration: 0.170s, episode steps: 12, steps per second: 70, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [0.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6118/150000: episode: 363, duration: 0.161s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.667 [3.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6254/150000: episode: 364, duration: 0.636s, episode steps: 136, steps per second: 214, episode reward: 2.000, mean reward: 0.015 [0.000, 1.000], mean action: 11.956 [0.000, 20.000], mean observation: -0.630 [-1.011, 1.058], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6266/150000: episode: 365, duration: 0.163s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.006], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6278/150000: episode: 366, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6290/150000: episode: 367, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.500 [1.000, 12.000], mean observation: -0.635 [-1.011, 1.008], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6302/150000: episode: 368, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.917 [1.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6314/150000: episode: 369, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.667 [1.000, 19.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6326/150000: episode: 370, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.583 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.062], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6338/150000: episode: 371, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6351/150000: episode: 372, duration: 0.170s, episode steps: 13, steps per second: 76, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.538 [5.000, 20.000], mean observation: -0.636 [-1.011, 1.008], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6363/150000: episode: 373, duration: 0.153s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6375/150000: episode: 374, duration: 0.163s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [0.000, 12.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6387/150000: episode: 375, duration: 0.147s, episode steps: 12, steps per second: 82, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6399/150000: episode: 376, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.750 [5.000, 16.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6411/150000: episode: 377, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.583 [5.000, 14.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6423/150000: episode: 378, duration: 0.164s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 13.000], mean observation: -0.636 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6436/150000: episode: 379, duration: 0.159s, episode steps: 13, steps per second: 82, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 12.000 [5.000, 16.000], mean observation: -0.636 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6448/150000: episode: 380, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.583 [2.000, 12.000], mean observation: -0.635 [-1.011, 1.004], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6460/150000: episode: 381, duration: 0.149s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.149], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6472/150000: episode: 382, duration: 0.162s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.917 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6484/150000: episode: 383, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.167 [5.000, 16.000], mean observation: -0.635 [-1.041, 1.011], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6494/150000: episode: 384, duration: 0.147s, episode steps: 10, steps per second: 68, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.700 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.017], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6506/150000: episode: 385, duration: 0.161s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6519/150000: episode: 386, duration: 0.157s, episode steps: 13, steps per second: 83, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.538 [5.000, 13.000], mean observation: -0.634 [-1.011, 1.305], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6531/150000: episode: 387, duration: 0.153s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6544/150000: episode: 388, duration: 0.158s, episode steps: 13, steps per second: 82, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 12.154 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.120], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6556/150000: episode: 389, duration: 0.149s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.250 [5.000, 15.000], mean observation: -0.634 [-1.011, 1.193], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6566/150000: episode: 390, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 12.700 [5.000, 20.000], mean observation: -0.636 [-1.011, 1.014], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6578/150000: episode: 391, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.576], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6590/150000: episode: 392, duration: 0.155s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.583 [2.000, 12.000], mean observation: -0.634 [-1.011, 1.239], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6602/150000: episode: 393, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6727/150000: episode: 394, duration: 0.582s, episode steps: 125, steps per second: 215, episode reward: 2.000, mean reward: 0.016 [0.000, 1.800], mean action: 11.568 [0.000, 20.000], mean observation: -0.632 [-1.011, 1.058], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6739/150000: episode: 395, duration: 0.165s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.500 [2.000, 17.000], mean observation: -0.636 [-1.011, 1.006], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6802/150000: episode: 396, duration: 0.394s, episode steps: 63, steps per second: 160, episode reward: 0.900, mean reward: 0.014 [0.000, 0.900], mean action: 12.063 [5.000, 20.000], mean observation: -0.618 [-1.011, 7.668], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6816/150000: episode: 397, duration: 0.155s, episode steps: 14, steps per second: 90, episode reward: 2.000, mean reward: 0.143 [0.000, 1.000], mean action: 10.000 [0.000, 12.000], mean observation: -0.635 [-1.040, 1.006], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6919/150000: episode: 398, duration: 0.527s, episode steps: 103, steps per second: 195, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 11.621 [3.000, 12.000], mean observation: -0.632 [-1.011, 1.517], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6931/150000: episode: 399, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.667 [5.000, 20.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6946/150000: episode: 400, duration: 0.181s, episode steps: 15, steps per second: 83, episode reward: 2.000, mean reward: 0.133 [0.000, 2.000], mean action: 12.133 [12.000, 14.000], mean observation: -0.636 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6958/150000: episode: 401, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.453], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6970/150000: episode: 402, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.343], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6979/150000: episode: 403, duration: 0.117s, episode steps: 9, steps per second: 77, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.333 [5.000, 13.000], mean observation: -0.634 [-1.009, 1.020], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   6988/150000: episode: 404, duration: 0.144s, episode steps: 9, steps per second: 62, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.889 [5.000, 18.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7000/150000: episode: 405, duration: 0.162s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.917 [5.000, 15.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7012/150000: episode: 406, duration: 0.161s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.917 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7024/150000: episode: 407, duration: 0.148s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [0.000, 12.000], mean observation: -0.634 [-1.011, 1.146], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7036/150000: episode: 408, duration: 0.162s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.333 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.032], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7048/150000: episode: 409, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.167 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7060/150000: episode: 410, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.016], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7072/150000: episode: 411, duration: 0.167s, episode steps: 12, steps per second: 72, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7085/150000: episode: 412, duration: 0.158s, episode steps: 13, steps per second: 82, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.538 [5.000, 15.000], mean observation: -0.634 [-1.044, 1.458], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7100/150000: episode: 413, duration: 0.161s, episode steps: 15, steps per second: 93, episode reward: 2.000, mean reward: 0.133 [0.000, 1.100], mean action: 10.867 [2.000, 12.000], mean observation: -0.632 [-1.011, 1.982], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7109/150000: episode: 414, duration: 0.126s, episode steps: 9, steps per second: 72, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 10.444 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.023], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7123/150000: episode: 415, duration: 0.137s, episode steps: 14, steps per second: 102, episode reward: 2.000, mean reward: 0.143 [0.000, 2.000], mean action: 12.429 [12.000, 18.000], mean observation: -0.633 [-1.009, 1.178], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7135/150000: episode: 416, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.750 [5.000, 18.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7147/150000: episode: 417, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [5.000, 12.000], mean observation: -0.636 [-1.041, 1.010], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7159/150000: episode: 418, duration: 0.164s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.833 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7169/150000: episode: 419, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.400 [3.000, 12.000], mean observation: -0.636 [-1.041, 1.008], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7181/150000: episode: 420, duration: 0.162s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.004], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7193/150000: episode: 421, duration: 0.153s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.167 [3.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7206/150000: episode: 422, duration: 0.167s, episode steps: 13, steps per second: 78, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 10.538 [2.000, 12.000], mean observation: -0.636 [-1.011, 1.006], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7218/150000: episode: 423, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.833 [0.000, 14.000], mean observation: -0.636 [-1.011, 1.004], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7232/150000: episode: 424, duration: 0.153s, episode steps: 14, steps per second: 91, episode reward: 2.000, mean reward: 0.143 [0.000, 2.000], mean action: 12.429 [12.000, 18.000], mean observation: -0.634 [-1.009, 1.019], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7242/150000: episode: 425, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.700 [3.000, 15.000], mean observation: -0.635 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7255/150000: episode: 426, duration: 0.163s, episode steps: 13, steps per second: 80, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 10.692 [2.000, 12.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7267/150000: episode: 427, duration: 0.151s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.583 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.154], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7279/150000: episode: 428, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7291/150000: episode: 429, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.333 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.056], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7303/150000: episode: 430, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.250 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7315/150000: episode: 431, duration: 0.162s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7327/150000: episode: 432, duration: 0.157s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.234], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7339/150000: episode: 433, duration: 0.148s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7354/150000: episode: 434, duration: 0.170s, episode steps: 15, steps per second: 88, episode reward: 2.000, mean reward: 0.133 [0.000, 2.000], mean action: 12.933 [12.000, 20.000], mean observation: -0.631 [-1.011, 2.047], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7366/150000: episode: 435, duration: 0.149s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.250 [0.000, 18.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7376/150000: episode: 436, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 12.100 [5.000, 20.000], mean observation: -0.633 [-1.011, 1.192], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7385/150000: episode: 437, duration: 0.121s, episode steps: 9, steps per second: 74, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 9.889 [0.000, 12.000], mean observation: -0.632 [-1.011, 1.108], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7397/150000: episode: 438, duration: 0.165s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [0.000, 12.000], mean observation: -0.636 [-1.041, 1.009], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7410/150000: episode: 439, duration: 0.156s, episode steps: 13, steps per second: 84, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.231 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.009], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7422/150000: episode: 440, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.033], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7434/150000: episode: 441, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7446/150000: episode: 442, duration: 0.155s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7458/150000: episode: 443, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.583 [3.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7468/150000: episode: 444, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.632 [-1.011, 1.479], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7480/150000: episode: 445, duration: 0.155s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7492/150000: episode: 446, duration: 0.159s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7502/150000: episode: 447, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 9.400 [1.000, 12.000], mean observation: -0.631 [-1.038, 2.043], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7512/150000: episode: 448, duration: 0.153s, episode steps: 10, steps per second: 65, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.200 [3.000, 12.000], mean observation: -0.635 [-1.011, 1.012], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7524/150000: episode: 449, duration: 0.149s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7536/150000: episode: 450, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.004], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7548/150000: episode: 451, duration: 0.162s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.667 [5.000, 15.000], mean observation: -0.634 [-1.011, 1.074], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7560/150000: episode: 452, duration: 0.155s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7569/150000: episode: 453, duration: 0.146s, episode steps: 9, steps per second: 62, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.222 [5.000, 19.000], mean observation: -0.636 [-1.039, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7581/150000: episode: 454, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7593/150000: episode: 455, duration: 0.147s, episode steps: 12, steps per second: 82, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 13.000], mean observation: -0.634 [-1.011, 1.263], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7611/150000: episode: 456, duration: 0.205s, episode steps: 18, steps per second: 88, episode reward: 0.900, mean reward: 0.050 [0.000, 0.900], mean action: 11.500 [1.000, 13.000], mean observation: -0.632 [-1.011, 1.740], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7623/150000: episode: 457, duration: 0.162s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.750 [5.000, 16.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7635/150000: episode: 458, duration: 0.159s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7647/150000: episode: 459, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.833 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7659/150000: episode: 460, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [4.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7788/150000: episode: 461, duration: 0.619s, episode steps: 129, steps per second: 209, episode reward: 0.900, mean reward: 0.007 [0.000, 0.900], mean action: 11.659 [0.000, 20.000], mean observation: -0.629 [-1.009, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7798/150000: episode: 462, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.400 [3.000, 12.000], mean observation: -0.636 [-1.011, 1.015], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7810/150000: episode: 463, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7819/150000: episode: 464, duration: 0.116s, episode steps: 9, steps per second: 78, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.333 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.025], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7829/150000: episode: 465, duration: 0.144s, episode steps: 10, steps per second: 69, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 12.000 [5.000, 17.000], mean observation: -0.635 [-1.044, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7841/150000: episode: 466, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7853/150000: episode: 467, duration: 0.161s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.333 [3.000, 15.000], mean observation: -0.636 [-1.041, 1.011], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7865/150000: episode: 468, duration: 0.161s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.167 [5.000, 18.000], mean observation: -0.635 [-1.011, 1.004], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7874/150000: episode: 469, duration: 0.124s, episode steps: 9, steps per second: 73, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 10.778 [5.000, 15.000], mean observation: -0.635 [-1.009, 1.020], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7886/150000: episode: 470, duration: 0.155s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.583 [5.000, 14.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7896/150000: episode: 471, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.600 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.011], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7908/150000: episode: 472, duration: 0.147s, episode steps: 12, steps per second: 82, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7925/150000: episode: 473, duration: 0.165s, episode steps: 17, steps per second: 103, episode reward: 2.000, mean reward: 0.118 [0.000, 1.000], mean action: 11.588 [4.000, 19.000], mean observation: -0.635 [-1.011, 1.024], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7940/150000: episode: 474, duration: 0.161s, episode steps: 15, steps per second: 93, episode reward: 2.000, mean reward: 0.133 [0.000, 2.000], mean action: 12.133 [12.000, 14.000], mean observation: -0.636 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7952/150000: episode: 475, duration: 0.162s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [4.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7964/150000: episode: 476, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7976/150000: episode: 477, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.917 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7986/150000: episode: 478, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.000 [5.000, 13.000], mean observation: -0.634 [-1.011, 1.017], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   7998/150000: episode: 479, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8010/150000: episode: 480, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.167 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8022/150000: episode: 481, duration: 0.173s, episode steps: 12, steps per second: 69, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.333 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.232], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8031/150000: episode: 482, duration: 0.142s, episode steps: 9, steps per second: 63, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.889 [5.000, 18.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8043/150000: episode: 483, duration: 0.170s, episode steps: 12, steps per second: 71, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8055/150000: episode: 484, duration: 0.174s, episode steps: 12, steps per second: 69, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.209], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8065/150000: episode: 485, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 12.500 [5.000, 20.000], mean observation: -0.633 [-1.011, 1.156], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8077/150000: episode: 486, duration: 0.151s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.333 [1.000, 20.000], mean observation: -0.636 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8089/150000: episode: 487, duration: 0.153s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [4.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8101/150000: episode: 488, duration: 0.159s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.583 [2.000, 12.000], mean observation: -0.635 [-1.011, 1.006], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8113/150000: episode: 489, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8125/150000: episode: 490, duration: 0.148s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.833 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.284], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8137/150000: episode: 491, duration: 0.153s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.833 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8149/150000: episode: 492, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8161/150000: episode: 493, duration: 0.151s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8170/150000: episode: 494, duration: 0.151s, episode steps: 9, steps per second: 59, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.778 [5.000, 17.000], mean observation: -0.637 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8182/150000: episode: 495, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.355], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8194/150000: episode: 496, duration: 0.151s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8206/150000: episode: 497, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.419], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8215/150000: episode: 498, duration: 0.118s, episode steps: 9, steps per second: 76, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.556 [5.000, 15.000], mean observation: -0.635 [-1.011, 1.022], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8227/150000: episode: 499, duration: 0.153s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.230], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8236/150000: episode: 500, duration: 0.120s, episode steps: 9, steps per second: 75, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 9.889 [0.000, 12.000], mean observation: -0.635 [-1.009, 1.022], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8248/150000: episode: 501, duration: 0.165s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8264/150000: episode: 502, duration: 0.167s, episode steps: 16, steps per second: 96, episode reward: 2.000, mean reward: 0.125 [0.000, 1.100], mean action: 10.125 [1.000, 12.000], mean observation: -0.634 [-1.040, 1.052], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8276/150000: episode: 503, duration: 0.165s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.083 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8288/150000: episode: 504, duration: 0.146s, episode steps: 12, steps per second: 82, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8301/150000: episode: 505, duration: 0.160s, episode steps: 13, steps per second: 81, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 10.462 [1.000, 12.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8313/150000: episode: 506, duration: 0.150s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.667 [5.000, 15.000], mean observation: -0.633 [-1.011, 1.438], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8325/150000: episode: 507, duration: 0.155s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.583 [2.000, 12.000], mean observation: -0.635 [-1.011, 1.011], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8438/150000: episode: 508, duration: 0.515s, episode steps: 113, steps per second: 219, episode reward: 2.000, mean reward: 0.018 [0.000, 1.800], mean action: 11.770 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.013], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8450/150000: episode: 509, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.167 [5.000, 20.000], mean observation: -0.636 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8462/150000: episode: 510, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 17.000], mean observation: -0.636 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8474/150000: episode: 511, duration: 0.147s, episode steps: 12, steps per second: 82, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8486/150000: episode: 512, duration: 0.171s, episode steps: 12, steps per second: 70, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.917 [5.000, 19.000], mean observation: -0.636 [-1.011, 1.008], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8495/150000: episode: 513, duration: 0.140s, episode steps: 9, steps per second: 64, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.667 [5.000, 16.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8504/150000: episode: 514, duration: 0.127s, episode steps: 9, steps per second: 71, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 10.111 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.024], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8516/150000: episode: 515, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.148], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8529/150000: episode: 516, duration: 0.161s, episode steps: 13, steps per second: 81, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.538 [5.000, 13.000], mean observation: -0.636 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8539/150000: episode: 517, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.100 [4.000, 19.000], mean observation: -0.633 [-1.040, 1.292], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8551/150000: episode: 518, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.636 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8560/150000: episode: 519, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.444 [5.000, 14.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8572/150000: episode: 520, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.006], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8581/150000: episode: 521, duration: 0.140s, episode steps: 9, steps per second: 64, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 12.111 [5.000, 20.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8593/150000: episode: 522, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.167 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8606/150000: episode: 523, duration: 0.153s, episode steps: 13, steps per second: 85, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 12.308 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.124], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8618/150000: episode: 524, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.667 [0.000, 20.000], mean observation: -0.635 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8630/150000: episode: 525, duration: 0.148s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.031], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8642/150000: episode: 526, duration: 0.171s, episode steps: 12, steps per second: 70, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.750 [5.000, 16.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8654/150000: episode: 527, duration: 0.148s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8666/150000: episode: 528, duration: 0.155s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.750 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.190], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8678/150000: episode: 529, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.092], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8690/150000: episode: 530, duration: 0.157s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [4.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8703/150000: episode: 531, duration: 0.153s, episode steps: 13, steps per second: 85, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.769 [5.000, 20.000], mean observation: -0.633 [-1.011, 1.625], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8715/150000: episode: 532, duration: 0.167s, episode steps: 12, steps per second: 72, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.500 [1.000, 12.000], mean observation: -0.635 [-1.011, 1.003], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8727/150000: episode: 533, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.637 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8875/150000: episode: 534, duration: 0.650s, episode steps: 148, steps per second: 228, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 11.736 [1.000, 20.000], mean observation: -0.628 [-1.009, 1.389], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8887/150000: episode: 535, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.159], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8986/150000: episode: 536, duration: 0.508s, episode steps: 99, steps per second: 195, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 11.889 [1.000, 20.000], mean observation: -0.631 [-1.009, 2.223], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   8998/150000: episode: 537, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.583 [2.000, 12.000], mean observation: -0.634 [-1.011, 1.093], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9010/150000: episode: 538, duration: 0.159s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.750 [5.000, 16.000], mean observation: -0.635 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9022/150000: episode: 539, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.198], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9037/150000: episode: 540, duration: 0.178s, episode steps: 15, steps per second: 84, episode reward: 2.000, mean reward: 0.133 [0.000, 2.000], mean action: 12.067 [12.000, 13.000], mean observation: -0.634 [-1.011, 1.167], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9047/150000: episode: 541, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.800 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.018], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9059/150000: episode: 542, duration: 0.159s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.667 [3.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9071/150000: episode: 543, duration: 0.148s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.917 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.408], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9121/150000: episode: 544, duration: 0.322s, episode steps: 50, steps per second: 155, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 11.960 [5.000, 20.000], mean observation: -0.611 [-1.009, 7.668], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9133/150000: episode: 545, duration: 0.159s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.167 [3.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9145/150000: episode: 546, duration: 0.161s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.583 [2.000, 12.000], mean observation: -0.635 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9157/150000: episode: 547, duration: 0.149s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.418], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9169/150000: episode: 548, duration: 0.163s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9182/150000: episode: 549, duration: 0.156s, episode steps: 13, steps per second: 83, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.538 [5.000, 13.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9195/150000: episode: 550, duration: 0.163s, episode steps: 13, steps per second: 80, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.692 [5.000, 15.000], mean observation: -0.636 [-1.011, 1.006], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9207/150000: episode: 551, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.667 [5.000, 15.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9217/150000: episode: 552, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.600 [5.000, 15.000], mean observation: -0.632 [-1.011, 1.435], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9312/150000: episode: 553, duration: 0.486s, episode steps: 95, steps per second: 196, episode reward: 0.200, mean reward: 0.002 [0.000, 0.200], mean action: 11.505 [0.000, 14.000], mean observation: -0.632 [-1.009, 1.712], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9324/150000: episode: 554, duration: 0.147s, episode steps: 12, steps per second: 82, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9336/150000: episode: 555, duration: 0.157s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [4.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9348/150000: episode: 556, duration: 0.157s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.085], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9360/150000: episode: 557, duration: 0.165s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.583 [2.000, 12.000], mean observation: -0.635 [-1.011, 1.001], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9372/150000: episode: 558, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9382/150000: episode: 559, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 2.000, mean reward: 0.200 [0.000, 2.000], mean action: 12.800 [6.000, 19.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9394/150000: episode: 560, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.250 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.043], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9406/150000: episode: 561, duration: 0.159s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.583 [5.000, 14.000], mean observation: -0.636 [-1.040, 1.010], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9419/150000: episode: 562, duration: 0.153s, episode steps: 13, steps per second: 85, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.009], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9429/150000: episode: 563, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 9.900 [3.000, 12.000], mean observation: -0.632 [-1.011, 1.523], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9442/150000: episode: 564, duration: 0.156s, episode steps: 13, steps per second: 83, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 10.615 [1.000, 12.000], mean observation: -0.635 [-1.011, 1.004], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9454/150000: episode: 565, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.583 [5.000, 14.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9466/150000: episode: 566, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.007], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9476/150000: episode: 567, duration: 0.145s, episode steps: 10, steps per second: 69, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.300 [3.000, 20.000], mean observation: -0.631 [-1.037, 1.699], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9486/150000: episode: 568, duration: 0.146s, episode steps: 10, steps per second: 68, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.800 [2.000, 17.000], mean observation: -0.635 [-1.011, 1.016], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9498/150000: episode: 569, duration: 0.161s, episode steps: 12, steps per second: 75, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9510/150000: episode: 570, duration: 0.163s, episode steps: 12, steps per second: 74, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9522/150000: episode: 571, duration: 0.165s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.344], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9534/150000: episode: 572, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.661], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9546/150000: episode: 573, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.284], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9558/150000: episode: 574, duration: 0.156s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9665/150000: episode: 575, duration: 0.547s, episode steps: 107, steps per second: 196, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 11.804 [0.000, 20.000], mean observation: -0.632 [-1.009, 1.571], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9678/150000: episode: 576, duration: 0.153s, episode steps: 13, steps per second: 85, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 10.538 [0.000, 12.000], mean observation: -0.635 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9690/150000: episode: 577, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.083 [5.000, 20.000], mean observation: -0.636 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9700/150000: episode: 578, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.400 [3.000, 12.000], mean observation: -0.633 [-1.011, 1.088], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9710/150000: episode: 579, duration: 0.152s, episode steps: 10, steps per second: 66, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.000 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.032], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9722/150000: episode: 580, duration: 0.167s, episode steps: 12, steps per second: 72, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.114], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9734/150000: episode: 581, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.637], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9746/150000: episode: 582, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9758/150000: episode: 583, duration: 0.164s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.667 [5.000, 18.000], mean observation: -0.635 [-1.011, 1.004], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9770/150000: episode: 584, duration: 0.149s, episode steps: 12, steps per second: 80, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.000 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.004], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9782/150000: episode: 585, duration: 0.164s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.250 [4.000, 12.000], mean observation: -0.636 [-1.011, 1.004], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9794/150000: episode: 586, duration: 0.148s, episode steps: 12, steps per second: 81, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.372], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9806/150000: episode: 587, duration: 0.167s, episode steps: 12, steps per second: 72, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.004], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9819/150000: episode: 588, duration: 0.157s, episode steps: 13, steps per second: 83, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.846 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.114], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9831/150000: episode: 589, duration: 0.154s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.158], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9843/150000: episode: 590, duration: 0.146s, episode steps: 12, steps per second: 82, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.249], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9860/150000: episode: 591, duration: 0.208s, episode steps: 17, steps per second: 82, episode reward: 0.900, mean reward: 0.053 [0.000, 0.900], mean action: 11.588 [3.000, 14.000], mean observation: -0.630 [-1.009, 2.679], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9874/150000: episode: 592, duration: 0.136s, episode steps: 14, steps per second: 103, episode reward: 2.000, mean reward: 0.143 [0.000, 2.000], mean action: 12.286 [12.000, 16.000], mean observation: -0.633 [-1.011, 1.021], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9886/150000: episode: 593, duration: 0.170s, episode steps: 12, steps per second: 71, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.008], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9898/150000: episode: 594, duration: 0.153s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9910/150000: episode: 595, duration: 0.155s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.917 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.005], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9922/150000: episode: 596, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 13.000], mean observation: -0.634 [-1.011, 1.033], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9934/150000: episode: 597, duration: 0.158s, episode steps: 12, steps per second: 76, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.667 [3.000, 12.000], mean observation: -0.636 [-1.011, 1.002], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9944/150000: episode: 598, duration: 0.156s, episode steps: 10, steps per second: 64, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 9.800 [1.000, 12.000], mean observation: -0.633 [-1.011, 1.047], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9956/150000: episode: 599, duration: 0.164s, episode steps: 12, steps per second: 73, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.333 [3.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9971/150000: episode: 600, duration: 0.166s, episode steps: 15, steps per second: 90, episode reward: 2.000, mean reward: 0.133 [0.000, 2.000], mean action: 12.467 [12.000, 19.000], mean observation: -0.632 [-1.011, 1.613], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9983/150000: episode: 601, duration: 0.155s, episode steps: 12, steps per second: 77, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.277], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   9995/150000: episode: 602, duration: 0.153s, episode steps: 12, steps per second: 78, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "  10142/150000: episode: 603, duration: 1.991s, episode steps: 147, steps per second: 74, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 6.367 [1.000, 20.000], mean observation: -0.633 [-1.011, 1.319], loss: 0.007825, mean_absolute_error: 0.879028, mean_q: 0.955015\n",
            "  10179/150000: episode: 604, duration: 0.589s, episode steps: 37, steps per second: 63, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 4.892 [1.000, 7.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.011283, mean_absolute_error: 0.890089, mean_q: 0.978733\n",
            "  10203/150000: episode: 605, duration: 0.445s, episode steps: 24, steps per second: 54, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 6.708 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.005483, mean_absolute_error: 0.906003, mean_q: 0.984422\n",
            "  10213/150000: episode: 606, duration: 0.256s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 6.800 [0.000, 14.000], mean observation: -0.633 [-1.011, 1.248], loss: 0.013764, mean_absolute_error: 0.910237, mean_q: 0.975834\n",
            "  10243/150000: episode: 607, duration: 0.516s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.900 [2.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006005, mean_absolute_error: 0.907726, mean_q: 0.982385\n",
            "  10273/150000: episode: 608, duration: 0.503s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006037, mean_absolute_error: 0.912188, mean_q: 0.986851\n",
            "  10308/150000: episode: 609, duration: 0.584s, episode steps: 35, steps per second: 60, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 5.314 [5.000, 13.000], mean observation: -0.634 [-1.011, 1.015], loss: 0.003991, mean_absolute_error: 0.915031, mean_q: 0.991191\n",
            "  10320/150000: episode: 610, duration: 0.281s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 7.417 [5.000, 12.000], mean observation: -0.636 [-1.041, 1.015], loss: 0.011558, mean_absolute_error: 0.917808, mean_q: 0.998384\n",
            "  10341/150000: episode: 611, duration: 0.403s, episode steps: 21, steps per second: 52, episode reward: 2.000, mean reward: 0.095 [0.000, 1.000], mean action: 5.905 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.010], loss: 0.010220, mean_absolute_error: 0.917770, mean_q: 1.004597\n",
            "  10354/150000: episode: 612, duration: 0.299s, episode steps: 13, steps per second: 43, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 7.000 [4.000, 12.000], mean observation: -0.635 [-1.011, 1.009], loss: 0.004313, mean_absolute_error: 0.921814, mean_q: 0.995651\n",
            "  10646/150000: episode: 613, duration: 3.959s, episode steps: 292, steps per second: 74, episode reward: 0.900, mean reward: 0.003 [0.000, 0.900], mean action: 5.777 [0.000, 20.000], mean observation: -0.636 [-1.011, 1.068], loss: 0.007137, mean_absolute_error: 0.948947, mean_q: 1.034892\n",
            "  10658/150000: episode: 614, duration: 0.281s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 8.667 [4.000, 19.000], mean observation: -0.635 [-1.039, 1.000], loss: 0.011459, mean_absolute_error: 0.973962, mean_q: 1.071862\n",
            "  10670/150000: episode: 615, duration: 0.261s, episode steps: 12, steps per second: 46, episode reward: 2.000, mean reward: 0.167 [0.000, 1.000], mean action: 5.167 [0.000, 12.000], mean observation: -0.635 [-1.011, 1.020], loss: 0.010734, mean_absolute_error: 0.973937, mean_q: 1.066703\n",
            "  10752/150000: episode: 616, duration: 1.243s, episode steps: 82, steps per second: 66, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 4.585 [1.000, 10.000], mean observation: -0.615 [-1.016, 8.380], loss: 0.007531, mean_absolute_error: 0.982415, mean_q: 1.070395\n",
            "  10826/150000: episode: 617, duration: 1.089s, episode steps: 74, steps per second: 68, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 5.419 [1.000, 20.000], mean observation: -0.637 [-1.009, 1.000], loss: 0.006973, mean_absolute_error: 0.989209, mean_q: 1.073507\n",
            "  11226/150000: episode: 618, duration: 5.299s, episode steps: 400, steps per second: 75, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 5.522 [0.000, 20.000], mean observation: -0.633 [-1.011, 8.272], loss: 0.007477, mean_absolute_error: 1.020378, mean_q: 1.105236\n",
            "  11264/150000: episode: 619, duration: 0.650s, episode steps: 38, steps per second: 58, episode reward: 1.000, mean reward: 0.026 [0.000, 0.900], mean action: 6.500 [3.000, 20.000], mean observation: -0.633 [-1.009, 1.016], loss: 0.009388, mean_absolute_error: 1.044678, mean_q: 1.125656\n",
            "  11300/150000: episode: 620, duration: 0.600s, episode steps: 36, steps per second: 60, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 5.333 [3.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.009444, mean_absolute_error: 1.048071, mean_q: 1.136472\n",
            "  11330/150000: episode: 621, duration: 0.514s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.733 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.006853, mean_absolute_error: 1.047559, mean_q: 1.142871\n",
            "  11355/150000: episode: 622, duration: 0.469s, episode steps: 25, steps per second: 53, episode reward: 2.000, mean reward: 0.080 [0.000, 1.000], mean action: 5.960 [5.000, 16.000], mean observation: -0.635 [-1.011, 1.005], loss: 0.006737, mean_absolute_error: 1.060718, mean_q: 1.147851\n",
            "  11583/150000: episode: 623, duration: 3.169s, episode steps: 228, steps per second: 72, episode reward: 0.600, mean reward: 0.003 [0.000, 0.200], mean action: 6.219 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.103], loss: 0.009422, mean_absolute_error: 1.073405, mean_q: 1.162671\n",
            "  11620/150000: episode: 624, duration: 0.588s, episode steps: 37, steps per second: 63, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.135 [2.000, 15.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.008126, mean_absolute_error: 1.091279, mean_q: 1.193562\n",
            "  11775/150000: episode: 625, duration: 2.151s, episode steps: 155, steps per second: 72, episode reward: 2.000, mean reward: 0.013 [0.000, 1.000], mean action: 3.368 [0.000, 16.000], mean observation: -0.629 [-1.011, 1.006], loss: 0.010150, mean_absolute_error: 1.101323, mean_q: 1.194811\n",
            "  11805/150000: episode: 626, duration: 0.537s, episode steps: 30, steps per second: 56, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.433 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.012245, mean_absolute_error: 1.114596, mean_q: 1.213474\n",
            "  11835/150000: episode: 627, duration: 0.507s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.300 [4.000, 15.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006993, mean_absolute_error: 1.109653, mean_q: 1.187750\n",
            "  11922/150000: episode: 628, duration: 1.282s, episode steps: 87, steps per second: 68, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 5.839 [0.000, 19.000], mean observation: -0.637 [-1.011, 1.029], loss: 0.009013, mean_absolute_error: 1.116463, mean_q: 1.212401\n",
            "  11955/150000: episode: 629, duration: 0.612s, episode steps: 33, steps per second: 54, episode reward: 1.000, mean reward: 0.030 [0.000, 0.900], mean action: 10.091 [5.000, 17.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.009078, mean_absolute_error: 1.126509, mean_q: 1.207757\n",
            "  11965/150000: episode: 630, duration: 0.259s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 6.900 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.007], loss: 0.017558, mean_absolute_error: 1.128206, mean_q: 1.199390\n",
            "  12074/150000: episode: 631, duration: 1.566s, episode steps: 109, steps per second: 70, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 5.927 [1.000, 20.000], mean observation: -0.631 [-1.009, 1.235], loss: 0.007902, mean_absolute_error: 1.130974, mean_q: 1.224010\n",
            "  12164/150000: episode: 632, duration: 1.384s, episode steps: 90, steps per second: 65, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 6.711 [1.000, 20.000], mean observation: -0.632 [-1.009, 1.263], loss: 0.009892, mean_absolute_error: 1.142371, mean_q: 1.236665\n",
            "  12272/150000: episode: 633, duration: 1.617s, episode steps: 108, steps per second: 67, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 9.954 [2.000, 20.000], mean observation: -0.629 [-1.011, 1.169], loss: 0.011235, mean_absolute_error: 1.151509, mean_q: 1.244851\n",
            "  12357/150000: episode: 634, duration: 1.227s, episode steps: 85, steps per second: 69, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 7.635 [0.000, 20.000], mean observation: -0.633 [-1.009, 1.204], loss: 0.014317, mean_absolute_error: 1.152132, mean_q: 1.248258\n",
            "  12375/150000: episode: 635, duration: 0.372s, episode steps: 18, steps per second: 48, episode reward: 2.000, mean reward: 0.111 [0.000, 1.000], mean action: 7.889 [5.000, 15.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.006984, mean_absolute_error: 1.152985, mean_q: 1.240737\n",
            "  12405/150000: episode: 636, duration: 0.514s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 7.533 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010494, mean_absolute_error: 1.157773, mean_q: 1.251926\n",
            "  12495/150000: episode: 637, duration: 1.273s, episode steps: 90, steps per second: 71, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 6.911 [0.000, 20.000], mean observation: -0.633 [-1.009, 1.000], loss: 0.009865, mean_absolute_error: 1.156706, mean_q: 1.247273\n",
            "  12587/150000: episode: 638, duration: 1.352s, episode steps: 92, steps per second: 68, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 7.065 [0.000, 20.000], mean observation: -0.632 [-1.009, 1.174], loss: 0.009722, mean_absolute_error: 1.150954, mean_q: 1.245154\n",
            "  12660/150000: episode: 639, duration: 1.108s, episode steps: 73, steps per second: 66, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 7.959 [0.000, 15.000], mean observation: -0.635 [-1.009, 1.000], loss: 0.009897, mean_absolute_error: 1.153872, mean_q: 1.241677\n",
            "  12812/150000: episode: 640, duration: 2.161s, episode steps: 152, steps per second: 70, episode reward: 2.000, mean reward: 0.013 [0.000, 1.000], mean action: 9.520 [1.000, 20.000], mean observation: -0.628 [-1.011, 1.229], loss: 0.009519, mean_absolute_error: 1.172903, mean_q: 1.278102\n",
            "  12907/150000: episode: 641, duration: 1.420s, episode steps: 95, steps per second: 67, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.358 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.338], loss: 0.007594, mean_absolute_error: 1.184156, mean_q: 1.280433\n",
            "  12935/150000: episode: 642, duration: 0.514s, episode steps: 28, steps per second: 54, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 6.679 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.012407, mean_absolute_error: 1.182696, mean_q: 1.299197\n",
            "  12952/150000: episode: 643, duration: 0.349s, episode steps: 17, steps per second: 49, episode reward: 2.000, mean reward: 0.118 [0.000, 1.000], mean action: 5.294 [3.000, 15.000], mean observation: -0.636 [-1.011, 1.014], loss: 0.010785, mean_absolute_error: 1.185459, mean_q: 1.283615\n",
            "  13006/150000: episode: 644, duration: 0.841s, episode steps: 54, steps per second: 64, episode reward: 0.900, mean reward: 0.017 [0.000, 0.900], mean action: 5.889 [1.000, 20.000], mean observation: -0.606 [-1.107, 9.637], loss: 0.010873, mean_absolute_error: 1.194715, mean_q: 1.283695\n",
            "  13128/150000: episode: 645, duration: 1.698s, episode steps: 122, steps per second: 72, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 5.730 [4.000, 17.000], mean observation: -0.634 [-1.011, 1.226], loss: 0.010990, mean_absolute_error: 1.197071, mean_q: 1.297539\n",
            "  13224/150000: episode: 646, duration: 1.356s, episode steps: 96, steps per second: 71, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 4.083 [1.000, 20.000], mean observation: -0.632 [-1.011, 1.000], loss: 0.011036, mean_absolute_error: 1.211434, mean_q: 1.308390\n",
            "  13261/150000: episode: 647, duration: 0.598s, episode steps: 37, steps per second: 62, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.892 [1.000, 16.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.013869, mean_absolute_error: 1.208799, mean_q: 1.301984\n",
            "  13355/150000: episode: 648, duration: 1.354s, episode steps: 94, steps per second: 69, episode reward: -0.100, mean reward: -0.001 [-1.000, 0.900], mean action: 5.436 [0.000, 18.000], mean observation: -0.625 [-1.022, 8.799], loss: 0.010966, mean_absolute_error: 1.210384, mean_q: 1.306083\n",
            "  13393/150000: episode: 649, duration: 0.648s, episode steps: 38, steps per second: 59, episode reward: 1.000, mean reward: 0.026 [0.000, 0.900], mean action: 5.842 [0.000, 20.000], mean observation: -0.632 [-1.009, 1.009], loss: 0.008721, mean_absolute_error: 1.210732, mean_q: 1.307188\n",
            "  13423/150000: episode: 650, duration: 0.521s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.333 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.012809, mean_absolute_error: 1.206538, mean_q: 1.303836\n",
            "  13455/150000: episode: 651, duration: 0.526s, episode steps: 32, steps per second: 61, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 13.406 [4.000, 18.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.009078, mean_absolute_error: 1.206591, mean_q: 1.298007\n",
            "  13492/150000: episode: 652, duration: 0.655s, episode steps: 37, steps per second: 56, episode reward: 1.000, mean reward: 0.027 [0.000, 0.900], mean action: 4.622 [3.000, 14.000], mean observation: -0.634 [-1.009, 1.006], loss: 0.014503, mean_absolute_error: 1.208966, mean_q: 1.313635\n",
            "  13559/150000: episode: 653, duration: 1.212s, episode steps: 67, steps per second: 55, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 3.642 [3.000, 17.000], mean observation: -0.636 [-1.011, 1.426], loss: 0.011275, mean_absolute_error: 1.204814, mean_q: 1.300112\n",
            "  13645/150000: episode: 654, duration: 1.203s, episode steps: 86, steps per second: 71, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 9.965 [3.000, 19.000], mean observation: -0.636 [-1.011, 1.167], loss: 0.007678, mean_absolute_error: 1.208450, mean_q: 1.301345\n",
            "  13738/150000: episode: 655, duration: 1.341s, episode steps: 93, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 8.118 [0.000, 18.000], mean observation: -0.631 [-1.011, 1.000], loss: 0.013529, mean_absolute_error: 1.220435, mean_q: 1.325845\n",
            "  13830/150000: episode: 656, duration: 1.344s, episode steps: 92, steps per second: 68, episode reward: 2.000, mean reward: 0.022 [0.000, 1.000], mean action: 7.543 [1.000, 20.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.010259, mean_absolute_error: 1.222482, mean_q: 1.327363\n",
            "  13939/150000: episode: 657, duration: 1.528s, episode steps: 109, steps per second: 71, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 5.147 [1.000, 20.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.010411, mean_absolute_error: 1.227850, mean_q: 1.325988\n",
            "  14006/150000: episode: 658, duration: 0.988s, episode steps: 67, steps per second: 68, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 7.746 [2.000, 19.000], mean observation: -0.611 [-1.041, 8.372], loss: 0.009978, mean_absolute_error: 1.228629, mean_q: 1.323036\n",
            "  14031/150000: episode: 659, duration: 0.462s, episode steps: 25, steps per second: 54, episode reward: 2.000, mean reward: 0.080 [0.000, 1.000], mean action: 6.160 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.009524, mean_absolute_error: 1.226150, mean_q: 1.318969\n",
            "  14043/150000: episode: 660, duration: 0.275s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 7.167 [0.000, 13.000], mean observation: -0.635 [-1.037, 1.000], loss: 0.015250, mean_absolute_error: 1.227104, mean_q: 1.318374\n",
            "  14070/150000: episode: 661, duration: 0.506s, episode steps: 27, steps per second: 53, episode reward: 1.000, mean reward: 0.037 [0.000, 0.900], mean action: 11.148 [0.000, 16.000], mean observation: -0.634 [-1.009, 1.011], loss: 0.007824, mean_absolute_error: 1.227197, mean_q: 1.321827\n",
            "  14101/150000: episode: 662, duration: 0.531s, episode steps: 31, steps per second: 58, episode reward: 2.000, mean reward: 0.065 [0.000, 1.000], mean action: 7.258 [1.000, 18.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.014009, mean_absolute_error: 1.226368, mean_q: 1.322109\n",
            "  14143/150000: episode: 663, duration: 0.673s, episode steps: 42, steps per second: 62, episode reward: 2.000, mean reward: 0.048 [0.000, 1.000], mean action: 5.762 [3.000, 19.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.007549, mean_absolute_error: 1.222023, mean_q: 1.324213\n",
            "  14540/150000: episode: 664, duration: 5.396s, episode steps: 397, steps per second: 74, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 6.393 [0.000, 20.000], mean observation: -0.632 [-1.011, 8.154], loss: 0.010035, mean_absolute_error: 1.233387, mean_q: 1.335395\n",
            "  14632/150000: episode: 665, duration: 1.305s, episode steps: 92, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.696 [0.000, 19.000], mean observation: -0.632 [-1.009, 1.255], loss: 0.010670, mean_absolute_error: 1.251313, mean_q: 1.353663\n",
            "  14738/150000: episode: 666, duration: 1.477s, episode steps: 106, steps per second: 72, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 5.547 [0.000, 15.000], mean observation: -0.636 [-1.011, 1.276], loss: 0.010302, mean_absolute_error: 1.261082, mean_q: 1.364686\n",
            "  14776/150000: episode: 667, duration: 0.657s, episode steps: 38, steps per second: 58, episode reward: 1.000, mean reward: 0.026 [0.000, 0.900], mean action: 5.395 [3.000, 13.000], mean observation: -0.632 [-1.009, 1.037], loss: 0.009332, mean_absolute_error: 1.267695, mean_q: 1.379396\n",
            "  14806/150000: episode: 668, duration: 0.529s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.011651, mean_absolute_error: 1.272602, mean_q: 1.378026\n",
            "  14839/150000: episode: 669, duration: 0.555s, episode steps: 33, steps per second: 60, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.788 [5.000, 20.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.014461, mean_absolute_error: 1.268798, mean_q: 1.399554\n",
            "  14928/150000: episode: 670, duration: 1.332s, episode steps: 89, steps per second: 67, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 4.258 [3.000, 19.000], mean observation: -0.632 [-1.009, 1.010], loss: 0.011947, mean_absolute_error: 1.283126, mean_q: 1.383339\n",
            "  15002/150000: episode: 671, duration: 1.108s, episode steps: 74, steps per second: 67, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 6.514 [1.000, 18.000], mean observation: -0.637 [-1.011, 1.325], loss: 0.009956, mean_absolute_error: 1.280638, mean_q: 1.387500\n",
            "  15120/150000: episode: 672, duration: 1.633s, episode steps: 118, steps per second: 72, episode reward: 2.000, mean reward: 0.017 [0.000, 1.100], mean action: 5.839 [0.000, 18.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.010163, mean_absolute_error: 1.287863, mean_q: 1.389849\n",
            "  15150/150000: episode: 673, duration: 0.531s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.167 [5.000, 10.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.011097, mean_absolute_error: 1.291897, mean_q: 1.401220\n",
            "  15179/150000: episode: 674, duration: 0.511s, episode steps: 29, steps per second: 57, episode reward: 2.000, mean reward: 0.069 [0.000, 1.000], mean action: 6.517 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.004], loss: 0.012750, mean_absolute_error: 1.293060, mean_q: 1.395934\n",
            "  15208/150000: episode: 675, duration: 0.513s, episode steps: 29, steps per second: 57, episode reward: 2.000, mean reward: 0.069 [0.000, 1.000], mean action: 12.276 [3.000, 18.000], mean observation: -0.634 [-1.011, 1.001], loss: 0.008043, mean_absolute_error: 1.293349, mean_q: 1.386292\n",
            "  15264/150000: episode: 676, duration: 0.871s, episode steps: 56, steps per second: 64, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 5.625 [1.000, 17.000], mean observation: -0.609 [-1.026, 8.405], loss: 0.010298, mean_absolute_error: 1.293704, mean_q: 1.389778\n",
            "  15294/150000: episode: 677, duration: 0.519s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.009269, mean_absolute_error: 1.296715, mean_q: 1.407625\n",
            "  15383/150000: episode: 678, duration: 1.246s, episode steps: 89, steps per second: 71, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 4.079 [1.000, 20.000], mean observation: -0.634 [-1.009, 1.121], loss: 0.010002, mean_absolute_error: 1.300072, mean_q: 1.397920\n",
            "  15455/150000: episode: 679, duration: 1.062s, episode steps: 72, steps per second: 68, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 3.306 [1.000, 18.000], mean observation: -0.621 [-1.023, 8.189], loss: 0.010120, mean_absolute_error: 1.301679, mean_q: 1.394901\n",
            "  15485/150000: episode: 680, duration: 0.514s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.833 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.010214, mean_absolute_error: 1.295531, mean_q: 1.389721\n",
            "  15515/150000: episode: 681, duration: 0.507s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.367 [4.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010291, mean_absolute_error: 1.286111, mean_q: 1.382353\n",
            "  15668/150000: episode: 682, duration: 2.174s, episode steps: 153, steps per second: 70, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 5.085 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.449], loss: 0.012151, mean_absolute_error: 1.291475, mean_q: 1.396608\n",
            "  15698/150000: episode: 683, duration: 0.531s, episode steps: 30, steps per second: 56, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.300 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.012233, mean_absolute_error: 1.296873, mean_q: 1.403574\n",
            "  15730/150000: episode: 684, duration: 0.534s, episode steps: 32, steps per second: 60, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 5.750 [2.000, 18.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.011308, mean_absolute_error: 1.302284, mean_q: 1.408927\n",
            "  15918/150000: episode: 685, duration: 2.593s, episode steps: 188, steps per second: 73, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 7.872 [1.000, 20.000], mean observation: -0.628 [-1.023, 8.592], loss: 0.010083, mean_absolute_error: 1.300956, mean_q: 1.404085\n",
            "  15948/150000: episode: 686, duration: 0.510s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.300 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.014525, mean_absolute_error: 1.302980, mean_q: 1.398537\n",
            "  16272/150000: episode: 687, duration: 4.417s, episode steps: 324, steps per second: 73, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 6.167 [0.000, 20.000], mean observation: -0.636 [-1.009, 6.223], loss: 0.010147, mean_absolute_error: 1.290654, mean_q: 1.390986\n",
            "  16672/150000: episode: 688, duration: 5.401s, episode steps: 400, steps per second: 74, episode reward: 0.900, mean reward: 0.002 [0.000, 0.900], mean action: 5.755 [0.000, 20.000], mean observation: -0.638 [-1.009, 5.686], loss: 0.010915, mean_absolute_error: 1.291485, mean_q: 1.394338\n",
            "  16703/150000: episode: 689, duration: 0.536s, episode steps: 31, steps per second: 58, episode reward: 2.000, mean reward: 0.065 [0.000, 1.000], mean action: 6.548 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007222, mean_absolute_error: 1.289024, mean_q: 1.404416\n",
            "  16728/150000: episode: 690, duration: 0.442s, episode steps: 25, steps per second: 57, episode reward: 2.000, mean reward: 0.080 [0.000, 1.000], mean action: 5.680 [5.000, 14.000], mean observation: -0.635 [-1.011, 1.005], loss: 0.008379, mean_absolute_error: 1.291040, mean_q: 1.388042\n",
            "  16762/150000: episode: 691, duration: 0.577s, episode steps: 34, steps per second: 59, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 5.412 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.015259, mean_absolute_error: 1.284064, mean_q: 1.381765\n",
            "  16781/150000: episode: 692, duration: 0.363s, episode steps: 19, steps per second: 52, episode reward: 2.000, mean reward: 0.105 [0.000, 1.000], mean action: 6.579 [0.000, 14.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.010818, mean_absolute_error: 1.286400, mean_q: 1.390402\n",
            "  16817/150000: episode: 693, duration: 0.580s, episode steps: 36, steps per second: 62, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 5.833 [2.000, 18.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.009036, mean_absolute_error: 1.287995, mean_q: 1.386136\n",
            "  16918/150000: episode: 694, duration: 1.432s, episode steps: 101, steps per second: 71, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 5.842 [0.000, 18.000], mean observation: -0.631 [-1.011, 1.012], loss: 0.015623, mean_absolute_error: 1.285293, mean_q: 1.389189\n",
            "  16982/150000: episode: 695, duration: 0.987s, episode steps: 64, steps per second: 65, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 5.188 [1.000, 19.000], mean observation: -0.614 [-1.011, 8.114], loss: 0.008709, mean_absolute_error: 1.296045, mean_q: 1.408036\n",
            "  16992/150000: episode: 696, duration: 0.249s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 2.000], mean action: 5.800 [3.000, 12.000], mean observation: -0.628 [-1.011, 2.193], loss: 0.012317, mean_absolute_error: 1.303553, mean_q: 1.394264\n",
            "  17022/150000: episode: 697, duration: 0.525s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.467 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.015], loss: 0.009167, mean_absolute_error: 1.298932, mean_q: 1.403618\n",
            "  17067/150000: episode: 698, duration: 0.710s, episode steps: 45, steps per second: 63, episode reward: 2.000, mean reward: 0.044 [0.000, 1.000], mean action: 8.311 [2.000, 20.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.008033, mean_absolute_error: 1.300652, mean_q: 1.394215\n",
            "  17117/150000: episode: 699, duration: 0.786s, episode steps: 50, steps per second: 64, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 5.620 [1.000, 19.000], mean observation: -0.611 [-1.009, 7.651], loss: 0.013174, mean_absolute_error: 1.300695, mean_q: 1.412917\n",
            "  17239/150000: episode: 700, duration: 1.717s, episode steps: 122, steps per second: 71, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 6.262 [2.000, 20.000], mean observation: -0.632 [-1.011, 1.428], loss: 0.009673, mean_absolute_error: 1.303589, mean_q: 1.404131\n",
            "  17369/150000: episode: 701, duration: 1.812s, episode steps: 130, steps per second: 72, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 6.338 [0.000, 18.000], mean observation: -0.630 [-1.009, 1.099], loss: 0.012212, mean_absolute_error: 1.305605, mean_q: 1.403960\n",
            "  17406/150000: episode: 702, duration: 0.606s, episode steps: 37, steps per second: 61, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.351 [4.000, 17.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.010696, mean_absolute_error: 1.304118, mean_q: 1.406286\n",
            "  17423/150000: episode: 703, duration: 0.346s, episode steps: 17, steps per second: 49, episode reward: 2.000, mean reward: 0.118 [0.000, 1.100], mean action: 5.647 [3.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.008323, mean_absolute_error: 1.303656, mean_q: 1.398508\n",
            "  17512/150000: episode: 704, duration: 1.294s, episode steps: 89, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.258 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.011619, mean_absolute_error: 1.306969, mean_q: 1.404991\n",
            "  17545/150000: episode: 705, duration: 0.546s, episode steps: 33, steps per second: 60, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.455 [5.000, 18.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.008509, mean_absolute_error: 1.304784, mean_q: 1.419471\n",
            "  17619/150000: episode: 706, duration: 1.068s, episode steps: 74, steps per second: 69, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 7.919 [2.000, 17.000], mean observation: -0.612 [-1.009, 9.504], loss: 0.008617, mean_absolute_error: 1.309419, mean_q: 1.413108\n",
            "  17649/150000: episode: 707, duration: 0.519s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.167 [5.000, 10.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.015204, mean_absolute_error: 1.305719, mean_q: 1.410606\n",
            "  17738/150000: episode: 708, duration: 1.287s, episode steps: 89, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.169 [1.000, 17.000], mean observation: -0.632 [-1.009, 1.000], loss: 0.010190, mean_absolute_error: 1.313915, mean_q: 1.414050\n",
            "  17777/150000: episode: 709, duration: 0.652s, episode steps: 39, steps per second: 60, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 5.744 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.009], loss: 0.011823, mean_absolute_error: 1.306296, mean_q: 1.398532\n",
            "  17805/150000: episode: 710, duration: 0.519s, episode steps: 28, steps per second: 54, episode reward: 1.000, mean reward: 0.036 [0.000, 0.900], mean action: 2.214 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.001], loss: 0.014358, mean_absolute_error: 1.299957, mean_q: 1.392454\n",
            "  17844/150000: episode: 711, duration: 0.693s, episode steps: 39, steps per second: 56, episode reward: 1.000, mean reward: 0.026 [0.000, 0.900], mean action: 4.154 [0.000, 18.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.011269, mean_absolute_error: 1.300275, mean_q: 1.385339\n",
            "  17931/150000: episode: 712, duration: 1.231s, episode steps: 87, steps per second: 71, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 6.379 [2.000, 17.000], mean observation: -0.609 [-1.018, 9.803], loss: 0.012212, mean_absolute_error: 1.289051, mean_q: 1.383502\n",
            "  18004/150000: episode: 713, duration: 1.075s, episode steps: 73, steps per second: 68, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 2.644 [2.000, 17.000], mean observation: -0.640 [-1.011, 1.000], loss: 0.007356, mean_absolute_error: 1.281302, mean_q: 1.391126\n",
            "  18039/150000: episode: 714, duration: 0.591s, episode steps: 35, steps per second: 59, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 5.343 [2.000, 19.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.009787, mean_absolute_error: 1.286118, mean_q: 1.391989\n",
            "  18097/150000: episode: 715, duration: 0.871s, episode steps: 58, steps per second: 67, episode reward: 0.900, mean reward: 0.016 [0.000, 0.900], mean action: 5.259 [5.000, 10.000], mean observation: -0.613 [-1.021, 7.965], loss: 0.012761, mean_absolute_error: 1.287923, mean_q: 1.405464\n",
            "  18211/150000: episode: 716, duration: 1.621s, episode steps: 114, steps per second: 70, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 5.219 [1.000, 18.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.011620, mean_absolute_error: 1.293427, mean_q: 1.401156\n",
            "  18314/150000: episode: 717, duration: 1.466s, episode steps: 103, steps per second: 70, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 5.825 [0.000, 20.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.009542, mean_absolute_error: 1.293841, mean_q: 1.398000\n",
            "  18344/150000: episode: 718, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.733 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010062, mean_absolute_error: 1.299591, mean_q: 1.396273\n",
            "  18374/150000: episode: 719, duration: 0.510s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.500 [4.000, 18.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.010671, mean_absolute_error: 1.299253, mean_q: 1.408719\n",
            "  18509/150000: episode: 720, duration: 1.890s, episode steps: 135, steps per second: 71, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 7.511 [1.000, 20.000], mean observation: -0.622 [-1.009, 1.000], loss: 0.009925, mean_absolute_error: 1.300366, mean_q: 1.401049\n",
            "  18595/150000: episode: 721, duration: 1.246s, episode steps: 86, steps per second: 69, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 5.942 [2.000, 19.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.010845, mean_absolute_error: 1.284074, mean_q: 1.386664\n",
            "  18629/150000: episode: 722, duration: 0.549s, episode steps: 34, steps per second: 62, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 5.324 [3.000, 12.000], mean observation: -0.634 [-1.011, 1.021], loss: 0.011509, mean_absolute_error: 1.283643, mean_q: 1.394251\n",
            "  18743/150000: episode: 723, duration: 1.611s, episode steps: 114, steps per second: 71, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 5.412 [0.000, 19.000], mean observation: -0.632 [-1.009, 1.000], loss: 0.009461, mean_absolute_error: 1.290839, mean_q: 1.398633\n",
            "  18773/150000: episode: 724, duration: 0.510s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.367 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010533, mean_absolute_error: 1.301170, mean_q: 1.417362\n",
            "  18799/150000: episode: 725, duration: 0.485s, episode steps: 26, steps per second: 54, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 6.769 [2.000, 19.000], mean observation: -0.635 [-1.011, 1.010], loss: 0.012492, mean_absolute_error: 1.291737, mean_q: 1.403693\n",
            "  18829/150000: episode: 726, duration: 0.523s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.867 [0.000, 6.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.014729, mean_absolute_error: 1.289146, mean_q: 1.397835\n",
            "  18861/150000: episode: 727, duration: 0.542s, episode steps: 32, steps per second: 59, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 5.469 [3.000, 15.000], mean observation: -0.633 [-1.009, 1.015], loss: 0.005732, mean_absolute_error: 1.294195, mean_q: 1.409450\n",
            "  19120/150000: episode: 728, duration: 3.703s, episode steps: 259, steps per second: 70, episode reward: 1.000, mean reward: 0.004 [0.000, 0.900], mean action: 8.726 [0.000, 20.000], mean observation: -0.626 [-1.011, 8.396], loss: 0.012279, mean_absolute_error: 1.300161, mean_q: 1.403777\n",
            "  19144/150000: episode: 729, duration: 0.452s, episode steps: 24, steps per second: 53, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 9.625 [5.000, 15.000], mean observation: -0.635 [-1.011, 1.006], loss: 0.016621, mean_absolute_error: 1.290718, mean_q: 1.394648\n",
            "  19174/150000: episode: 730, duration: 0.534s, episode steps: 30, steps per second: 56, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.967 [2.000, 10.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008761, mean_absolute_error: 1.291755, mean_q: 1.395028\n",
            "  19204/150000: episode: 731, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.867 [2.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.011232, mean_absolute_error: 1.292204, mean_q: 1.390146\n",
            "  19256/150000: episode: 732, duration: 0.799s, episode steps: 52, steps per second: 65, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 6.077 [1.000, 20.000], mean observation: -0.609 [-1.009, 8.189], loss: 0.010434, mean_absolute_error: 1.280736, mean_q: 1.391720\n",
            "  19340/150000: episode: 733, duration: 1.248s, episode steps: 84, steps per second: 67, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 13.393 [1.000, 18.000], mean observation: -0.630 [-1.009, 1.015], loss: 0.013262, mean_absolute_error: 1.280648, mean_q: 1.380755\n",
            "  19370/150000: episode: 734, duration: 0.512s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.012721, mean_absolute_error: 1.276682, mean_q: 1.390926\n",
            "  19380/150000: episode: 735, duration: 0.260s, episode steps: 10, steps per second: 38, episode reward: 2.000, mean reward: 0.200 [0.000, 2.000], mean action: 6.000 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.017], loss: 0.013333, mean_absolute_error: 1.276545, mean_q: 1.376184\n",
            "  19454/150000: episode: 736, duration: 1.116s, episode steps: 74, steps per second: 66, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 5.162 [1.000, 14.000], mean observation: -0.609 [-1.103, 9.743], loss: 0.009522, mean_absolute_error: 1.277902, mean_q: 1.381004\n",
            "  19491/150000: episode: 737, duration: 0.595s, episode steps: 37, steps per second: 62, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.378 [3.000, 17.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.007708, mean_absolute_error: 1.284008, mean_q: 1.384374\n",
            "  19521/150000: episode: 738, duration: 0.520s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.500 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.009799, mean_absolute_error: 1.283330, mean_q: 1.387207\n",
            "  19564/150000: episode: 739, duration: 0.676s, episode steps: 43, steps per second: 64, episode reward: 2.000, mean reward: 0.047 [0.000, 1.000], mean action: 5.674 [4.000, 19.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.012748, mean_absolute_error: 1.286728, mean_q: 1.398372\n",
            "  19699/150000: episode: 740, duration: 1.908s, episode steps: 135, steps per second: 71, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 5.385 [0.000, 20.000], mean observation: -0.633 [-1.009, 1.317], loss: 0.010489, mean_absolute_error: 1.294680, mean_q: 1.399581\n",
            "  19749/150000: episode: 741, duration: 0.784s, episode steps: 50, steps per second: 64, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 5.340 [0.000, 18.000], mean observation: -0.611 [-1.009, 7.662], loss: 0.008075, mean_absolute_error: 1.301014, mean_q: 1.398510\n",
            "  19779/150000: episode: 742, duration: 0.509s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.433 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.016936, mean_absolute_error: 1.305853, mean_q: 1.409022\n",
            "  19809/150000: episode: 743, duration: 0.543s, episode steps: 30, steps per second: 55, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010062, mean_absolute_error: 1.306726, mean_q: 1.418322\n",
            "  20074/150000: episode: 744, duration: 3.680s, episode steps: 265, steps per second: 72, episode reward: 0.000, mean reward: 0.000 [-1.000, 0.900], mean action: 6.955 [0.000, 20.000], mean observation: -0.632 [-1.011, 6.137], loss: 0.011765, mean_absolute_error: 1.307765, mean_q: 1.414972\n",
            "  20176/150000: episode: 745, duration: 1.465s, episode steps: 102, steps per second: 70, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 3.951 [0.000, 15.000], mean observation: -0.640 [-1.009, 1.245], loss: 0.009492, mean_absolute_error: 1.305413, mean_q: 1.404275\n",
            "  20223/150000: episode: 746, duration: 0.777s, episode steps: 47, steps per second: 60, episode reward: 1.000, mean reward: 0.021 [0.000, 0.900], mean action: 5.106 [3.000, 20.000], mean observation: -0.634 [-1.009, 1.010], loss: 0.015201, mean_absolute_error: 1.300074, mean_q: 1.394257\n",
            "  20263/150000: episode: 747, duration: 0.645s, episode steps: 40, steps per second: 62, episode reward: 2.000, mean reward: 0.050 [0.000, 1.000], mean action: 6.700 [1.000, 20.000], mean observation: -0.633 [-1.011, 1.003], loss: 0.015586, mean_absolute_error: 1.290221, mean_q: 1.396875\n",
            "  20348/150000: episode: 748, duration: 1.245s, episode steps: 85, steps per second: 68, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 11.400 [0.000, 20.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.010794, mean_absolute_error: 1.287076, mean_q: 1.387058\n",
            "  20378/150000: episode: 749, duration: 0.510s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.400 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007107, mean_absolute_error: 1.283028, mean_q: 1.385688\n",
            "  20410/150000: episode: 750, duration: 0.531s, episode steps: 32, steps per second: 60, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 5.344 [2.000, 18.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.010460, mean_absolute_error: 1.291921, mean_q: 1.399151\n",
            "  20440/150000: episode: 751, duration: 0.518s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.400 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.014404, mean_absolute_error: 1.296822, mean_q: 1.412079\n",
            "  20470/150000: episode: 752, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.167 [1.000, 11.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.013018, mean_absolute_error: 1.299739, mean_q: 1.424572\n",
            "  20559/150000: episode: 753, duration: 1.295s, episode steps: 89, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 6.708 [1.000, 20.000], mean observation: -0.631 [-1.011, 1.013], loss: 0.010512, mean_absolute_error: 1.307547, mean_q: 1.407660\n",
            "  20820/150000: episode: 754, duration: 3.548s, episode steps: 261, steps per second: 74, episode reward: 1.000, mean reward: 0.004 [0.000, 0.900], mean action: 7.563 [0.000, 20.000], mean observation: -0.634 [-1.011, 9.458], loss: 0.009539, mean_absolute_error: 1.294043, mean_q: 1.390338\n",
            "  20843/150000: episode: 755, duration: 0.427s, episode steps: 23, steps per second: 54, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 6.826 [4.000, 16.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.010656, mean_absolute_error: 1.283823, mean_q: 1.382707\n",
            "  20959/150000: episode: 756, duration: 1.637s, episode steps: 116, steps per second: 71, episode reward: 2.000, mean reward: 0.017 [0.000, 1.000], mean action: 6.948 [1.000, 19.000], mean observation: -0.630 [-1.011, 2.074], loss: 0.010947, mean_absolute_error: 1.284972, mean_q: 1.386141\n",
            "  20989/150000: episode: 757, duration: 0.513s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.016039, mean_absolute_error: 1.284200, mean_q: 1.386807\n",
            "  21025/150000: episode: 758, duration: 0.577s, episode steps: 36, steps per second: 62, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 5.861 [4.000, 20.000], mean observation: -0.634 [-1.011, 1.007], loss: 0.010199, mean_absolute_error: 1.284494, mean_q: 1.394841\n",
            "  21167/150000: episode: 759, duration: 1.980s, episode steps: 142, steps per second: 72, episode reward: 0.900, mean reward: 0.006 [0.000, 0.900], mean action: 5.613 [1.000, 20.000], mean observation: -0.626 [-1.009, 8.469], loss: 0.011894, mean_absolute_error: 1.286283, mean_q: 1.385710\n",
            "  21233/150000: episode: 760, duration: 0.985s, episode steps: 66, steps per second: 67, episode reward: 2.000, mean reward: 0.030 [0.000, 1.100], mean action: 3.924 [2.000, 15.000], mean observation: -0.635 [-1.011, 1.018], loss: 0.011293, mean_absolute_error: 1.282758, mean_q: 1.392833\n",
            "  21263/150000: episode: 761, duration: 0.537s, episode steps: 30, steps per second: 56, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007446, mean_absolute_error: 1.291586, mean_q: 1.401817\n",
            "  21296/150000: episode: 762, duration: 0.544s, episode steps: 33, steps per second: 61, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 4.818 [1.000, 12.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.005800, mean_absolute_error: 1.292303, mean_q: 1.395332\n",
            "  21326/150000: episode: 763, duration: 0.521s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.700 [4.000, 20.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.007182, mean_absolute_error: 1.292542, mean_q: 1.387716\n",
            "  21379/150000: episode: 764, duration: 0.810s, episode steps: 53, steps per second: 65, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 5.981 [2.000, 20.000], mean observation: -0.612 [-1.018, 7.981], loss: 0.009601, mean_absolute_error: 1.286219, mean_q: 1.380158\n",
            "  21409/150000: episode: 765, duration: 0.505s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.013124, mean_absolute_error: 1.282714, mean_q: 1.380126\n",
            "  21507/150000: episode: 766, duration: 1.366s, episode steps: 98, steps per second: 72, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 6.888 [0.000, 18.000], mean observation: -0.630 [-1.009, 1.021], loss: 0.009819, mean_absolute_error: 1.281381, mean_q: 1.377673\n",
            "  21537/150000: episode: 767, duration: 0.513s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.367 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.011778, mean_absolute_error: 1.277882, mean_q: 1.371280\n",
            "  21567/150000: episode: 768, duration: 0.504s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 7.767 [3.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008189, mean_absolute_error: 1.278169, mean_q: 1.384999\n",
            "  21599/150000: episode: 769, duration: 0.528s, episode steps: 32, steps per second: 61, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 9.625 [0.000, 17.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.015217, mean_absolute_error: 1.280543, mean_q: 1.389679\n",
            "  21679/150000: episode: 770, duration: 1.154s, episode steps: 80, steps per second: 69, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 7.688 [1.000, 19.000], mean observation: -0.635 [-1.009, 1.000], loss: 0.010957, mean_absolute_error: 1.275135, mean_q: 1.380859\n",
            "  21810/150000: episode: 771, duration: 1.821s, episode steps: 131, steps per second: 72, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 5.771 [2.000, 17.000], mean observation: -0.629 [-1.011, 1.015], loss: 0.008157, mean_absolute_error: 1.277730, mean_q: 1.383018\n",
            "  21915/150000: episode: 772, duration: 1.480s, episode steps: 105, steps per second: 71, episode reward: 2.000, mean reward: 0.019 [0.000, 1.000], mean action: 7.990 [1.000, 17.000], mean observation: -0.631 [-1.011, 1.827], loss: 0.012305, mean_absolute_error: 1.280140, mean_q: 1.381371\n",
            "  21951/150000: episode: 773, duration: 0.619s, episode steps: 36, steps per second: 58, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 5.389 [1.000, 17.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.012522, mean_absolute_error: 1.274774, mean_q: 1.392487\n",
            "  22000/150000: episode: 774, duration: 0.790s, episode steps: 49, steps per second: 62, episode reward: 1.000, mean reward: 0.020 [0.000, 0.900], mean action: 4.204 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.010890, mean_absolute_error: 1.279780, mean_q: 1.376132\n",
            "  22074/150000: episode: 775, duration: 1.069s, episode steps: 74, steps per second: 69, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 3.622 [0.000, 17.000], mean observation: -0.640 [-1.009, 1.000], loss: 0.010156, mean_absolute_error: 1.283001, mean_q: 1.379105\n",
            "  22112/150000: episode: 776, duration: 0.613s, episode steps: 38, steps per second: 62, episode reward: 2.000, mean reward: 0.053 [0.000, 1.000], mean action: 5.605 [2.000, 20.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.011984, mean_absolute_error: 1.279425, mean_q: 1.390981\n",
            "  22194/150000: episode: 777, duration: 1.218s, episode steps: 82, steps per second: 67, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 4.366 [0.000, 19.000], mean observation: -0.632 [-1.011, 1.012], loss: 0.009175, mean_absolute_error: 1.283945, mean_q: 1.378580\n",
            "  22214/150000: episode: 778, duration: 0.401s, episode steps: 20, steps per second: 50, episode reward: 2.000, mean reward: 0.100 [0.000, 1.000], mean action: 4.850 [0.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.009603, mean_absolute_error: 1.282048, mean_q: 1.382608\n",
            "  22286/150000: episode: 779, duration: 1.080s, episode steps: 72, steps per second: 67, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 5.708 [0.000, 20.000], mean observation: -0.613 [-1.037, 8.406], loss: 0.012106, mean_absolute_error: 1.289851, mean_q: 1.394475\n",
            "  22321/150000: episode: 780, duration: 0.553s, episode steps: 35, steps per second: 63, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 9.514 [5.000, 18.000], mean observation: -0.632 [-1.009, 1.020], loss: 0.012505, mean_absolute_error: 1.284811, mean_q: 1.396655\n",
            "  22374/150000: episode: 781, duration: 0.836s, episode steps: 53, steps per second: 63, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 13.774 [3.000, 17.000], mean observation: -0.609 [-1.009, 8.114], loss: 0.008944, mean_absolute_error: 1.283088, mean_q: 1.379920\n",
            "  22469/150000: episode: 782, duration: 1.396s, episode steps: 95, steps per second: 68, episode reward: 2.000, mean reward: 0.021 [0.000, 1.100], mean action: 5.779 [2.000, 19.000], mean observation: -0.610 [-1.011, 7.668], loss: 0.012074, mean_absolute_error: 1.278783, mean_q: 1.381133\n",
            "  22604/150000: episode: 783, duration: 1.955s, episode steps: 135, steps per second: 69, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 7.859 [1.000, 20.000], mean observation: -0.626 [-1.046, 8.371], loss: 0.010750, mean_absolute_error: 1.273577, mean_q: 1.369841\n",
            "  22654/150000: episode: 784, duration: 0.809s, episode steps: 50, steps per second: 62, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 6.200 [1.000, 16.000], mean observation: -0.611 [-1.011, 7.662], loss: 0.008628, mean_absolute_error: 1.268050, mean_q: 1.374107\n",
            "  22666/150000: episode: 785, duration: 0.286s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.000], mean action: 6.833 [3.000, 17.000], mean observation: -0.636 [-1.036, 1.000], loss: 0.009676, mean_absolute_error: 1.272596, mean_q: 1.377458\n",
            "  22757/150000: episode: 786, duration: 1.351s, episode steps: 91, steps per second: 67, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.275 [0.000, 20.000], mean observation: -0.617 [-1.011, 7.825], loss: 0.011436, mean_absolute_error: 1.273937, mean_q: 1.377092\n",
            "  22855/150000: episode: 787, duration: 1.443s, episode steps: 98, steps per second: 68, episode reward: 0.000, mean reward: 0.000 [-1.000, 0.900], mean action: 6.051 [5.000, 20.000], mean observation: -0.624 [-1.020, 8.376], loss: 0.011106, mean_absolute_error: 1.284901, mean_q: 1.390004\n",
            "  22928/150000: episode: 788, duration: 1.119s, episode steps: 73, steps per second: 65, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 7.027 [0.000, 16.000], mean observation: -0.638 [-1.011, 1.000], loss: 0.012247, mean_absolute_error: 1.295834, mean_q: 1.396913\n",
            "  23023/150000: episode: 789, duration: 1.615s, episode steps: 95, steps per second: 59, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 7.726 [5.000, 20.000], mean observation: -0.626 [-1.046, 8.289], loss: 0.008576, mean_absolute_error: 1.300685, mean_q: 1.397375\n",
            "  23119/150000: episode: 790, duration: 1.455s, episode steps: 96, steps per second: 66, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 6.042 [0.000, 20.000], mean observation: -0.635 [-1.009, 1.014], loss: 0.012374, mean_absolute_error: 1.292311, mean_q: 1.395228\n",
            "  23149/150000: episode: 791, duration: 0.529s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.467 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.009842, mean_absolute_error: 1.293405, mean_q: 1.398215\n",
            "  23179/150000: episode: 792, duration: 0.522s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.133 [1.000, 18.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.015630, mean_absolute_error: 1.290792, mean_q: 1.401579\n",
            "  23255/150000: episode: 793, duration: 1.197s, episode steps: 76, steps per second: 64, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 5.908 [1.000, 20.000], mean observation: -0.614 [-1.039, 8.391], loss: 0.010381, mean_absolute_error: 1.292838, mean_q: 1.406160\n",
            "  23285/150000: episode: 794, duration: 0.562s, episode steps: 30, steps per second: 53, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.533 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.009480, mean_absolute_error: 1.298935, mean_q: 1.413367\n",
            "  23315/150000: episode: 795, duration: 0.518s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.433 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.012731, mean_absolute_error: 1.302942, mean_q: 1.417900\n",
            "  23338/150000: episode: 796, duration: 0.435s, episode steps: 23, steps per second: 53, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 5.565 [1.000, 16.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.008559, mean_absolute_error: 1.305778, mean_q: 1.425606\n",
            "  23425/150000: episode: 797, duration: 1.254s, episode steps: 87, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.632 [0.000, 19.000], mean observation: -0.620 [-1.024, 7.922], loss: 0.009161, mean_absolute_error: 1.310496, mean_q: 1.411247\n",
            "  23455/150000: episode: 798, duration: 0.517s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.100 [5.000, 7.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.009871, mean_absolute_error: 1.305809, mean_q: 1.403527\n",
            "  23487/150000: episode: 799, duration: 0.530s, episode steps: 32, steps per second: 60, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 5.594 [3.000, 18.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.008933, mean_absolute_error: 1.294371, mean_q: 1.392794\n",
            "  23517/150000: episode: 800, duration: 0.518s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.600 [5.000, 11.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010510, mean_absolute_error: 1.295489, mean_q: 1.396258\n",
            "  23638/150000: episode: 801, duration: 1.722s, episode steps: 121, steps per second: 70, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 6.463 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.012901, mean_absolute_error: 1.284028, mean_q: 1.386162\n",
            "  23668/150000: episode: 802, duration: 0.517s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.433 [3.000, 18.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.009554, mean_absolute_error: 1.287679, mean_q: 1.392531\n",
            "  23751/150000: episode: 803, duration: 1.220s, episode steps: 83, steps per second: 68, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 8.771 [0.000, 18.000], mean observation: -0.628 [-1.009, 5.032], loss: 0.008765, mean_absolute_error: 1.282856, mean_q: 1.377017\n",
            "  23849/150000: episode: 804, duration: 1.415s, episode steps: 98, steps per second: 69, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 4.367 [1.000, 19.000], mean observation: -0.633 [-1.064, 1.148], loss: 0.010017, mean_absolute_error: 1.275900, mean_q: 1.378215\n",
            "  23886/150000: episode: 805, duration: 0.611s, episode steps: 37, steps per second: 61, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.541 [1.000, 18.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.012632, mean_absolute_error: 1.276673, mean_q: 1.390795\n",
            "  23902/150000: episode: 806, duration: 0.334s, episode steps: 16, steps per second: 48, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 6.438 [1.000, 17.000], mean observation: -0.635 [-1.017, 1.002], loss: 0.007981, mean_absolute_error: 1.277052, mean_q: 1.396278\n",
            "  23932/150000: episode: 807, duration: 0.522s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [3.000, 12.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.009324, mean_absolute_error: 1.286561, mean_q: 1.389083\n",
            "  23958/150000: episode: 808, duration: 0.480s, episode steps: 26, steps per second: 54, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 5.115 [1.000, 13.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.013476, mean_absolute_error: 1.279732, mean_q: 1.380453\n",
            "  23988/150000: episode: 809, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.400 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.009630, mean_absolute_error: 1.287176, mean_q: 1.393313\n",
            "  24047/150000: episode: 810, duration: 0.919s, episode steps: 59, steps per second: 64, episode reward: 1.000, mean reward: 0.017 [0.000, 0.900], mean action: 5.814 [3.000, 20.000], mean observation: -0.611 [-1.042, 8.358], loss: 0.010975, mean_absolute_error: 1.289459, mean_q: 1.392780\n",
            "  24136/150000: episode: 811, duration: 1.277s, episode steps: 89, steps per second: 70, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 11.562 [0.000, 20.000], mean observation: -0.633 [-1.009, 1.000], loss: 0.010221, mean_absolute_error: 1.280031, mean_q: 1.377234\n",
            "  24361/150000: episode: 812, duration: 3.097s, episode steps: 225, steps per second: 73, episode reward: 0.900, mean reward: 0.004 [0.000, 0.900], mean action: 5.738 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.560], loss: 0.008838, mean_absolute_error: 1.274444, mean_q: 1.373715\n",
            "  24477/150000: episode: 813, duration: 1.662s, episode steps: 116, steps per second: 70, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 7.069 [0.000, 19.000], mean observation: -0.632 [-1.011, 1.818], loss: 0.010600, mean_absolute_error: 1.262246, mean_q: 1.361847\n",
            "  24663/150000: episode: 814, duration: 2.539s, episode steps: 186, steps per second: 73, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 4.597 [0.000, 20.000], mean observation: -0.623 [-1.009, 9.394], loss: 0.010811, mean_absolute_error: 1.255822, mean_q: 1.348131\n",
            "  24735/150000: episode: 815, duration: 1.165s, episode steps: 72, steps per second: 62, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 5.847 [1.000, 20.000], mean observation: -0.618 [-1.009, 8.189], loss: 0.009890, mean_absolute_error: 1.246861, mean_q: 1.337723\n",
            "  24821/150000: episode: 816, duration: 1.251s, episode steps: 86, steps per second: 69, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 4.756 [1.000, 17.000], mean observation: -0.637 [-1.009, 1.295], loss: 0.010683, mean_absolute_error: 1.242224, mean_q: 1.347268\n",
            "  24914/150000: episode: 817, duration: 1.375s, episode steps: 93, steps per second: 68, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 7.237 [1.000, 20.000], mean observation: -0.637 [-1.009, 1.167], loss: 0.010665, mean_absolute_error: 1.241613, mean_q: 1.344384\n",
            "  25054/150000: episode: 818, duration: 1.992s, episode steps: 140, steps per second: 70, episode reward: 2.000, mean reward: 0.014 [0.000, 1.000], mean action: 6.136 [0.000, 20.000], mean observation: -0.629 [-1.011, 1.002], loss: 0.009622, mean_absolute_error: 1.248087, mean_q: 1.355081\n",
            "  25064/150000: episode: 819, duration: 0.250s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 2.000], mean action: 14.800 [5.000, 18.000], mean observation: -0.631 [-1.041, 1.547], loss: 0.004729, mean_absolute_error: 1.260506, mean_q: 1.362406\n",
            "  25118/150000: episode: 820, duration: 0.824s, episode steps: 54, steps per second: 66, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 9.241 [0.000, 20.000], mean observation: -0.610 [-1.011, 8.059], loss: 0.010988, mean_absolute_error: 1.252927, mean_q: 1.345550\n",
            "  25150/150000: episode: 821, duration: 0.524s, episode steps: 32, steps per second: 61, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 8.031 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.009579, mean_absolute_error: 1.252373, mean_q: 1.347020\n",
            "  25180/150000: episode: 822, duration: 0.517s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.933 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.012629, mean_absolute_error: 1.245530, mean_q: 1.351213\n",
            "  25210/150000: episode: 823, duration: 0.504s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 9.767 [1.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007806, mean_absolute_error: 1.251584, mean_q: 1.340860\n",
            "  25240/150000: episode: 824, duration: 0.523s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.933 [3.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.009865, mean_absolute_error: 1.253251, mean_q: 1.352115\n",
            "  25295/150000: episode: 825, duration: 0.827s, episode steps: 55, steps per second: 67, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 6.618 [2.000, 16.000], mean observation: -0.609 [-1.017, 8.381], loss: 0.013483, mean_absolute_error: 1.243463, mean_q: 1.342918\n",
            "  25344/150000: episode: 826, duration: 0.802s, episode steps: 49, steps per second: 61, episode reward: 1.000, mean reward: 0.020 [0.000, 0.900], mean action: 5.449 [2.000, 16.000], mean observation: -0.633 [-1.009, 1.011], loss: 0.006702, mean_absolute_error: 1.245256, mean_q: 1.346575\n",
            "  25381/150000: episode: 827, duration: 0.638s, episode steps: 37, steps per second: 58, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.189 [3.000, 14.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.013666, mean_absolute_error: 1.248728, mean_q: 1.343407\n",
            "  25499/150000: episode: 828, duration: 1.684s, episode steps: 118, steps per second: 70, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 6.466 [4.000, 20.000], mean observation: -0.631 [-1.009, 1.217], loss: 0.012415, mean_absolute_error: 1.258693, mean_q: 1.366392\n",
            "  25525/150000: episode: 829, duration: 0.467s, episode steps: 26, steps per second: 56, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 7.462 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.013022, mean_absolute_error: 1.260795, mean_q: 1.363361\n",
            "  25537/150000: episode: 830, duration: 0.258s, episode steps: 12, steps per second: 46, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.667 [5.000, 13.000], mean observation: -0.632 [-1.011, 1.354], loss: 0.013976, mean_absolute_error: 1.262148, mean_q: 1.367466\n",
            "  25573/150000: episode: 831, duration: 0.635s, episode steps: 36, steps per second: 57, episode reward: 1.000, mean reward: 0.028 [0.000, 0.900], mean action: 5.333 [4.000, 13.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.008954, mean_absolute_error: 1.268089, mean_q: 1.377010\n",
            "  25645/150000: episode: 832, duration: 1.067s, episode steps: 72, steps per second: 67, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 5.417 [1.000, 20.000], mean observation: -0.637 [-1.009, 1.401], loss: 0.009352, mean_absolute_error: 1.265747, mean_q: 1.368378\n",
            "  25730/150000: episode: 833, duration: 1.228s, episode steps: 85, steps per second: 69, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 9.106 [0.000, 18.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.010602, mean_absolute_error: 1.263554, mean_q: 1.367637\n",
            "  25755/150000: episode: 834, duration: 0.454s, episode steps: 25, steps per second: 55, episode reward: 2.000, mean reward: 0.080 [0.000, 1.000], mean action: 5.400 [5.000, 13.000], mean observation: -0.634 [-1.011, 1.006], loss: 0.012056, mean_absolute_error: 1.255824, mean_q: 1.356761\n",
            "  25823/150000: episode: 835, duration: 1.063s, episode steps: 68, steps per second: 64, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 4.500 [2.000, 19.000], mean observation: -0.632 [-1.011, 1.007], loss: 0.010875, mean_absolute_error: 1.257128, mean_q: 1.363959\n",
            "  25862/150000: episode: 836, duration: 0.624s, episode steps: 39, steps per second: 63, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 10.000 [0.000, 18.000], mean observation: -0.633 [-1.011, 1.007], loss: 0.012354, mean_absolute_error: 1.253649, mean_q: 1.355558\n",
            "  26004/150000: episode: 837, duration: 1.980s, episode steps: 142, steps per second: 72, episode reward: 2.000, mean reward: 0.014 [0.000, 1.000], mean action: 8.415 [0.000, 20.000], mean observation: -0.630 [-1.011, 1.013], loss: 0.011911, mean_absolute_error: 1.246745, mean_q: 1.347104\n",
            "  26043/150000: episode: 838, duration: 0.658s, episode steps: 39, steps per second: 59, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 8.487 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.013625, mean_absolute_error: 1.242220, mean_q: 1.337561\n",
            "  26073/150000: episode: 839, duration: 0.505s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.300 [1.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.012356, mean_absolute_error: 1.239788, mean_q: 1.360199\n",
            "  26103/150000: episode: 840, duration: 0.519s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.600 [5.000, 15.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.009424, mean_absolute_error: 1.246242, mean_q: 1.361008\n",
            "  26139/150000: episode: 841, duration: 0.587s, episode steps: 36, steps per second: 61, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 4.944 [3.000, 5.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.010796, mean_absolute_error: 1.259749, mean_q: 1.373385\n",
            "  26169/150000: episode: 842, duration: 0.519s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 7.633 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.011235, mean_absolute_error: 1.267346, mean_q: 1.358361\n",
            "  26202/150000: episode: 843, duration: 0.549s, episode steps: 33, steps per second: 60, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.636 [5.000, 16.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.007818, mean_absolute_error: 1.260273, mean_q: 1.353688\n",
            "  26316/150000: episode: 844, duration: 1.588s, episode steps: 114, steps per second: 72, episode reward: 2.000, mean reward: 0.018 [0.000, 1.000], mean action: 4.702 [1.000, 20.000], mean observation: -0.634 [-1.026, 1.667], loss: 0.008871, mean_absolute_error: 1.255585, mean_q: 1.351300\n",
            "  26390/150000: episode: 845, duration: 1.105s, episode steps: 74, steps per second: 67, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 5.338 [0.000, 18.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.013242, mean_absolute_error: 1.241520, mean_q: 1.333019\n",
            "  26427/150000: episode: 846, duration: 0.600s, episode steps: 37, steps per second: 62, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.432 [2.000, 14.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.010378, mean_absolute_error: 1.231498, mean_q: 1.347526\n",
            "  26465/150000: episode: 847, duration: 0.611s, episode steps: 38, steps per second: 62, episode reward: 2.000, mean reward: 0.053 [0.000, 1.000], mean action: 5.632 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.009], loss: 0.012724, mean_absolute_error: 1.245590, mean_q: 1.341343\n",
            "  26581/150000: episode: 848, duration: 1.634s, episode steps: 116, steps per second: 71, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 5.319 [2.000, 19.000], mean observation: -0.635 [-1.009, 1.057], loss: 0.011953, mean_absolute_error: 1.250170, mean_q: 1.345138\n",
            "  26610/150000: episode: 849, duration: 0.498s, episode steps: 29, steps per second: 58, episode reward: 2.000, mean reward: 0.069 [0.000, 1.000], mean action: 6.172 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.006], loss: 0.010801, mean_absolute_error: 1.250033, mean_q: 1.360197\n",
            "  26688/150000: episode: 850, duration: 1.137s, episode steps: 78, steps per second: 69, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 6.897 [0.000, 17.000], mean observation: -0.634 [-1.009, 1.000], loss: 0.011726, mean_absolute_error: 1.249158, mean_q: 1.342098\n",
            "  26741/150000: episode: 851, duration: 0.842s, episode steps: 53, steps per second: 63, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 7.321 [3.000, 20.000], mean observation: -0.609 [-1.011, 8.114], loss: 0.009463, mean_absolute_error: 1.248027, mean_q: 1.340214\n",
            "  26807/150000: episode: 852, duration: 0.996s, episode steps: 66, steps per second: 66, episode reward: 0.900, mean reward: 0.014 [0.000, 0.900], mean action: 3.939 [2.000, 15.000], mean observation: -0.622 [-1.011, 7.662], loss: 0.010897, mean_absolute_error: 1.249192, mean_q: 1.341807\n",
            "  26955/150000: episode: 853, duration: 2.042s, episode steps: 148, steps per second: 72, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 5.696 [0.000, 17.000], mean observation: -0.630 [-1.011, 1.360], loss: 0.009524, mean_absolute_error: 1.237386, mean_q: 1.332569\n",
            "  26985/150000: episode: 854, duration: 0.500s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.333 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005787, mean_absolute_error: 1.237279, mean_q: 1.339625\n",
            "  26997/150000: episode: 855, duration: 0.288s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.000], mean action: 5.250 [1.000, 12.000], mean observation: -0.636 [-1.037, 1.000], loss: 0.005754, mean_absolute_error: 1.232283, mean_q: 1.326360\n",
            "  27027/150000: episode: 856, duration: 0.506s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.033 [5.000, 6.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.010444, mean_absolute_error: 1.231069, mean_q: 1.333811\n",
            "  27063/150000: episode: 857, duration: 0.592s, episode steps: 36, steps per second: 61, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 4.417 [0.000, 18.000], mean observation: -0.634 [-1.011, 1.007], loss: 0.006752, mean_absolute_error: 1.232916, mean_q: 1.321665\n",
            "  27118/150000: episode: 858, duration: 0.834s, episode steps: 55, steps per second: 66, episode reward: 2.000, mean reward: 0.036 [0.000, 1.000], mean action: 2.691 [0.000, 19.000], mean observation: -0.632 [-1.011, 1.013], loss: 0.008498, mean_absolute_error: 1.224189, mean_q: 1.315645\n",
            "  27148/150000: episode: 859, duration: 0.499s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.700 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006502, mean_absolute_error: 1.218976, mean_q: 1.325253\n",
            "  27180/150000: episode: 860, duration: 0.545s, episode steps: 32, steps per second: 59, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 5.344 [4.000, 11.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.010222, mean_absolute_error: 1.219504, mean_q: 1.323184\n",
            "  27332/150000: episode: 861, duration: 2.107s, episode steps: 152, steps per second: 72, episode reward: 0.900, mean reward: 0.006 [0.000, 0.900], mean action: 9.020 [1.000, 20.000], mean observation: -0.633 [-1.009, 6.188], loss: 0.009165, mean_absolute_error: 1.214698, mean_q: 1.308013\n",
            "  27379/150000: episode: 862, duration: 0.719s, episode steps: 47, steps per second: 65, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 5.298 [2.000, 20.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.010641, mean_absolute_error: 1.216394, mean_q: 1.312723\n",
            "  27409/150000: episode: 863, duration: 0.535s, episode steps: 30, steps per second: 56, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.433 [2.000, 13.000], mean observation: -0.634 [-1.011, 1.015], loss: 0.010400, mean_absolute_error: 1.217154, mean_q: 1.316911\n",
            "  27443/150000: episode: 864, duration: 0.551s, episode steps: 34, steps per second: 62, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 5.676 [4.000, 20.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.007801, mean_absolute_error: 1.222532, mean_q: 1.315530\n",
            "  27485/150000: episode: 865, duration: 0.671s, episode steps: 42, steps per second: 63, episode reward: 2.000, mean reward: 0.048 [0.000, 1.000], mean action: 5.619 [1.000, 16.000], mean observation: -0.632 [-1.011, 1.012], loss: 0.009982, mean_absolute_error: 1.221711, mean_q: 1.320804\n",
            "  27659/150000: episode: 866, duration: 2.378s, episode steps: 174, steps per second: 73, episode reward: 0.000, mean reward: 0.000 [-1.000, 0.900], mean action: 5.879 [0.000, 20.000], mean observation: -0.628 [-1.017, 8.244], loss: 0.010940, mean_absolute_error: 1.227156, mean_q: 1.324402\n",
            "  27695/150000: episode: 867, duration: 0.585s, episode steps: 36, steps per second: 61, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 5.222 [0.000, 18.000], mean observation: -0.634 [-1.011, 1.007], loss: 0.013041, mean_absolute_error: 1.237740, mean_q: 1.342175\n",
            "  28095/150000: episode: 868, duration: 5.386s, episode steps: 400, steps per second: 74, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 9.307 [0.000, 20.000], mean observation: -0.636 [-1.011, 1.060], loss: 0.009876, mean_absolute_error: 1.216146, mean_q: 1.308502\n",
            "  28125/150000: episode: 869, duration: 0.505s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.500 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.006559, mean_absolute_error: 1.205534, mean_q: 1.298968\n",
            "  28162/150000: episode: 870, duration: 0.615s, episode steps: 37, steps per second: 60, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 4.946 [2.000, 6.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.011508, mean_absolute_error: 1.202949, mean_q: 1.294548\n",
            "  28199/150000: episode: 871, duration: 0.598s, episode steps: 37, steps per second: 62, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 7.270 [2.000, 20.000], mean observation: -0.634 [-1.011, 1.015], loss: 0.009254, mean_absolute_error: 1.201205, mean_q: 1.299852\n",
            "  28248/150000: episode: 872, duration: 0.744s, episode steps: 49, steps per second: 66, episode reward: 2.000, mean reward: 0.041 [0.000, 1.000], mean action: 18.490 [0.000, 20.000], mean observation: -0.632 [-1.009, 1.016], loss: 0.011437, mean_absolute_error: 1.205255, mean_q: 1.300635\n",
            "  28278/150000: episode: 873, duration: 0.501s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.500 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.013145, mean_absolute_error: 1.202838, mean_q: 1.309919\n",
            "  28308/150000: episode: 874, duration: 0.523s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.433 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.005015, mean_absolute_error: 1.212387, mean_q: 1.308639\n",
            "  28338/150000: episode: 875, duration: 0.504s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.867 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.011123, mean_absolute_error: 1.210170, mean_q: 1.302469\n",
            "  28368/150000: episode: 876, duration: 0.520s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.800 [0.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.009138, mean_absolute_error: 1.213495, mean_q: 1.311584\n",
            "  28394/150000: episode: 877, duration: 0.453s, episode steps: 26, steps per second: 57, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 5.769 [3.000, 17.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.014123, mean_absolute_error: 1.212464, mean_q: 1.317286\n",
            "  28474/150000: episode: 878, duration: 1.160s, episode steps: 80, steps per second: 69, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 5.600 [2.000, 20.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.011561, mean_absolute_error: 1.224317, mean_q: 1.323881\n",
            "  28562/150000: episode: 879, duration: 1.266s, episode steps: 88, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 6.125 [2.000, 20.000], mean observation: -0.637 [-1.009, 1.000], loss: 0.008113, mean_absolute_error: 1.231189, mean_q: 1.324905\n",
            "  28665/150000: episode: 880, duration: 1.430s, episode steps: 103, steps per second: 72, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 8.825 [0.000, 19.000], mean observation: -0.631 [-1.011, 1.224], loss: 0.009057, mean_absolute_error: 1.226702, mean_q: 1.321944\n",
            "  28704/150000: episode: 881, duration: 0.636s, episode steps: 39, steps per second: 61, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 7.641 [1.000, 19.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.011544, mean_absolute_error: 1.230647, mean_q: 1.326917\n",
            "  28734/150000: episode: 882, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.833 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.011734, mean_absolute_error: 1.225443, mean_q: 1.340554\n",
            "  28773/150000: episode: 883, duration: 0.644s, episode steps: 39, steps per second: 61, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 6.077 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.006729, mean_absolute_error: 1.235493, mean_q: 1.339795\n",
            "  28886/150000: episode: 884, duration: 1.588s, episode steps: 113, steps per second: 71, episode reward: 0.000, mean reward: 0.000 [-1.000, 0.900], mean action: 7.841 [1.000, 20.000], mean observation: -0.625 [-1.023, 9.056], loss: 0.012184, mean_absolute_error: 1.232150, mean_q: 1.322198\n",
            "  28958/150000: episode: 885, duration: 1.071s, episode steps: 72, steps per second: 67, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 12.208 [1.000, 20.000], mean observation: -0.622 [-1.037, 8.189], loss: 0.010639, mean_absolute_error: 1.221755, mean_q: 1.309968\n",
            "  29064/150000: episode: 886, duration: 1.472s, episode steps: 106, steps per second: 72, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 7.708 [2.000, 17.000], mean observation: -0.635 [-1.009, 1.081], loss: 0.010009, mean_absolute_error: 1.209112, mean_q: 1.306412\n",
            "  29195/150000: episode: 887, duration: 1.798s, episode steps: 131, steps per second: 73, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 6.450 [0.000, 20.000], mean observation: -0.631 [-1.011, 1.266], loss: 0.009150, mean_absolute_error: 1.214636, mean_q: 1.307730\n",
            "  29225/150000: episode: 888, duration: 0.518s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.533 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.012321, mean_absolute_error: 1.215553, mean_q: 1.315354\n",
            "  29266/150000: episode: 889, duration: 0.634s, episode steps: 41, steps per second: 65, episode reward: 2.000, mean reward: 0.049 [0.000, 1.000], mean action: 5.463 [2.000, 19.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.012828, mean_absolute_error: 1.213946, mean_q: 1.317610\n",
            "  29316/150000: episode: 890, duration: 0.771s, episode steps: 50, steps per second: 65, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 5.460 [5.000, 17.000], mean observation: -0.611 [-1.011, 7.668], loss: 0.010877, mean_absolute_error: 1.218540, mean_q: 1.315998\n",
            "  29346/150000: episode: 891, duration: 0.505s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 15.733 [3.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006729, mean_absolute_error: 1.216694, mean_q: 1.302432\n",
            "  29393/150000: episode: 892, duration: 0.736s, episode steps: 47, steps per second: 64, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 5.489 [1.000, 19.000], mean observation: -0.634 [-1.011, 1.004], loss: 0.010260, mean_absolute_error: 1.221006, mean_q: 1.328263\n",
            "  29425/150000: episode: 893, duration: 0.548s, episode steps: 32, steps per second: 58, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 12.062 [2.000, 20.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.008412, mean_absolute_error: 1.218317, mean_q: 1.308262\n",
            "  29452/150000: episode: 894, duration: 0.474s, episode steps: 27, steps per second: 57, episode reward: 2.000, mean reward: 0.074 [0.000, 1.000], mean action: 5.630 [2.000, 13.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.010189, mean_absolute_error: 1.212666, mean_q: 1.308380\n",
            "  29490/150000: episode: 895, duration: 0.614s, episode steps: 38, steps per second: 62, episode reward: 2.000, mean reward: 0.053 [0.000, 1.000], mean action: 5.632 [2.000, 19.000], mean observation: -0.633 [-1.011, 1.009], loss: 0.010897, mean_absolute_error: 1.214994, mean_q: 1.320081\n",
            "  29520/150000: episode: 896, duration: 0.499s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 12.900 [4.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010710, mean_absolute_error: 1.219378, mean_q: 1.312440\n",
            "  29600/150000: episode: 897, duration: 1.191s, episode steps: 80, steps per second: 67, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 5.438 [0.000, 20.000], mean observation: -0.611 [-1.062, 9.684], loss: 0.010059, mean_absolute_error: 1.211649, mean_q: 1.310173\n",
            "  29692/150000: episode: 898, duration: 1.292s, episode steps: 92, steps per second: 71, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 4.913 [3.000, 11.000], mean observation: -0.633 [-1.009, 1.353], loss: 0.009042, mean_absolute_error: 1.206438, mean_q: 1.307073\n",
            "  29731/150000: episode: 899, duration: 0.624s, episode steps: 39, steps per second: 62, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 10.026 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.005668, mean_absolute_error: 1.213825, mean_q: 1.309066\n",
            "  29757/150000: episode: 900, duration: 0.463s, episode steps: 26, steps per second: 56, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 5.038 [0.000, 12.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.012568, mean_absolute_error: 1.213882, mean_q: 1.313966\n",
            "  29782/150000: episode: 901, duration: 0.459s, episode steps: 25, steps per second: 54, episode reward: 2.000, mean reward: 0.080 [0.000, 1.000], mean action: 5.360 [5.000, 13.000], mean observation: -0.634 [-1.011, 1.015], loss: 0.007062, mean_absolute_error: 1.210176, mean_q: 1.314688\n",
            "  29873/150000: episode: 902, duration: 1.307s, episode steps: 91, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 4.923 [1.000, 19.000], mean observation: -0.637 [-1.009, 1.288], loss: 0.009885, mean_absolute_error: 1.217175, mean_q: 1.311101\n",
            "  29903/150000: episode: 903, duration: 0.505s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.009550, mean_absolute_error: 1.212950, mean_q: 1.309871\n",
            "  30005/150000: episode: 904, duration: 1.435s, episode steps: 102, steps per second: 71, episode reward: 0.500, mean reward: 0.005 [0.000, 0.200], mean action: 5.549 [3.000, 20.000], mean observation: -0.635 [-1.009, 1.467], loss: 0.008113, mean_absolute_error: 1.220195, mean_q: 1.312415\n",
            "  30042/150000: episode: 905, duration: 0.592s, episode steps: 37, steps per second: 63, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.757 [2.000, 18.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.015579, mean_absolute_error: 1.219251, mean_q: 1.324432\n",
            "  30072/150000: episode: 906, duration: 0.550s, episode steps: 30, steps per second: 55, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.400 [2.000, 20.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.009868, mean_absolute_error: 1.218615, mean_q: 1.319943\n",
            "  30177/150000: episode: 907, duration: 1.472s, episode steps: 105, steps per second: 71, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 5.962 [1.000, 14.000], mean observation: -0.633 [-1.009, 1.136], loss: 0.007289, mean_absolute_error: 1.223572, mean_q: 1.318661\n",
            "  30232/150000: episode: 908, duration: 0.846s, episode steps: 55, steps per second: 65, episode reward: 0.900, mean reward: 0.016 [0.000, 0.900], mean action: 7.218 [1.000, 20.000], mean observation: -0.606 [-1.067, 9.570], loss: 0.011412, mean_absolute_error: 1.210183, mean_q: 1.299756\n",
            "  30370/150000: episode: 909, duration: 1.895s, episode steps: 138, steps per second: 73, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 6.572 [0.000, 20.000], mean observation: -0.635 [-1.009, 1.278], loss: 0.010858, mean_absolute_error: 1.207986, mean_q: 1.308529\n",
            "  30444/150000: episode: 910, duration: 1.067s, episode steps: 74, steps per second: 69, episode reward: 2.000, mean reward: 0.027 [0.000, 1.000], mean action: 5.041 [1.000, 17.000], mean observation: -0.631 [-1.011, 1.014], loss: 0.009398, mean_absolute_error: 1.207939, mean_q: 1.305596\n",
            "  30530/150000: episode: 911, duration: 1.250s, episode steps: 86, steps per second: 69, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 7.500 [0.000, 19.000], mean observation: -0.616 [-1.013, 8.033], loss: 0.009943, mean_absolute_error: 1.220625, mean_q: 1.320829\n",
            "  30622/150000: episode: 912, duration: 1.302s, episode steps: 92, steps per second: 71, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 8.424 [5.000, 20.000], mean observation: -0.631 [-1.011, 1.000], loss: 0.011167, mean_absolute_error: 1.217218, mean_q: 1.314675\n",
            "  30671/150000: episode: 913, duration: 0.738s, episode steps: 49, steps per second: 66, episode reward: 2.000, mean reward: 0.041 [0.000, 1.000], mean action: 5.490 [2.000, 14.000], mean observation: -0.632 [-1.011, 1.012], loss: 0.009448, mean_absolute_error: 1.219957, mean_q: 1.315238\n",
            "  30728/150000: episode: 914, duration: 0.862s, episode steps: 57, steps per second: 66, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 8.667 [5.000, 19.000], mean observation: -0.612 [-1.009, 7.830], loss: 0.010959, mean_absolute_error: 1.219291, mean_q: 1.306511\n",
            "  30838/150000: episode: 915, duration: 1.570s, episode steps: 110, steps per second: 70, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 7.464 [0.000, 19.000], mean observation: -0.631 [-1.009, 1.564], loss: 0.011409, mean_absolute_error: 1.209584, mean_q: 1.307932\n",
            "  30933/150000: episode: 916, duration: 1.372s, episode steps: 95, steps per second: 69, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 4.263 [0.000, 19.000], mean observation: -0.636 [-1.009, 1.025], loss: 0.011947, mean_absolute_error: 1.204402, mean_q: 1.302297\n",
            "  30948/150000: episode: 917, duration: 0.369s, episode steps: 15, steps per second: 41, episode reward: 0.900, mean reward: 0.060 [0.000, 0.900], mean action: 8.600 [7.000, 14.000], mean observation: -0.630 [-1.009, 1.707], loss: 0.013364, mean_absolute_error: 1.204106, mean_q: 1.293522\n",
            "  30985/150000: episode: 918, duration: 0.649s, episode steps: 37, steps per second: 57, episode reward: 1.000, mean reward: 0.027 [0.000, 0.900], mean action: 8.432 [3.000, 13.000], mean observation: -0.634 [-1.009, 1.017], loss: 0.008666, mean_absolute_error: 1.203168, mean_q: 1.292796\n",
            "  31052/150000: episode: 919, duration: 1.030s, episode steps: 67, steps per second: 65, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 5.493 [1.000, 18.000], mean observation: -0.613 [-1.015, 8.397], loss: 0.010019, mean_absolute_error: 1.202518, mean_q: 1.290381\n",
            "  31137/150000: episode: 920, duration: 1.226s, episode steps: 85, steps per second: 69, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 4.882 [0.000, 19.000], mean observation: -0.635 [-1.009, 1.000], loss: 0.010007, mean_absolute_error: 1.197560, mean_q: 1.292826\n",
            "  31192/150000: episode: 921, duration: 0.860s, episode steps: 55, steps per second: 64, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 5.345 [5.000, 19.000], mean observation: -0.611 [-1.009, 8.189], loss: 0.013981, mean_absolute_error: 1.194772, mean_q: 1.307331\n",
            "  31337/150000: episode: 922, duration: 2.030s, episode steps: 145, steps per second: 71, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 6.214 [0.000, 19.000], mean observation: -0.628 [-1.009, 1.012], loss: 0.009300, mean_absolute_error: 1.203478, mean_q: 1.294840\n",
            "  31430/150000: episode: 923, duration: 1.309s, episode steps: 93, steps per second: 71, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.871 [5.000, 18.000], mean observation: -0.631 [-1.011, 1.370], loss: 0.010140, mean_absolute_error: 1.197276, mean_q: 1.289494\n",
            "  31512/150000: episode: 924, duration: 1.167s, episode steps: 82, steps per second: 70, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 6.695 [0.000, 20.000], mean observation: -0.636 [-1.009, 1.141], loss: 0.010573, mean_absolute_error: 1.191840, mean_q: 1.287476\n",
            "  31542/150000: episode: 925, duration: 0.518s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 19.133 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008223, mean_absolute_error: 1.189332, mean_q: 1.299020\n",
            "  31591/150000: episode: 926, duration: 0.746s, episode steps: 49, steps per second: 66, episode reward: 2.000, mean reward: 0.041 [0.000, 1.000], mean action: 14.857 [5.000, 20.000], mean observation: -0.632 [-1.011, 1.013], loss: 0.008283, mean_absolute_error: 1.186902, mean_q: 1.275279\n",
            "  31621/150000: episode: 927, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004500, mean_absolute_error: 1.187104, mean_q: 1.273457\n",
            "  31707/150000: episode: 928, duration: 1.217s, episode steps: 86, steps per second: 71, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 7.407 [4.000, 20.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.010436, mean_absolute_error: 1.182181, mean_q: 1.272151\n",
            "  31717/150000: episode: 929, duration: 0.245s, episode steps: 10, steps per second: 41, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 5.700 [5.000, 12.000], mean observation: -0.632 [-1.041, 1.492], loss: 0.012815, mean_absolute_error: 1.185621, mean_q: 1.283955\n",
            "  31846/150000: episode: 930, duration: 1.791s, episode steps: 129, steps per second: 72, episode reward: 0.900, mean reward: 0.007 [0.000, 0.200], mean action: 6.186 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.009975, mean_absolute_error: 1.181061, mean_q: 1.277598\n",
            "  31875/150000: episode: 931, duration: 0.482s, episode steps: 29, steps per second: 60, episode reward: 2.000, mean reward: 0.069 [0.000, 1.000], mean action: 6.172 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.004], loss: 0.004781, mean_absolute_error: 1.179971, mean_q: 1.267251\n",
            "  32157/150000: episode: 932, duration: 3.753s, episode steps: 282, steps per second: 75, episode reward: 1.000, mean reward: 0.004 [0.000, 0.900], mean action: 8.862 [0.000, 20.000], mean observation: -0.631 [-1.009, 8.478], loss: 0.008719, mean_absolute_error: 1.165028, mean_q: 1.251860\n",
            "  32557/150000: episode: 933, duration: 5.325s, episode steps: 400, steps per second: 75, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 6.723 [0.000, 20.000], mean observation: -0.629 [-1.011, 5.166], loss: 0.009666, mean_absolute_error: 1.141537, mean_q: 1.230486\n",
            "  32584/150000: episode: 934, duration: 0.509s, episode steps: 27, steps per second: 53, episode reward: 0.900, mean reward: 0.033 [0.000, 0.900], mean action: 3.889 [0.000, 19.000], mean observation: -0.635 [-1.009, 1.000], loss: 0.007549, mean_absolute_error: 1.128295, mean_q: 1.209611\n",
            "  32662/150000: episode: 935, duration: 1.120s, episode steps: 78, steps per second: 70, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 4.769 [2.000, 20.000], mean observation: -0.611 [-1.086, 9.868], loss: 0.008588, mean_absolute_error: 1.132071, mean_q: 1.222596\n",
            "  32692/150000: episode: 936, duration: 0.501s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.167 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.015], loss: 0.008072, mean_absolute_error: 1.136040, mean_q: 1.229580\n",
            "  33063/150000: episode: 937, duration: 4.905s, episode steps: 371, steps per second: 76, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 9.183 [1.000, 20.000], mean observation: -0.634 [-1.009, 1.842], loss: 0.010359, mean_absolute_error: 1.140853, mean_q: 1.236026\n",
            "  33121/150000: episode: 938, duration: 0.843s, episode steps: 58, steps per second: 69, episode reward: 2.000, mean reward: 0.034 [0.000, 1.000], mean action: 18.517 [1.000, 20.000], mean observation: -0.633 [-1.011, 1.518], loss: 0.009318, mean_absolute_error: 1.137392, mean_q: 1.227876\n",
            "  33143/150000: episode: 939, duration: 0.406s, episode steps: 22, steps per second: 54, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 7.364 [5.000, 20.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007813, mean_absolute_error: 1.140660, mean_q: 1.233531\n",
            "  33174/150000: episode: 940, duration: 0.538s, episode steps: 31, steps per second: 58, episode reward: 2.000, mean reward: 0.065 [0.000, 1.000], mean action: 5.935 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.008140, mean_absolute_error: 1.137798, mean_q: 1.240693\n",
            "  33211/150000: episode: 941, duration: 0.579s, episode steps: 37, steps per second: 64, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 7.297 [5.000, 16.000], mean observation: -0.633 [-1.011, 1.007], loss: 0.011501, mean_absolute_error: 1.139123, mean_q: 1.226723\n",
            "  33241/150000: episode: 942, duration: 0.509s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.467 [1.000, 11.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004391, mean_absolute_error: 1.142181, mean_q: 1.227722\n",
            "  33375/150000: episode: 943, duration: 1.855s, episode steps: 134, steps per second: 72, episode reward: 2.000, mean reward: 0.015 [0.000, 1.000], mean action: 5.597 [0.000, 20.000], mean observation: -0.632 [-1.011, 1.008], loss: 0.011191, mean_absolute_error: 1.144127, mean_q: 1.245905\n",
            "  33405/150000: episode: 944, duration: 0.488s, episode steps: 30, steps per second: 61, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.200 [0.000, 19.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.011223, mean_absolute_error: 1.155771, mean_q: 1.249383\n",
            "  33442/150000: episode: 945, duration: 0.593s, episode steps: 37, steps per second: 62, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 4.486 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.008129, mean_absolute_error: 1.155506, mean_q: 1.250982\n",
            "  33487/150000: episode: 946, duration: 0.676s, episode steps: 45, steps per second: 67, episode reward: 2.000, mean reward: 0.044 [0.000, 1.000], mean action: 13.956 [5.000, 20.000], mean observation: -0.633 [-1.011, 1.304], loss: 0.007418, mean_absolute_error: 1.156482, mean_q: 1.246314\n",
            "  33517/150000: episode: 947, duration: 0.499s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.067 [2.000, 14.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.011116, mean_absolute_error: 1.155729, mean_q: 1.245391\n",
            "  33544/150000: episode: 948, duration: 0.465s, episode steps: 27, steps per second: 58, episode reward: 2.000, mean reward: 0.074 [0.000, 1.000], mean action: 5.296 [5.000, 13.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.010069, mean_absolute_error: 1.150913, mean_q: 1.252529\n",
            "  33564/150000: episode: 949, duration: 0.381s, episode steps: 20, steps per second: 53, episode reward: 2.000, mean reward: 0.100 [0.000, 1.000], mean action: 5.300 [4.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007175, mean_absolute_error: 1.155828, mean_q: 1.249467\n",
            "  33617/150000: episode: 950, duration: 0.812s, episode steps: 53, steps per second: 65, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 10.792 [5.000, 19.000], mean observation: -0.609 [-1.009, 8.114], loss: 0.008432, mean_absolute_error: 1.157199, mean_q: 1.245221\n",
            "  33671/150000: episode: 951, duration: 0.805s, episode steps: 54, steps per second: 67, episode reward: 2.000, mean reward: 0.037 [0.000, 1.000], mean action: 5.796 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.009265, mean_absolute_error: 1.151594, mean_q: 1.245504\n",
            "  33685/150000: episode: 952, duration: 0.306s, episode steps: 14, steps per second: 46, episode reward: 2.000, mean reward: 0.143 [0.000, 1.000], mean action: 6.929 [3.000, 16.000], mean observation: -0.635 [-1.040, 1.033], loss: 0.014069, mean_absolute_error: 1.157030, mean_q: 1.255535\n",
            "  33781/150000: episode: 953, duration: 1.321s, episode steps: 96, steps per second: 73, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 5.385 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.008818, mean_absolute_error: 1.157413, mean_q: 1.249397\n",
            "  33834/150000: episode: 954, duration: 0.821s, episode steps: 53, steps per second: 65, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 5.906 [1.000, 17.000], mean observation: -0.631 [-1.009, 1.012], loss: 0.007569, mean_absolute_error: 1.161953, mean_q: 1.246167\n",
            "  33880/150000: episode: 955, duration: 0.716s, episode steps: 46, steps per second: 64, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 5.522 [1.000, 19.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.008717, mean_absolute_error: 1.157308, mean_q: 1.256145\n",
            "  33910/150000: episode: 956, duration: 0.500s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.967 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.005250, mean_absolute_error: 1.161015, mean_q: 1.248546\n",
            "  33920/150000: episode: 957, duration: 0.255s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 5.700 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.007568, mean_absolute_error: 1.162143, mean_q: 1.241381\n",
            "  33996/150000: episode: 958, duration: 1.104s, episode steps: 76, steps per second: 69, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 11.961 [0.000, 17.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.009008, mean_absolute_error: 1.160146, mean_q: 1.248607\n",
            "  34033/150000: episode: 959, duration: 0.592s, episode steps: 37, steps per second: 63, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 9.514 [1.000, 16.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.011655, mean_absolute_error: 1.155506, mean_q: 1.246201\n",
            "  34136/150000: episode: 960, duration: 1.425s, episode steps: 103, steps per second: 72, episode reward: 0.000, mean reward: 0.000 [-1.000, 0.900], mean action: 11.903 [0.000, 20.000], mean observation: -0.625 [-1.025, 8.986], loss: 0.008462, mean_absolute_error: 1.149025, mean_q: 1.242348\n",
            "  34174/150000: episode: 961, duration: 0.610s, episode steps: 38, steps per second: 62, episode reward: 2.000, mean reward: 0.053 [0.000, 1.000], mean action: 13.289 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.009], loss: 0.008396, mean_absolute_error: 1.156352, mean_q: 1.245178\n",
            "  34204/150000: episode: 962, duration: 0.495s, episode steps: 30, steps per second: 61, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [0.000, 10.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006840, mean_absolute_error: 1.147039, mean_q: 1.233880\n",
            "  34337/150000: episode: 963, duration: 1.866s, episode steps: 133, steps per second: 71, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 7.955 [0.000, 18.000], mean observation: -0.627 [-1.009, 1.007], loss: 0.007791, mean_absolute_error: 1.146798, mean_q: 1.236694\n",
            "  34360/150000: episode: 964, duration: 0.417s, episode steps: 23, steps per second: 55, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 5.913 [3.000, 20.000], mean observation: -0.634 [-1.011, 1.004], loss: 0.007605, mean_absolute_error: 1.143446, mean_q: 1.228473\n",
            "  34415/150000: episode: 965, duration: 0.821s, episode steps: 55, steps per second: 67, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 5.436 [1.000, 14.000], mean observation: -0.609 [-1.017, 8.381], loss: 0.007697, mean_absolute_error: 1.146787, mean_q: 1.236568\n",
            "  34472/150000: episode: 966, duration: 0.841s, episode steps: 57, steps per second: 68, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 5.368 [0.000, 14.000], mean observation: -0.605 [-1.057, 9.690], loss: 0.010402, mean_absolute_error: 1.148505, mean_q: 1.232522\n",
            "  34542/150000: episode: 967, duration: 1.022s, episode steps: 70, steps per second: 69, episode reward: 2.000, mean reward: 0.029 [0.000, 1.100], mean action: 4.514 [0.000, 18.000], mean observation: -0.632 [-1.011, 2.461], loss: 0.010492, mean_absolute_error: 1.138565, mean_q: 1.229373\n",
            "  34572/150000: episode: 968, duration: 0.501s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010262, mean_absolute_error: 1.147638, mean_q: 1.246801\n",
            "  34602/150000: episode: 969, duration: 0.492s, episode steps: 30, steps per second: 61, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.567 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007211, mean_absolute_error: 1.147835, mean_q: 1.254024\n",
            "  34635/150000: episode: 970, duration: 0.549s, episode steps: 33, steps per second: 60, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 6.000 [4.000, 18.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.012559, mean_absolute_error: 1.146861, mean_q: 1.237625\n",
            "  34677/150000: episode: 971, duration: 0.690s, episode steps: 42, steps per second: 61, episode reward: 1.000, mean reward: 0.024 [0.000, 0.900], mean action: 12.571 [5.000, 19.000], mean observation: -0.632 [-1.011, 1.012], loss: 0.011825, mean_absolute_error: 1.153173, mean_q: 1.254159\n",
            "  34787/150000: episode: 972, duration: 1.529s, episode steps: 110, steps per second: 72, episode reward: 2.000, mean reward: 0.018 [0.000, 1.000], mean action: 8.882 [0.000, 18.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.009085, mean_absolute_error: 1.154384, mean_q: 1.249021\n",
            "  34993/150000: episode: 973, duration: 2.767s, episode steps: 206, steps per second: 74, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 6.107 [1.000, 20.000], mean observation: -0.636 [-1.009, 1.361], loss: 0.008490, mean_absolute_error: 1.159218, mean_q: 1.248283\n",
            "  35016/150000: episode: 974, duration: 0.418s, episode steps: 23, steps per second: 55, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 8.348 [2.000, 13.000], mean observation: -0.634 [-1.011, 1.004], loss: 0.002571, mean_absolute_error: 1.159896, mean_q: 1.262976\n",
            "  35142/150000: episode: 975, duration: 1.706s, episode steps: 126, steps per second: 74, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 12.484 [0.000, 20.000], mean observation: -0.627 [-1.009, 1.000], loss: 0.008296, mean_absolute_error: 1.163311, mean_q: 1.258270\n",
            "  35282/150000: episode: 976, duration: 1.910s, episode steps: 140, steps per second: 73, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 6.350 [0.000, 19.000], mean observation: -0.630 [-1.011, 1.196], loss: 0.008409, mean_absolute_error: 1.158917, mean_q: 1.249527\n",
            "  35311/150000: episode: 977, duration: 0.490s, episode steps: 29, steps per second: 59, episode reward: 2.000, mean reward: 0.069 [0.000, 1.000], mean action: 17.276 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.006714, mean_absolute_error: 1.145841, mean_q: 1.253314\n",
            "  35409/150000: episode: 978, duration: 1.343s, episode steps: 98, steps per second: 73, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 12.276 [5.000, 19.000], mean observation: -0.630 [-1.009, 1.113], loss: 0.008498, mean_absolute_error: 1.150857, mean_q: 1.242930\n",
            "  35525/150000: episode: 979, duration: 1.597s, episode steps: 116, steps per second: 73, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 7.190 [3.000, 19.000], mean observation: -0.629 [-1.011, 1.005], loss: 0.008366, mean_absolute_error: 1.146316, mean_q: 1.241250\n",
            "  35565/150000: episode: 980, duration: 0.602s, episode steps: 40, steps per second: 66, episode reward: 2.000, mean reward: 0.050 [0.000, 1.000], mean action: 10.875 [0.000, 19.000], mean observation: -0.632 [-1.009, 1.015], loss: 0.008401, mean_absolute_error: 1.146853, mean_q: 1.242493\n",
            "  35646/150000: episode: 981, duration: 1.126s, episode steps: 81, steps per second: 72, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 7.815 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.230], loss: 0.007013, mean_absolute_error: 1.150978, mean_q: 1.246616\n",
            "  35711/150000: episode: 982, duration: 0.990s, episode steps: 65, steps per second: 66, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 6.923 [3.000, 18.000], mean observation: -0.632 [-1.009, 2.505], loss: 0.007252, mean_absolute_error: 1.148374, mean_q: 1.232683\n",
            "  35765/150000: episode: 983, duration: 0.810s, episode steps: 54, steps per second: 67, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 5.407 [3.000, 15.000], mean observation: -0.612 [-1.018, 7.981], loss: 0.008029, mean_absolute_error: 1.140374, mean_q: 1.228930\n",
            "  35785/150000: episode: 984, duration: 0.384s, episode steps: 20, steps per second: 52, episode reward: 2.000, mean reward: 0.100 [0.000, 1.000], mean action: 8.100 [0.000, 18.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007122, mean_absolute_error: 1.134848, mean_q: 1.223716\n",
            "  35815/150000: episode: 985, duration: 0.497s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 0.733 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.009634, mean_absolute_error: 1.133793, mean_q: 1.225329\n",
            "  35847/150000: episode: 986, duration: 0.540s, episode steps: 32, steps per second: 59, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 4.531 [0.000, 19.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.011730, mean_absolute_error: 1.132633, mean_q: 1.221328\n",
            "  35857/150000: episode: 987, duration: 0.251s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 12.400 [5.000, 14.000], mean observation: -0.635 [-1.011, 1.017], loss: 0.001565, mean_absolute_error: 1.125627, mean_q: 1.211983\n",
            "  35919/150000: episode: 988, duration: 0.894s, episode steps: 62, steps per second: 69, episode reward: 2.000, mean reward: 0.032 [0.000, 1.000], mean action: 8.048 [0.000, 20.000], mean observation: -0.632 [-1.011, 1.010], loss: 0.008883, mean_absolute_error: 1.126312, mean_q: 1.209673\n",
            "  35959/150000: episode: 989, duration: 0.616s, episode steps: 40, steps per second: 65, episode reward: 2.000, mean reward: 0.050 [0.000, 1.000], mean action: 4.175 [0.000, 14.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.005929, mean_absolute_error: 1.124374, mean_q: 1.204900\n",
            "  35989/150000: episode: 990, duration: 0.547s, episode steps: 30, steps per second: 55, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.400 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006876, mean_absolute_error: 1.124315, mean_q: 1.214256\n",
            "  36019/150000: episode: 991, duration: 0.497s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.333 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008439, mean_absolute_error: 1.125403, mean_q: 1.213386\n",
            "  36049/150000: episode: 992, duration: 0.506s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.767 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008637, mean_absolute_error: 1.127312, mean_q: 1.230837\n",
            "  36088/150000: episode: 993, duration: 0.643s, episode steps: 39, steps per second: 61, episode reward: 0.900, mean reward: 0.023 [0.000, 0.900], mean action: 13.179 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.020], loss: 0.008904, mean_absolute_error: 1.133985, mean_q: 1.237107\n",
            "  36139/150000: episode: 994, duration: 0.760s, episode steps: 51, steps per second: 67, episode reward: 2.000, mean reward: 0.039 [0.000, 1.000], mean action: 8.118 [5.000, 17.000], mean observation: -0.632 [-1.011, 1.012], loss: 0.011370, mean_absolute_error: 1.133746, mean_q: 1.224984\n",
            "  36156/150000: episode: 995, duration: 0.342s, episode steps: 17, steps per second: 50, episode reward: 2.000, mean reward: 0.118 [0.000, 1.000], mean action: 6.235 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.080], loss: 0.010713, mean_absolute_error: 1.133650, mean_q: 1.239392\n",
            "  36189/150000: episode: 996, duration: 0.537s, episode steps: 33, steps per second: 61, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.333 [4.000, 17.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.011084, mean_absolute_error: 1.135112, mean_q: 1.226484\n",
            "  36226/150000: episode: 997, duration: 0.608s, episode steps: 37, steps per second: 61, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.405 [5.000, 18.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.007916, mean_absolute_error: 1.138351, mean_q: 1.236727\n",
            "  36250/150000: episode: 998, duration: 0.418s, episode steps: 24, steps per second: 57, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 9.167 [4.000, 19.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007468, mean_absolute_error: 1.142553, mean_q: 1.224680\n",
            "  36565/150000: episode: 999, duration: 4.155s, episode steps: 315, steps per second: 76, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 4.911 [0.000, 20.000], mean observation: -0.637 [-1.009, 1.350], loss: 0.008938, mean_absolute_error: 1.135750, mean_q: 1.228616\n",
            "  36814/150000: episode: 1000, duration: 3.331s, episode steps: 249, steps per second: 75, episode reward: 1.000, mean reward: 0.004 [0.000, 0.900], mean action: 8.177 [0.000, 20.000], mean observation: -0.635 [-1.011, 1.341], loss: 0.009197, mean_absolute_error: 1.136367, mean_q: 1.229446\n",
            "  36890/150000: episode: 1001, duration: 1.075s, episode steps: 76, steps per second: 71, episode reward: 2.000, mean reward: 0.026 [0.000, 1.000], mean action: 10.829 [3.000, 18.000], mean observation: -0.631 [-1.011, 1.011], loss: 0.008114, mean_absolute_error: 1.133300, mean_q: 1.221304\n",
            "  36950/150000: episode: 1002, duration: 0.905s, episode steps: 60, steps per second: 66, episode reward: 1.000, mean reward: 0.017 [0.000, 0.900], mean action: 5.317 [0.000, 14.000], mean observation: -0.613 [-1.011, 7.735], loss: 0.008436, mean_absolute_error: 1.141420, mean_q: 1.228804\n",
            "  36980/150000: episode: 1003, duration: 0.507s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.300 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.010101, mean_absolute_error: 1.138002, mean_q: 1.228156\n",
            "  36993/150000: episode: 1004, duration: 0.286s, episode steps: 13, steps per second: 45, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 5.538 [5.000, 12.000], mean observation: -0.635 [-1.039, 1.000], loss: 0.007952, mean_absolute_error: 1.134350, mean_q: 1.225718\n",
            "  37124/150000: episode: 1005, duration: 1.795s, episode steps: 131, steps per second: 73, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 9.046 [1.000, 19.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.008419, mean_absolute_error: 1.134579, mean_q: 1.223024\n",
            "  37213/150000: episode: 1006, duration: 1.280s, episode steps: 89, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 8.079 [1.000, 20.000], mean observation: -0.628 [-1.009, 1.015], loss: 0.010782, mean_absolute_error: 1.135986, mean_q: 1.222113\n",
            "  37250/150000: episode: 1007, duration: 0.587s, episode steps: 37, steps per second: 63, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 4.919 [1.000, 11.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.005438, mean_absolute_error: 1.136512, mean_q: 1.228103\n",
            "  37280/150000: episode: 1008, duration: 0.499s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 14.567 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.005626, mean_absolute_error: 1.136558, mean_q: 1.215637\n",
            "  37307/150000: episode: 1009, duration: 0.463s, episode steps: 27, steps per second: 58, episode reward: 2.000, mean reward: 0.074 [0.000, 1.000], mean action: 6.519 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.006], loss: 0.008236, mean_absolute_error: 1.137691, mean_q: 1.223591\n",
            "  37365/150000: episode: 1010, duration: 0.849s, episode steps: 58, steps per second: 68, episode reward: 2.000, mean reward: 0.034 [0.000, 1.000], mean action: 4.586 [2.000, 18.000], mean observation: -0.632 [-1.011, 1.013], loss: 0.008101, mean_absolute_error: 1.132766, mean_q: 1.220287\n",
            "  37387/150000: episode: 1011, duration: 0.411s, episode steps: 22, steps per second: 54, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 8.364 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.004591, mean_absolute_error: 1.137831, mean_q: 1.233246\n",
            "  37476/150000: episode: 1012, duration: 1.263s, episode steps: 89, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 8.483 [3.000, 19.000], mean observation: -0.633 [-1.009, 1.240], loss: 0.008288, mean_absolute_error: 1.133516, mean_q: 1.230483\n",
            "  37506/150000: episode: 1013, duration: 0.499s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010425, mean_absolute_error: 1.135706, mean_q: 1.229241\n",
            "  37516/150000: episode: 1014, duration: 0.263s, episode steps: 10, steps per second: 38, episode reward: 2.000, mean reward: 0.200 [0.000, 2.000], mean action: 8.900 [5.000, 13.000], mean observation: -0.634 [-1.037, 1.006], loss: 0.013815, mean_absolute_error: 1.130543, mean_q: 1.213541\n",
            "  37916/150000: episode: 1015, duration: 5.246s, episode steps: 400, steps per second: 76, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 5.070 [0.000, 19.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.009549, mean_absolute_error: 1.124082, mean_q: 1.212300\n",
            "  38008/150000: episode: 1016, duration: 1.321s, episode steps: 92, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 10.717 [1.000, 20.000], mean observation: -0.632 [-1.009, 1.011], loss: 0.009569, mean_absolute_error: 1.111143, mean_q: 1.197769\n",
            "  38042/150000: episode: 1017, duration: 0.548s, episode steps: 34, steps per second: 62, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 9.735 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.005420, mean_absolute_error: 1.107703, mean_q: 1.206484\n",
            "  38153/150000: episode: 1018, duration: 1.607s, episode steps: 111, steps per second: 69, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 10.793 [0.000, 18.000], mean observation: -0.625 [-1.046, 9.145], loss: 0.007014, mean_absolute_error: 1.102902, mean_q: 1.188145\n",
            "  38199/150000: episode: 1019, duration: 0.690s, episode steps: 46, steps per second: 67, episode reward: 0.900, mean reward: 0.020 [0.000, 0.900], mean action: 8.826 [2.000, 17.000], mean observation: -0.604 [-1.063, 9.163], loss: 0.005539, mean_absolute_error: 1.093937, mean_q: 1.181921\n",
            "  38235/150000: episode: 1020, duration: 0.566s, episode steps: 36, steps per second: 64, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 5.333 [5.000, 15.000], mean observation: -0.633 [-1.011, 1.009], loss: 0.009745, mean_absolute_error: 1.094759, mean_q: 1.186937\n",
            "  38270/150000: episode: 1021, duration: 0.610s, episode steps: 35, steps per second: 57, episode reward: 1.000, mean reward: 0.029 [0.000, 0.900], mean action: 5.029 [4.000, 20.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.010993, mean_absolute_error: 1.092622, mean_q: 1.189824\n",
            "  38365/150000: episode: 1022, duration: 1.328s, episode steps: 95, steps per second: 72, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 6.084 [1.000, 16.000], mean observation: -0.632 [-1.009, 1.269], loss: 0.009840, mean_absolute_error: 1.093153, mean_q: 1.181696\n",
            "  38377/150000: episode: 1023, duration: 0.266s, episode steps: 12, steps per second: 45, episode reward: 2.000, mean reward: 0.167 [0.000, 1.000], mean action: 6.917 [5.000, 14.000], mean observation: -0.635 [-1.011, 1.020], loss: 0.016222, mean_absolute_error: 1.097251, mean_q: 1.192729\n",
            "  38403/150000: episode: 1024, duration: 0.452s, episode steps: 26, steps per second: 58, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 6.885 [3.000, 17.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.012751, mean_absolute_error: 1.089879, mean_q: 1.174592\n",
            "  38424/150000: episode: 1025, duration: 0.383s, episode steps: 21, steps per second: 55, episode reward: 2.000, mean reward: 0.095 [0.000, 1.000], mean action: 12.905 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.007], loss: 0.007556, mean_absolute_error: 1.093592, mean_q: 1.170105\n",
            "  38447/150000: episode: 1026, duration: 0.422s, episode steps: 23, steps per second: 54, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 9.174 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.009303, mean_absolute_error: 1.095481, mean_q: 1.177733\n",
            "  38512/150000: episode: 1027, duration: 1.006s, episode steps: 65, steps per second: 65, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 4.523 [0.000, 20.000], mean observation: -0.632 [-1.009, 1.013], loss: 0.010168, mean_absolute_error: 1.091845, mean_q: 1.180614\n",
            "  38542/150000: episode: 1028, duration: 0.492s, episode steps: 30, steps per second: 61, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.667 [0.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007207, mean_absolute_error: 1.086951, mean_q: 1.177417\n",
            "  38575/150000: episode: 1029, duration: 0.543s, episode steps: 33, steps per second: 61, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.182 [5.000, 10.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.011549, mean_absolute_error: 1.090512, mean_q: 1.176414\n",
            "  38605/150000: episode: 1030, duration: 0.493s, episode steps: 30, steps per second: 61, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.500 [2.000, 14.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.013256, mean_absolute_error: 1.083053, mean_q: 1.172800\n",
            "  38619/150000: episode: 1031, duration: 0.312s, episode steps: 14, steps per second: 45, episode reward: 2.000, mean reward: 0.143 [0.000, 1.000], mean action: 6.429 [5.000, 18.000], mean observation: -0.635 [-1.040, 1.000], loss: 0.011154, mean_absolute_error: 1.086666, mean_q: 1.167939\n",
            "  39019/150000: episode: 1032, duration: 5.305s, episode steps: 400, steps per second: 75, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 6.072 [0.000, 19.000], mean observation: -0.629 [-1.011, 2.518], loss: 0.007404, mean_absolute_error: 1.090786, mean_q: 1.175639\n",
            "  39071/150000: episode: 1033, duration: 0.796s, episode steps: 52, steps per second: 65, episode reward: 0.900, mean reward: 0.017 [0.000, 0.900], mean action: 5.519 [3.000, 15.000], mean observation: -0.632 [-1.011, 3.014], loss: 0.008821, mean_absolute_error: 1.087252, mean_q: 1.170914\n",
            "  39139/150000: episode: 1034, duration: 1.019s, episode steps: 68, steps per second: 67, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 9.588 [2.000, 19.000], mean observation: -0.631 [-1.011, 1.013], loss: 0.006599, mean_absolute_error: 1.088229, mean_q: 1.174877\n",
            "  39229/150000: episode: 1035, duration: 1.258s, episode steps: 90, steps per second: 72, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.867 [0.000, 19.000], mean observation: -0.634 [-1.009, 1.000], loss: 0.010220, mean_absolute_error: 1.092821, mean_q: 1.180224\n",
            "  39249/150000: episode: 1036, duration: 0.407s, episode steps: 20, steps per second: 49, episode reward: 0.900, mean reward: 0.045 [0.000, 0.900], mean action: 5.250 [2.000, 12.000], mean observation: -0.629 [-1.011, 2.888], loss: 0.009414, mean_absolute_error: 1.092665, mean_q: 1.180594\n",
            "  39280/150000: episode: 1037, duration: 0.542s, episode steps: 31, steps per second: 57, episode reward: 2.000, mean reward: 0.065 [0.000, 1.000], mean action: 5.452 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007684, mean_absolute_error: 1.097805, mean_q: 1.192600\n",
            "  39336/150000: episode: 1038, duration: 0.882s, episode steps: 56, steps per second: 64, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 5.554 [0.000, 19.000], mean observation: -0.605 [-1.108, 9.689], loss: 0.007679, mean_absolute_error: 1.103847, mean_q: 1.190352\n",
            "  39429/150000: episode: 1039, duration: 1.352s, episode steps: 93, steps per second: 69, episode reward: 0.200, mean reward: 0.002 [0.000, 0.200], mean action: 9.258 [0.000, 19.000], mean observation: -0.638 [-1.011, 1.000], loss: 0.010114, mean_absolute_error: 1.105895, mean_q: 1.190978\n",
            "  39459/150000: episode: 1040, duration: 0.526s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005197, mean_absolute_error: 1.104270, mean_q: 1.195397\n",
            "  39492/150000: episode: 1041, duration: 0.579s, episode steps: 33, steps per second: 57, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.061 [5.000, 6.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.006134, mean_absolute_error: 1.109426, mean_q: 1.201859\n",
            "  39535/150000: episode: 1042, duration: 0.726s, episode steps: 43, steps per second: 59, episode reward: 2.000, mean reward: 0.047 [0.000, 1.000], mean action: 5.628 [1.000, 20.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.006856, mean_absolute_error: 1.116630, mean_q: 1.213585\n",
            "  39560/150000: episode: 1043, duration: 0.452s, episode steps: 25, steps per second: 55, episode reward: 2.000, mean reward: 0.080 [0.000, 1.000], mean action: 6.600 [1.000, 17.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.007259, mean_absolute_error: 1.112564, mean_q: 1.189753\n",
            "  39590/150000: episode: 1044, duration: 0.514s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008385, mean_absolute_error: 1.116930, mean_q: 1.206979\n",
            "  39664/150000: episode: 1045, duration: 1.093s, episode steps: 74, steps per second: 68, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 7.203 [1.000, 20.000], mean observation: -0.637 [-1.009, 1.000], loss: 0.008711, mean_absolute_error: 1.110515, mean_q: 1.190393\n",
            "  39696/150000: episode: 1046, duration: 0.514s, episode steps: 32, steps per second: 62, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 8.969 [5.000, 20.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.008848, mean_absolute_error: 1.103210, mean_q: 1.190546\n",
            "  39724/150000: episode: 1047, duration: 0.488s, episode steps: 28, steps per second: 57, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 5.286 [5.000, 13.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.003702, mean_absolute_error: 1.104882, mean_q: 1.191503\n",
            "  39876/150000: episode: 1048, duration: 2.068s, episode steps: 152, steps per second: 73, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 9.671 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.006795, mean_absolute_error: 1.107335, mean_q: 1.193014\n",
            "  39906/150000: episode: 1049, duration: 0.497s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.267 [3.000, 15.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006091, mean_absolute_error: 1.106164, mean_q: 1.197205\n",
            "  39984/150000: episode: 1050, duration: 1.123s, episode steps: 78, steps per second: 69, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 5.462 [0.000, 19.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.009382, mean_absolute_error: 1.109292, mean_q: 1.195845\n",
            "  40014/150000: episode: 1051, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.900 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008119, mean_absolute_error: 1.110017, mean_q: 1.194849\n",
            "  40044/150000: episode: 1052, duration: 0.499s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.833 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007652, mean_absolute_error: 1.113979, mean_q: 1.206582\n",
            "  40053/150000: episode: 1053, duration: 0.214s, episode steps: 9, steps per second: 42, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 5.556 [3.000, 12.000], mean observation: -0.635 [-1.009, 1.023], loss: 0.007911, mean_absolute_error: 1.115303, mean_q: 1.206191\n",
            "  40083/150000: episode: 1054, duration: 0.533s, episode steps: 30, steps per second: 56, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.600 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.007918, mean_absolute_error: 1.117059, mean_q: 1.212736\n",
            "  40192/150000: episode: 1055, duration: 1.501s, episode steps: 109, steps per second: 73, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 7.706 [3.000, 19.000], mean observation: -0.632 [-1.009, 1.419], loss: 0.009432, mean_absolute_error: 1.117068, mean_q: 1.204326\n",
            "  40217/150000: episode: 1056, duration: 0.483s, episode steps: 25, steps per second: 52, episode reward: 2.000, mean reward: 0.080 [0.000, 1.000], mean action: 7.160 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.016], loss: 0.007705, mean_absolute_error: 1.112963, mean_q: 1.202823\n",
            "  40229/150000: episode: 1057, duration: 0.276s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 13.583 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.097], loss: 0.009425, mean_absolute_error: 1.112298, mean_q: 1.197409\n",
            "  40287/150000: episode: 1058, duration: 0.876s, episode steps: 58, steps per second: 66, episode reward: 1.000, mean reward: 0.017 [0.000, 0.900], mean action: 7.069 [5.000, 18.000], mean observation: -0.608 [-1.015, 8.397], loss: 0.007902, mean_absolute_error: 1.107377, mean_q: 1.195139\n",
            "  40379/150000: episode: 1059, duration: 1.274s, episode steps: 92, steps per second: 72, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 8.174 [0.000, 17.000], mean observation: -0.634 [-1.009, 1.000], loss: 0.009164, mean_absolute_error: 1.102273, mean_q: 1.195032\n",
            "  40407/150000: episode: 1060, duration: 0.489s, episode steps: 28, steps per second: 57, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 7.000 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.006], loss: 0.007901, mean_absolute_error: 1.103284, mean_q: 1.205495\n",
            "  40532/150000: episode: 1061, duration: 1.708s, episode steps: 125, steps per second: 73, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 6.344 [2.000, 19.000], mean observation: -0.635 [-1.009, 1.036], loss: 0.011720, mean_absolute_error: 1.113333, mean_q: 1.210510\n",
            "  40627/150000: episode: 1062, duration: 1.346s, episode steps: 95, steps per second: 71, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.916 [0.000, 16.000], mean observation: -0.634 [-1.009, 1.000], loss: 0.008663, mean_absolute_error: 1.123187, mean_q: 1.215131\n",
            "  40672/150000: episode: 1063, duration: 0.670s, episode steps: 45, steps per second: 67, episode reward: 2.000, mean reward: 0.044 [0.000, 1.000], mean action: 12.022 [5.000, 16.000], mean observation: -0.632 [-1.009, 1.016], loss: 0.005368, mean_absolute_error: 1.123098, mean_q: 1.211607\n",
            "  40702/150000: episode: 1064, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.500 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010499, mean_absolute_error: 1.125774, mean_q: 1.216877\n",
            "  40764/150000: episode: 1065, duration: 0.912s, episode steps: 62, steps per second: 68, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 5.500 [4.000, 17.000], mean observation: -0.609 [-1.028, 8.392], loss: 0.007874, mean_absolute_error: 1.124919, mean_q: 1.221872\n",
            "  40794/150000: episode: 1066, duration: 0.506s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.600 [4.000, 15.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.003640, mean_absolute_error: 1.133033, mean_q: 1.230400\n",
            "  40912/150000: episode: 1067, duration: 1.650s, episode steps: 118, steps per second: 72, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 5.424 [1.000, 20.000], mean observation: -0.630 [-1.011, 1.325], loss: 0.009133, mean_absolute_error: 1.136373, mean_q: 1.222594\n",
            "  40930/150000: episode: 1068, duration: 0.354s, episode steps: 18, steps per second: 51, episode reward: 2.000, mean reward: 0.111 [0.000, 1.000], mean action: 9.444 [5.000, 15.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007380, mean_absolute_error: 1.136467, mean_q: 1.219578\n",
            "  40963/150000: episode: 1069, duration: 0.554s, episode steps: 33, steps per second: 60, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 9.091 [5.000, 15.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.011771, mean_absolute_error: 1.129646, mean_q: 1.214998\n",
            "  41154/150000: episode: 1070, duration: 2.579s, episode steps: 191, steps per second: 74, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 9.330 [0.000, 20.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.009596, mean_absolute_error: 1.117567, mean_q: 1.201899\n",
            "  41207/150000: episode: 1071, duration: 0.792s, episode steps: 53, steps per second: 67, episode reward: 0.900, mean reward: 0.017 [0.000, 0.900], mean action: 6.679 [1.000, 20.000], mean observation: -0.607 [-1.017, 9.502], loss: 0.008899, mean_absolute_error: 1.109460, mean_q: 1.195840\n",
            "  41236/150000: episode: 1072, duration: 0.486s, episode steps: 29, steps per second: 60, episode reward: 2.000, mean reward: 0.069 [0.000, 1.000], mean action: 5.897 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.002], loss: 0.005426, mean_absolute_error: 1.105982, mean_q: 1.197920\n",
            "  41297/150000: episode: 1073, duration: 0.910s, episode steps: 61, steps per second: 67, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 9.016 [1.000, 20.000], mean observation: -0.612 [-1.018, 9.000], loss: 0.008274, mean_absolute_error: 1.107299, mean_q: 1.190496\n",
            "  41311/150000: episode: 1074, duration: 0.315s, episode steps: 14, steps per second: 44, episode reward: 2.000, mean reward: 0.143 [0.000, 1.100], mean action: 6.500 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.012578, mean_absolute_error: 1.109499, mean_q: 1.191182\n",
            "  41407/150000: episode: 1075, duration: 1.361s, episode steps: 96, steps per second: 71, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 16.490 [5.000, 20.000], mean observation: -0.633 [-1.009, 1.000], loss: 0.008438, mean_absolute_error: 1.106154, mean_q: 1.194739\n",
            "  41418/150000: episode: 1076, duration: 0.262s, episode steps: 11, steps per second: 42, episode reward: 2.000, mean reward: 0.182 [0.000, 2.000], mean action: 7.182 [5.000, 14.000], mean observation: -0.634 [-1.040, 1.009], loss: 0.005847, mean_absolute_error: 1.106017, mean_q: 1.195284\n",
            "  41467/150000: episode: 1077, duration: 0.790s, episode steps: 49, steps per second: 62, episode reward: 1.000, mean reward: 0.020 [0.000, 0.900], mean action: 6.796 [1.000, 20.000], mean observation: -0.631 [-1.009, 1.011], loss: 0.010236, mean_absolute_error: 1.109066, mean_q: 1.199270\n",
            "  41497/150000: episode: 1078, duration: 0.503s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006982, mean_absolute_error: 1.107595, mean_q: 1.193678\n",
            "  41527/150000: episode: 1079, duration: 0.517s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.500 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008436, mean_absolute_error: 1.112173, mean_q: 1.190828\n",
            "  41547/150000: episode: 1080, duration: 0.380s, episode steps: 20, steps per second: 53, episode reward: 2.000, mean reward: 0.100 [0.000, 1.000], mean action: 5.700 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007418, mean_absolute_error: 1.110890, mean_q: 1.206761\n",
            "  41601/150000: episode: 1081, duration: 0.841s, episode steps: 54, steps per second: 64, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 11.704 [2.000, 18.000], mean observation: -0.609 [-1.011, 8.059], loss: 0.014701, mean_absolute_error: 1.110427, mean_q: 1.201328\n",
            "  41722/150000: episode: 1082, duration: 1.675s, episode steps: 121, steps per second: 72, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 8.860 [0.000, 20.000], mean observation: -0.628 [-1.011, 1.000], loss: 0.009409, mean_absolute_error: 1.101173, mean_q: 1.195017\n",
            "  41772/150000: episode: 1083, duration: 0.781s, episode steps: 50, steps per second: 64, episode reward: 1.000, mean reward: 0.020 [0.000, 0.900], mean action: 12.360 [0.000, 20.000], mean observation: -0.632 [-1.009, 1.013], loss: 0.011181, mean_absolute_error: 1.101637, mean_q: 1.200540\n",
            "  41794/150000: episode: 1084, duration: 0.401s, episode steps: 22, steps per second: 55, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 9.864 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.063], loss: 0.007388, mean_absolute_error: 1.100478, mean_q: 1.200503\n",
            "  42107/150000: episode: 1085, duration: 4.205s, episode steps: 313, steps per second: 74, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 7.419 [0.000, 20.000], mean observation: -0.638 [-1.009, 1.000], loss: 0.008314, mean_absolute_error: 1.110248, mean_q: 1.199624\n",
            "  42137/150000: episode: 1086, duration: 0.497s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.700 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.011032, mean_absolute_error: 1.099028, mean_q: 1.203788\n",
            "  42167/150000: episode: 1087, duration: 0.521s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.533 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.011102, mean_absolute_error: 1.108167, mean_q: 1.202032\n",
            "  42193/150000: episode: 1088, duration: 0.446s, episode steps: 26, steps per second: 58, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 5.769 [3.000, 19.000], mean observation: -0.635 [-1.011, 1.008], loss: 0.005255, mean_absolute_error: 1.113686, mean_q: 1.199123\n",
            "  42228/150000: episode: 1089, duration: 0.577s, episode steps: 35, steps per second: 61, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 5.943 [2.000, 20.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.012104, mean_absolute_error: 1.113709, mean_q: 1.213032\n",
            "  42252/150000: episode: 1090, duration: 0.441s, episode steps: 24, steps per second: 54, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 5.083 [0.000, 12.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.007629, mean_absolute_error: 1.107822, mean_q: 1.222909\n",
            "  42269/150000: episode: 1091, duration: 0.331s, episode steps: 17, steps per second: 51, episode reward: 2.000, mean reward: 0.118 [0.000, 1.000], mean action: 5.176 [0.000, 12.000], mean observation: -0.635 [-1.011, 1.021], loss: 0.003877, mean_absolute_error: 1.117733, mean_q: 1.222289\n",
            "  42304/150000: episode: 1092, duration: 0.596s, episode steps: 35, steps per second: 59, episode reward: 1.000, mean reward: 0.029 [0.000, 0.900], mean action: 9.943 [0.000, 19.000], mean observation: -0.633 [-1.009, 1.008], loss: 0.005626, mean_absolute_error: 1.124543, mean_q: 1.221905\n",
            "  42370/150000: episode: 1093, duration: 1.027s, episode steps: 66, steps per second: 64, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 5.348 [2.000, 20.000], mean observation: -0.634 [-1.009, 1.008], loss: 0.011403, mean_absolute_error: 1.124228, mean_q: 1.215165\n",
            "  42400/150000: episode: 1094, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.967 [4.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.011765, mean_absolute_error: 1.114748, mean_q: 1.216304\n",
            "  42430/150000: episode: 1095, duration: 0.505s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.900 [2.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.009679, mean_absolute_error: 1.123611, mean_q: 1.231247\n",
            "  42452/150000: episode: 1096, duration: 0.448s, episode steps: 22, steps per second: 49, episode reward: 0.900, mean reward: 0.041 [0.000, 0.900], mean action: 8.636 [2.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.006511, mean_absolute_error: 1.124550, mean_q: 1.209056\n",
            "  42482/150000: episode: 1097, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 15.800 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008968, mean_absolute_error: 1.125040, mean_q: 1.216778\n",
            "  42500/150000: episode: 1098, duration: 0.365s, episode steps: 18, steps per second: 49, episode reward: 2.000, mean reward: 0.111 [0.000, 1.000], mean action: 5.833 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007547, mean_absolute_error: 1.123646, mean_q: 1.218633\n",
            "  42525/150000: episode: 1099, duration: 0.436s, episode steps: 25, steps per second: 57, episode reward: 2.000, mean reward: 0.080 [0.000, 1.000], mean action: 5.760 [5.000, 16.000], mean observation: -0.635 [-1.011, 1.006], loss: 0.008560, mean_absolute_error: 1.125099, mean_q: 1.226438\n",
            "  42570/150000: episode: 1100, duration: 0.710s, episode steps: 45, steps per second: 63, episode reward: 2.000, mean reward: 0.044 [0.000, 1.000], mean action: 9.511 [0.000, 16.000], mean observation: -0.634 [-1.011, 1.002], loss: 0.010131, mean_absolute_error: 1.124891, mean_q: 1.218587\n",
            "  42645/150000: episode: 1101, duration: 1.076s, episode steps: 75, steps per second: 70, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 6.960 [0.000, 20.000], mean observation: -0.637 [-1.011, 1.116], loss: 0.006728, mean_absolute_error: 1.128501, mean_q: 1.222417\n",
            "  42713/150000: episode: 1102, duration: 0.998s, episode steps: 68, steps per second: 68, episode reward: 2.000, mean reward: 0.029 [0.000, 1.100], mean action: 6.691 [1.000, 18.000], mean observation: -0.633 [-1.011, 1.009], loss: 0.010764, mean_absolute_error: 1.124343, mean_q: 1.206289\n",
            "  42792/150000: episode: 1103, duration: 1.167s, episode steps: 79, steps per second: 68, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 13.962 [2.000, 20.000], mean observation: -0.618 [-1.017, 7.993], loss: 0.006498, mean_absolute_error: 1.120471, mean_q: 1.204748\n",
            "  43014/150000: episode: 1104, duration: 3.005s, episode steps: 222, steps per second: 74, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 7.140 [0.000, 20.000], mean observation: -0.619 [-1.084, 9.607], loss: 0.007384, mean_absolute_error: 1.104234, mean_q: 1.187812\n",
            "  43053/150000: episode: 1105, duration: 0.633s, episode steps: 39, steps per second: 62, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 11.487 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.004449, mean_absolute_error: 1.095886, mean_q: 1.182015\n",
            "  43149/150000: episode: 1106, duration: 1.357s, episode steps: 96, steps per second: 71, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 5.542 [1.000, 19.000], mean observation: -0.633 [-1.009, 1.119], loss: 0.008251, mean_absolute_error: 1.100616, mean_q: 1.191938\n",
            "  43209/150000: episode: 1107, duration: 0.908s, episode steps: 60, steps per second: 66, episode reward: 1.000, mean reward: 0.017 [0.000, 0.900], mean action: 5.300 [0.000, 20.000], mean observation: -0.612 [-1.042, 8.358], loss: 0.009425, mean_absolute_error: 1.106237, mean_q: 1.202408\n",
            "  43284/150000: episode: 1108, duration: 1.101s, episode steps: 75, steps per second: 68, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 5.507 [0.000, 20.000], mean observation: -0.613 [-1.077, 9.774], loss: 0.007460, mean_absolute_error: 1.110411, mean_q: 1.195625\n",
            "  43323/150000: episode: 1109, duration: 0.611s, episode steps: 39, steps per second: 64, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 9.359 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.006991, mean_absolute_error: 1.111849, mean_q: 1.189101\n",
            "  43365/150000: episode: 1110, duration: 0.654s, episode steps: 42, steps per second: 64, episode reward: 2.000, mean reward: 0.048 [0.000, 1.000], mean action: 5.524 [3.000, 18.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.007929, mean_absolute_error: 1.111147, mean_q: 1.206088\n",
            "  43395/150000: episode: 1111, duration: 0.501s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010171, mean_absolute_error: 1.108314, mean_q: 1.193161\n",
            "  43432/150000: episode: 1112, duration: 0.615s, episode steps: 37, steps per second: 60, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 4.784 [3.000, 5.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.011546, mean_absolute_error: 1.108916, mean_q: 1.215101\n",
            "  43456/150000: episode: 1113, duration: 0.448s, episode steps: 24, steps per second: 54, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 6.583 [3.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007163, mean_absolute_error: 1.110087, mean_q: 1.216970\n",
            "  43493/150000: episode: 1114, duration: 0.619s, episode steps: 37, steps per second: 60, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.270 [3.000, 17.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.011631, mean_absolute_error: 1.120174, mean_q: 1.221206\n",
            "  43523/150000: episode: 1115, duration: 0.550s, episode steps: 30, steps per second: 55, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.700 [5.000, 9.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.005564, mean_absolute_error: 1.107442, mean_q: 1.206897\n",
            "  43644/150000: episode: 1116, duration: 1.791s, episode steps: 121, steps per second: 68, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 9.198 [0.000, 20.000], mean observation: -0.636 [-1.011, 1.616], loss: 0.011076, mean_absolute_error: 1.116209, mean_q: 1.205125\n",
            "  43681/150000: episode: 1117, duration: 0.644s, episode steps: 37, steps per second: 57, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 8.703 [5.000, 20.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.012139, mean_absolute_error: 1.107923, mean_q: 1.200575\n",
            "  43808/150000: episode: 1118, duration: 1.915s, episode steps: 127, steps per second: 66, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 6.661 [0.000, 20.000], mean observation: -0.633 [-1.009, 1.234], loss: 0.007919, mean_absolute_error: 1.115729, mean_q: 1.202301\n",
            "  43838/150000: episode: 1119, duration: 0.527s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.100 [2.000, 20.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.005610, mean_absolute_error: 1.110682, mean_q: 1.194759\n",
            "  43866/150000: episode: 1120, duration: 0.523s, episode steps: 28, steps per second: 54, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 6.036 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.009560, mean_absolute_error: 1.113519, mean_q: 1.198779\n",
            "  43900/150000: episode: 1121, duration: 0.574s, episode steps: 34, steps per second: 59, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 6.853 [2.000, 17.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.010501, mean_absolute_error: 1.116261, mean_q: 1.217025\n",
            "  43976/150000: episode: 1122, duration: 1.197s, episode steps: 76, steps per second: 63, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 5.592 [5.000, 18.000], mean observation: -0.613 [-1.009, 8.353], loss: 0.007300, mean_absolute_error: 1.114926, mean_q: 1.204312\n",
            "  44049/150000: episode: 1123, duration: 1.117s, episode steps: 73, steps per second: 65, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 5.041 [1.000, 20.000], mean observation: -0.611 [-1.096, 9.660], loss: 0.008371, mean_absolute_error: 1.117677, mean_q: 1.203310\n",
            "  44080/150000: episode: 1124, duration: 0.538s, episode steps: 31, steps per second: 58, episode reward: 2.000, mean reward: 0.065 [0.000, 1.000], mean action: 6.484 [0.000, 19.000], mean observation: -0.634 [-1.011, 1.004], loss: 0.008145, mean_absolute_error: 1.114394, mean_q: 1.226465\n",
            "  44116/150000: episode: 1125, duration: 0.588s, episode steps: 36, steps per second: 61, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 6.111 [4.000, 20.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.007523, mean_absolute_error: 1.119181, mean_q: 1.215830\n",
            "  44156/150000: episode: 1126, duration: 0.680s, episode steps: 40, steps per second: 59, episode reward: 1.000, mean reward: 0.025 [0.000, 0.900], mean action: 6.900 [5.000, 20.000], mean observation: -0.631 [-1.009, 1.009], loss: 0.009833, mean_absolute_error: 1.121582, mean_q: 1.215865\n",
            "  44207/150000: episode: 1127, duration: 0.836s, episode steps: 51, steps per second: 61, episode reward: 1.000, mean reward: 0.020 [0.000, 0.900], mean action: 5.588 [2.000, 12.000], mean observation: -0.631 [-1.009, 1.013], loss: 0.009163, mean_absolute_error: 1.122281, mean_q: 1.208917\n",
            "  44237/150000: episode: 1128, duration: 0.512s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.014508, mean_absolute_error: 1.108362, mean_q: 1.209745\n",
            "  44335/150000: episode: 1129, duration: 1.381s, episode steps: 98, steps per second: 71, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 5.653 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.393], loss: 0.009955, mean_absolute_error: 1.123928, mean_q: 1.218607\n",
            "  44454/150000: episode: 1130, duration: 1.683s, episode steps: 119, steps per second: 71, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 5.824 [2.000, 17.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.007726, mean_absolute_error: 1.134577, mean_q: 1.222504\n",
            "  44484/150000: episode: 1131, duration: 0.501s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.067 [4.000, 19.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.012322, mean_absolute_error: 1.133734, mean_q: 1.221327\n",
            "  44523/150000: episode: 1132, duration: 0.622s, episode steps: 39, steps per second: 63, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 5.667 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.008222, mean_absolute_error: 1.132389, mean_q: 1.220153\n",
            "  44609/150000: episode: 1133, duration: 1.226s, episode steps: 86, steps per second: 70, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 6.930 [0.000, 17.000], mean observation: -0.636 [-1.011, 1.023], loss: 0.008469, mean_absolute_error: 1.128842, mean_q: 1.215763\n",
            "  44741/150000: episode: 1134, duration: 1.815s, episode steps: 132, steps per second: 73, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 10.659 [1.000, 19.000], mean observation: -0.629 [-1.011, 1.000], loss: 0.011064, mean_absolute_error: 1.122537, mean_q: 1.213685\n",
            "  44754/150000: episode: 1135, duration: 0.261s, episode steps: 13, steps per second: 50, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 10.385 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.025], loss: 0.008295, mean_absolute_error: 1.117920, mean_q: 1.205306\n",
            "  44784/150000: episode: 1136, duration: 0.498s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006522, mean_absolute_error: 1.118603, mean_q: 1.206436\n",
            "  44859/150000: episode: 1137, duration: 1.138s, episode steps: 75, steps per second: 66, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 4.640 [1.000, 20.000], mean observation: -0.631 [-1.011, 1.015], loss: 0.009665, mean_absolute_error: 1.122629, mean_q: 1.215160\n",
            "  44889/150000: episode: 1138, duration: 0.666s, episode steps: 30, steps per second: 45, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.300 [1.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006535, mean_absolute_error: 1.117962, mean_q: 1.225412\n",
            "  44915/150000: episode: 1139, duration: 0.522s, episode steps: 26, steps per second: 50, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 7.269 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.011414, mean_absolute_error: 1.126879, mean_q: 1.211067\n",
            "  45011/150000: episode: 1140, duration: 1.365s, episode steps: 96, steps per second: 70, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 6.635 [4.000, 19.000], mean observation: -0.633 [-1.009, 1.011], loss: 0.008650, mean_absolute_error: 1.130401, mean_q: 1.217753\n",
            "  45027/150000: episode: 1141, duration: 0.332s, episode steps: 16, steps per second: 48, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 6.062 [5.000, 15.000], mean observation: -0.635 [-1.040, 1.000], loss: 0.012093, mean_absolute_error: 1.133108, mean_q: 1.223388\n",
            "  45193/150000: episode: 1142, duration: 2.312s, episode steps: 166, steps per second: 72, episode reward: 1.000, mean reward: 0.006 [0.000, 0.900], mean action: 7.771 [0.000, 20.000], mean observation: -0.635 [-1.011, 1.201], loss: 0.009020, mean_absolute_error: 1.122340, mean_q: 1.209697\n",
            "  45226/150000: episode: 1143, duration: 0.537s, episode steps: 33, steps per second: 61, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.758 [5.000, 18.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.010864, mean_absolute_error: 1.119138, mean_q: 1.216041\n",
            "  45256/150000: episode: 1144, duration: 0.516s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.300 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008069, mean_absolute_error: 1.123873, mean_q: 1.216837\n",
            "  45293/150000: episode: 1145, duration: 0.597s, episode steps: 37, steps per second: 62, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.270 [3.000, 17.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.007011, mean_absolute_error: 1.127526, mean_q: 1.219936\n",
            "  45388/150000: episode: 1146, duration: 1.356s, episode steps: 95, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.789 [1.000, 19.000], mean observation: -0.634 [-1.009, 1.000], loss: 0.006987, mean_absolute_error: 1.126034, mean_q: 1.214397\n",
            "  45413/150000: episode: 1147, duration: 0.445s, episode steps: 25, steps per second: 56, episode reward: 2.000, mean reward: 0.080 [0.000, 1.000], mean action: 5.760 [4.000, 17.000], mean observation: -0.634 [-1.011, 1.005], loss: 0.007359, mean_absolute_error: 1.122974, mean_q: 1.208295\n",
            "  45443/150000: episode: 1148, duration: 0.494s, episode steps: 30, steps per second: 61, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.567 [1.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008611, mean_absolute_error: 1.122861, mean_q: 1.205783\n",
            "  45473/150000: episode: 1149, duration: 0.518s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 14.833 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004947, mean_absolute_error: 1.120278, mean_q: 1.204071\n",
            "  45540/150000: episode: 1150, duration: 1.039s, episode steps: 67, steps per second: 64, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 11.433 [1.000, 20.000], mean observation: -0.621 [-1.011, 7.662], loss: 0.010147, mean_absolute_error: 1.118360, mean_q: 1.203864\n",
            "  45618/150000: episode: 1151, duration: 1.117s, episode steps: 78, steps per second: 70, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 9.244 [3.000, 20.000], mean observation: -0.637 [-1.009, 1.187], loss: 0.007070, mean_absolute_error: 1.111502, mean_q: 1.205055\n",
            "  45840/150000: episode: 1152, duration: 3.044s, episode steps: 222, steps per second: 73, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 7.847 [0.000, 19.000], mean observation: -0.630 [-1.011, 1.162], loss: 0.008923, mean_absolute_error: 1.105547, mean_q: 1.188429\n",
            "  45870/150000: episode: 1153, duration: 0.503s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 9.767 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006421, mean_absolute_error: 1.097944, mean_q: 1.176122\n",
            "  45978/150000: episode: 1154, duration: 1.526s, episode steps: 108, steps per second: 71, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 9.204 [0.000, 19.000], mean observation: -0.631 [-1.009, 1.766], loss: 0.009497, mean_absolute_error: 1.089254, mean_q: 1.175359\n",
            "  46053/150000: episode: 1155, duration: 1.088s, episode steps: 75, steps per second: 69, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 13.267 [2.000, 20.000], mean observation: -0.613 [-1.085, 9.603], loss: 0.007488, mean_absolute_error: 1.098866, mean_q: 1.189734\n",
            "  46127/150000: episode: 1156, duration: 1.098s, episode steps: 74, steps per second: 67, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 5.378 [2.000, 20.000], mean observation: -0.637 [-1.009, 1.020], loss: 0.006160, mean_absolute_error: 1.101363, mean_q: 1.186264\n",
            "  46140/150000: episode: 1157, duration: 0.271s, episode steps: 13, steps per second: 48, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 5.923 [1.000, 14.000], mean observation: -0.635 [-1.011, 1.020], loss: 0.008880, mean_absolute_error: 1.099050, mean_q: 1.190439\n",
            "  46195/150000: episode: 1158, duration: 0.883s, episode steps: 55, steps per second: 62, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 5.400 [0.000, 16.000], mean observation: -0.611 [-1.011, 8.189], loss: 0.007984, mean_absolute_error: 1.098524, mean_q: 1.201912\n",
            "  46230/150000: episode: 1159, duration: 0.584s, episode steps: 35, steps per second: 60, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 5.286 [5.000, 14.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.012212, mean_absolute_error: 1.097269, mean_q: 1.185610\n",
            "  46260/150000: episode: 1160, duration: 0.509s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 11.067 [0.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005935, mean_absolute_error: 1.102329, mean_q: 1.190110\n",
            "  46297/150000: episode: 1161, duration: 0.603s, episode steps: 37, steps per second: 61, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 4.892 [0.000, 18.000], mean observation: -0.633 [-1.011, 1.007], loss: 0.006597, mean_absolute_error: 1.109630, mean_q: 1.192797\n",
            "  46327/150000: episode: 1162, duration: 0.507s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.667 [3.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006992, mean_absolute_error: 1.107179, mean_q: 1.190684\n",
            "  46361/150000: episode: 1163, duration: 0.590s, episode steps: 34, steps per second: 58, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 5.235 [1.000, 15.000], mean observation: -0.634 [-1.011, 1.006], loss: 0.007395, mean_absolute_error: 1.109765, mean_q: 1.192318\n",
            "  46370/150000: episode: 1164, duration: 0.218s, episode steps: 9, steps per second: 41, episode reward: 2.000, mean reward: 0.222 [0.000, 2.000], mean action: 6.222 [5.000, 12.000], mean observation: -0.632 [-1.011, 1.021], loss: 0.005357, mean_absolute_error: 1.107851, mean_q: 1.187575\n",
            "  46400/150000: episode: 1165, duration: 0.525s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.733 [0.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008867, mean_absolute_error: 1.102499, mean_q: 1.185521\n",
            "  46427/150000: episode: 1166, duration: 0.488s, episode steps: 27, steps per second: 55, episode reward: 2.000, mean reward: 0.074 [0.000, 1.000], mean action: 11.852 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.007754, mean_absolute_error: 1.111236, mean_q: 1.185141\n",
            "  46541/150000: episode: 1167, duration: 1.632s, episode steps: 114, steps per second: 70, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 8.018 [1.000, 20.000], mean observation: -0.631 [-1.009, 1.000], loss: 0.007977, mean_absolute_error: 1.100796, mean_q: 1.183752\n",
            "  46551/150000: episode: 1168, duration: 0.251s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 9.400 [0.000, 12.000], mean observation: -0.632 [-1.011, 1.416], loss: 0.008189, mean_absolute_error: 1.103822, mean_q: 1.174170\n",
            "  46583/150000: episode: 1169, duration: 0.543s, episode steps: 32, steps per second: 59, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 5.719 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010944, mean_absolute_error: 1.091909, mean_q: 1.173833\n",
            "  46615/150000: episode: 1170, duration: 0.554s, episode steps: 32, steps per second: 58, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 8.656 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.007097, mean_absolute_error: 1.087929, mean_q: 1.173596\n",
            "  46637/150000: episode: 1171, duration: 0.420s, episode steps: 22, steps per second: 52, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 12.136 [0.000, 19.000], mean observation: -0.635 [-1.011, 1.010], loss: 0.011370, mean_absolute_error: 1.090742, mean_q: 1.167001\n",
            "  46667/150000: episode: 1172, duration: 0.512s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.800 [2.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010837, mean_absolute_error: 1.081128, mean_q: 1.165080\n",
            "  46740/150000: episode: 1173, duration: 1.083s, episode steps: 73, steps per second: 67, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 3.589 [2.000, 20.000], mean observation: -0.640 [-1.009, 1.000], loss: 0.006982, mean_absolute_error: 1.086015, mean_q: 1.175922\n",
            "  46829/150000: episode: 1174, duration: 1.289s, episode steps: 89, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 4.348 [2.000, 19.000], mean observation: -0.636 [-1.009, 1.283], loss: 0.009930, mean_absolute_error: 1.091165, mean_q: 1.183782\n",
            "  46925/150000: episode: 1175, duration: 1.383s, episode steps: 96, steps per second: 69, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 4.708 [2.000, 19.000], mean observation: -0.635 [-1.011, 1.264], loss: 0.007246, mean_absolute_error: 1.094449, mean_q: 1.178027\n",
            "  46950/150000: episode: 1176, duration: 0.448s, episode steps: 25, steps per second: 56, episode reward: 2.000, mean reward: 0.080 [0.000, 1.000], mean action: 4.640 [3.000, 13.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.007849, mean_absolute_error: 1.085752, mean_q: 1.170601\n",
            "  47001/150000: episode: 1177, duration: 0.831s, episode steps: 51, steps per second: 61, episode reward: 1.000, mean reward: 0.020 [0.000, 0.900], mean action: 8.490 [3.000, 18.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.009047, mean_absolute_error: 1.089477, mean_q: 1.173990\n",
            "  47116/150000: episode: 1178, duration: 1.652s, episode steps: 115, steps per second: 70, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 12.078 [5.000, 19.000], mean observation: -0.630 [-1.011, 1.008], loss: 0.008324, mean_absolute_error: 1.088185, mean_q: 1.171844\n",
            "  47159/150000: episode: 1179, duration: 0.707s, episode steps: 43, steps per second: 61, episode reward: 2.000, mean reward: 0.047 [0.000, 1.000], mean action: 6.000 [5.000, 13.000], mean observation: -0.634 [-1.011, 1.007], loss: 0.008578, mean_absolute_error: 1.081734, mean_q: 1.166259\n",
            "  47175/150000: episode: 1180, duration: 0.332s, episode steps: 16, steps per second: 48, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 6.938 [5.000, 15.000], mean observation: -0.636 [-1.040, 1.000], loss: 0.005332, mean_absolute_error: 1.084871, mean_q: 1.166901\n",
            "  47205/150000: episode: 1181, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.433 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.009044, mean_absolute_error: 1.085243, mean_q: 1.179720\n",
            "  47235/150000: episode: 1182, duration: 0.501s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.900 [0.000, 15.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.009767, mean_absolute_error: 1.083048, mean_q: 1.172354\n",
            "  47267/150000: episode: 1183, duration: 0.551s, episode steps: 32, steps per second: 58, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 6.500 [2.000, 13.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.007834, mean_absolute_error: 1.089325, mean_q: 1.166061\n",
            "  47276/150000: episode: 1184, duration: 0.214s, episode steps: 9, steps per second: 42, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 5.778 [5.000, 12.000], mean observation: -0.634 [-1.009, 1.019], loss: 0.017068, mean_absolute_error: 1.082222, mean_q: 1.157699\n",
            "  47417/150000: episode: 1185, duration: 2.012s, episode steps: 141, steps per second: 70, episode reward: 0.900, mean reward: 0.006 [0.000, 0.900], mean action: 6.277 [0.000, 19.000], mean observation: -0.632 [-1.009, 3.262], loss: 0.009504, mean_absolute_error: 1.084117, mean_q: 1.175333\n",
            "  47481/150000: episode: 1186, duration: 0.978s, episode steps: 64, steps per second: 65, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 5.344 [2.000, 14.000], mean observation: -0.611 [-1.009, 8.366], loss: 0.010648, mean_absolute_error: 1.089567, mean_q: 1.181752\n",
            "  47588/150000: episode: 1187, duration: 1.507s, episode steps: 107, steps per second: 71, episode reward: 2.000, mean reward: 0.019 [0.000, 1.800], mean action: 13.112 [0.000, 20.000], mean observation: -0.635 [-1.011, 1.014], loss: 0.009470, mean_absolute_error: 1.087285, mean_q: 1.182153\n",
            "  47637/150000: episode: 1188, duration: 0.749s, episode steps: 49, steps per second: 65, episode reward: 2.000, mean reward: 0.041 [0.000, 1.000], mean action: 14.551 [1.000, 20.000], mean observation: -0.632 [-1.011, 1.016], loss: 0.009320, mean_absolute_error: 1.093447, mean_q: 1.187073\n",
            "  47667/150000: episode: 1189, duration: 0.526s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 10.333 [1.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006652, mean_absolute_error: 1.087464, mean_q: 1.174678\n",
            "  47767/150000: episode: 1190, duration: 1.422s, episode steps: 100, steps per second: 70, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 8.030 [0.000, 20.000], mean observation: -0.631 [-1.011, 1.406], loss: 0.007610, mean_absolute_error: 1.091750, mean_q: 1.177489\n",
            "  47805/150000: episode: 1191, duration: 0.641s, episode steps: 38, steps per second: 59, episode reward: 2.000, mean reward: 0.053 [0.000, 1.000], mean action: 7.184 [3.000, 19.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.008478, mean_absolute_error: 1.091274, mean_q: 1.170203\n",
            "  47817/150000: episode: 1192, duration: 0.283s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.167 [1.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.007786, mean_absolute_error: 1.088558, mean_q: 1.172042\n",
            "  47844/150000: episode: 1193, duration: 0.514s, episode steps: 27, steps per second: 53, episode reward: 0.900, mean reward: 0.033 [0.000, 0.900], mean action: 2.444 [1.000, 12.000], mean observation: -0.631 [-1.011, 1.962], loss: 0.009738, mean_absolute_error: 1.085606, mean_q: 1.168424\n",
            "  47933/150000: episode: 1194, duration: 1.325s, episode steps: 89, steps per second: 67, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 4.315 [1.000, 20.000], mean observation: -0.633 [-1.011, 1.007], loss: 0.006574, mean_absolute_error: 1.085306, mean_q: 1.168809\n",
            "  47963/150000: episode: 1195, duration: 0.526s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.167 [0.000, 12.000], mean observation: -0.634 [-1.011, 1.002], loss: 0.009604, mean_absolute_error: 1.090221, mean_q: 1.195237\n",
            "  47993/150000: episode: 1196, duration: 0.531s, episode steps: 30, steps per second: 56, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.100 [5.000, 8.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010580, mean_absolute_error: 1.091559, mean_q: 1.188984\n",
            "  48131/150000: episode: 1197, duration: 2.009s, episode steps: 138, steps per second: 69, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 5.435 [0.000, 19.000], mean observation: -0.630 [-1.011, 1.388], loss: 0.008857, mean_absolute_error: 1.098233, mean_q: 1.199318\n",
            "  48198/150000: episode: 1198, duration: 1.022s, episode steps: 67, steps per second: 66, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 5.209 [1.000, 14.000], mean observation: -0.612 [-1.011, 8.366], loss: 0.009842, mean_absolute_error: 1.117114, mean_q: 1.207548\n",
            "  48234/150000: episode: 1199, duration: 0.588s, episode steps: 36, steps per second: 61, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 5.944 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.008395, mean_absolute_error: 1.116663, mean_q: 1.210781\n",
            "  48352/150000: episode: 1200, duration: 1.670s, episode steps: 118, steps per second: 71, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 8.983 [0.000, 19.000], mean observation: -0.633 [-1.009, 1.000], loss: 0.008590, mean_absolute_error: 1.118178, mean_q: 1.203096\n",
            "  48380/150000: episode: 1201, duration: 0.504s, episode steps: 28, steps per second: 56, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 6.036 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.010898, mean_absolute_error: 1.121760, mean_q: 1.210695\n",
            "  48482/150000: episode: 1202, duration: 1.444s, episode steps: 102, steps per second: 71, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 5.353 [3.000, 20.000], mean observation: -0.632 [-1.011, 1.424], loss: 0.009566, mean_absolute_error: 1.116299, mean_q: 1.198833\n",
            "  48524/150000: episode: 1203, duration: 0.695s, episode steps: 42, steps per second: 60, episode reward: 2.000, mean reward: 0.048 [0.000, 1.000], mean action: 5.619 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.011876, mean_absolute_error: 1.112643, mean_q: 1.205297\n",
            "  48554/150000: episode: 1204, duration: 0.498s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.000 [0.000, 18.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.006909, mean_absolute_error: 1.113772, mean_q: 1.212386\n",
            "  48584/150000: episode: 1205, duration: 0.514s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.167 [5.000, 10.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007311, mean_absolute_error: 1.120828, mean_q: 1.207172\n",
            "  48756/150000: episode: 1206, duration: 2.343s, episode steps: 172, steps per second: 73, episode reward: 1.000, mean reward: 0.006 [0.000, 0.900], mean action: 6.366 [0.000, 20.000], mean observation: -0.633 [-1.009, 1.000], loss: 0.007586, mean_absolute_error: 1.121608, mean_q: 1.207171\n",
            "  48777/150000: episode: 1207, duration: 0.389s, episode steps: 21, steps per second: 54, episode reward: 2.000, mean reward: 0.095 [0.000, 1.000], mean action: 5.952 [5.000, 15.000], mean observation: -0.633 [-1.011, 1.883], loss: 0.008493, mean_absolute_error: 1.113546, mean_q: 1.196473\n",
            "  48875/150000: episode: 1208, duration: 1.418s, episode steps: 98, steps per second: 69, episode reward: 0.000, mean reward: 0.000 [-1.000, 0.900], mean action: 8.255 [2.000, 18.000], mean observation: -0.624 [-1.011, 8.829], loss: 0.009371, mean_absolute_error: 1.111732, mean_q: 1.196269\n",
            "  48949/150000: episode: 1209, duration: 1.098s, episode steps: 74, steps per second: 67, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 6.068 [4.000, 20.000], mean observation: -0.618 [-1.011, 7.825], loss: 0.006863, mean_absolute_error: 1.108145, mean_q: 1.193393\n",
            "  49011/150000: episode: 1210, duration: 0.945s, episode steps: 62, steps per second: 66, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 10.097 [3.000, 20.000], mean observation: -0.613 [-1.011, 7.899], loss: 0.009277, mean_absolute_error: 1.116456, mean_q: 1.208222\n",
            "  49131/150000: episode: 1211, duration: 1.666s, episode steps: 120, steps per second: 72, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 13.408 [1.000, 20.000], mean observation: -0.633 [-1.009, 1.250], loss: 0.007953, mean_absolute_error: 1.109285, mean_q: 1.194224\n",
            "  49212/150000: episode: 1212, duration: 1.174s, episode steps: 81, steps per second: 69, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 8.827 [5.000, 18.000], mean observation: -0.637 [-1.009, 1.031], loss: 0.007944, mean_absolute_error: 1.104331, mean_q: 1.186164\n",
            "  49343/150000: episode: 1213, duration: 1.837s, episode steps: 131, steps per second: 71, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 6.115 [0.000, 16.000], mean observation: -0.634 [-1.009, 1.003], loss: 0.008046, mean_absolute_error: 1.095505, mean_q: 1.178861\n",
            "  49409/150000: episode: 1214, duration: 0.991s, episode steps: 66, steps per second: 67, episode reward: 0.900, mean reward: 0.014 [0.000, 0.900], mean action: 14.212 [2.000, 16.000], mean observation: -0.621 [-1.011, 7.668], loss: 0.010032, mean_absolute_error: 1.096746, mean_q: 1.174692\n",
            "  49446/150000: episode: 1215, duration: 0.597s, episode steps: 37, steps per second: 62, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.378 [5.000, 16.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.006129, mean_absolute_error: 1.087260, mean_q: 1.173519\n",
            "  49546/150000: episode: 1216, duration: 1.398s, episode steps: 100, steps per second: 72, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 8.190 [0.000, 19.000], mean observation: -0.631 [-1.011, 1.260], loss: 0.008333, mean_absolute_error: 1.086503, mean_q: 1.169288\n",
            "  49578/150000: episode: 1217, duration: 0.542s, episode steps: 32, steps per second: 59, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 9.344 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.011418, mean_absolute_error: 1.070897, mean_q: 1.154945\n",
            "  49608/150000: episode: 1218, duration: 0.497s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.200 [4.000, 13.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004384, mean_absolute_error: 1.080884, mean_q: 1.164643\n",
            "  49657/150000: episode: 1219, duration: 0.748s, episode steps: 49, steps per second: 65, episode reward: 2.000, mean reward: 0.041 [0.000, 1.000], mean action: 6.878 [2.000, 20.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.006641, mean_absolute_error: 1.079208, mean_q: 1.161561\n",
            "  49687/150000: episode: 1220, duration: 0.503s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.433 [2.000, 15.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.008991, mean_absolute_error: 1.076634, mean_q: 1.164790\n",
            "  49701/150000: episode: 1221, duration: 0.324s, episode steps: 14, steps per second: 43, episode reward: 2.000, mean reward: 0.143 [0.000, 1.000], mean action: 8.714 [5.000, 14.000], mean observation: -0.635 [-1.040, 1.005], loss: 0.009905, mean_absolute_error: 1.075934, mean_q: 1.162119\n",
            "  49731/150000: episode: 1222, duration: 0.500s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.433 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008135, mean_absolute_error: 1.076496, mean_q: 1.159736\n",
            "  49761/150000: episode: 1223, duration: 0.519s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 8.700 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005470, mean_absolute_error: 1.074725, mean_q: 1.152289\n",
            "  49796/150000: episode: 1224, duration: 0.560s, episode steps: 35, steps per second: 63, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 4.829 [2.000, 5.000], mean observation: -0.634 [-1.011, 1.007], loss: 0.009044, mean_absolute_error: 1.074025, mean_q: 1.155249\n",
            "  49906/150000: episode: 1225, duration: 1.570s, episode steps: 110, steps per second: 70, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 10.273 [1.000, 20.000], mean observation: -0.636 [-1.039, 1.010], loss: 0.008864, mean_absolute_error: 1.073012, mean_q: 1.162929\n",
            "  49936/150000: episode: 1226, duration: 0.502s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.267 [4.000, 14.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.009464, mean_absolute_error: 1.074075, mean_q: 1.155932\n",
            "  49966/150000: episode: 1227, duration: 0.512s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.867 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008770, mean_absolute_error: 1.075045, mean_q: 1.160687\n",
            "  50000/150000: episode: 1228, duration: 0.554s, episode steps: 34, steps per second: 61, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 8.206 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.009777, mean_absolute_error: 1.077514, mean_q: 1.160091\n",
            "  50043/150000: episode: 1229, duration: 0.723s, episode steps: 43, steps per second: 59, episode reward: 1.000, mean reward: 0.023 [0.000, 0.900], mean action: 5.186 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007312, mean_absolute_error: 1.083197, mean_q: 1.159812\n",
            "  50126/150000: episode: 1230, duration: 1.227s, episode steps: 83, steps per second: 68, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 8.590 [0.000, 20.000], mean observation: -0.615 [-1.013, 8.301], loss: 0.007560, mean_absolute_error: 1.084062, mean_q: 1.169741\n",
            "  50156/150000: episode: 1231, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 9.800 [0.000, 15.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.009598, mean_absolute_error: 1.084856, mean_q: 1.164178\n",
            "  50199/150000: episode: 1232, duration: 0.669s, episode steps: 43, steps per second: 64, episode reward: 2.000, mean reward: 0.047 [0.000, 1.000], mean action: 5.209 [1.000, 20.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.006232, mean_absolute_error: 1.086461, mean_q: 1.178702\n",
            "  50217/150000: episode: 1233, duration: 0.367s, episode steps: 18, steps per second: 49, episode reward: 2.000, mean reward: 0.111 [0.000, 1.000], mean action: 6.167 [5.000, 19.000], mean observation: -0.635 [-1.017, 1.000], loss: 0.008368, mean_absolute_error: 1.084601, mean_q: 1.178183\n",
            "  50327/150000: episode: 1234, duration: 1.531s, episode steps: 110, steps per second: 72, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 9.300 [1.000, 19.000], mean observation: -0.631 [-1.011, 1.418], loss: 0.008973, mean_absolute_error: 1.087643, mean_q: 1.172645\n",
            "  50357/150000: episode: 1235, duration: 0.512s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.333 [5.000, 15.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010933, mean_absolute_error: 1.084454, mean_q: 1.170019\n",
            "  50462/150000: episode: 1236, duration: 1.493s, episode steps: 105, steps per second: 70, episode reward: 2.000, mean reward: 0.019 [0.000, 1.000], mean action: 8.905 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.007807, mean_absolute_error: 1.080940, mean_q: 1.162569\n",
            "  50526/150000: episode: 1237, duration: 0.945s, episode steps: 64, steps per second: 68, episode reward: 2.000, mean reward: 0.031 [0.000, 1.000], mean action: 9.016 [1.000, 18.000], mean observation: -0.633 [-1.011, 1.005], loss: 0.007675, mean_absolute_error: 1.083364, mean_q: 1.171891\n",
            "  50560/150000: episode: 1238, duration: 0.559s, episode steps: 34, steps per second: 61, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 5.706 [2.000, 20.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.008433, mean_absolute_error: 1.085882, mean_q: 1.175259\n",
            "  50594/150000: episode: 1239, duration: 0.585s, episode steps: 34, steps per second: 58, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 6.206 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.008511, mean_absolute_error: 1.086497, mean_q: 1.165426\n",
            "  50629/150000: episode: 1240, duration: 0.580s, episode steps: 35, steps per second: 60, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 11.600 [3.000, 20.000], mean observation: -0.634 [-1.011, 1.007], loss: 0.008354, mean_absolute_error: 1.088984, mean_q: 1.176418\n",
            "  50738/150000: episode: 1241, duration: 1.540s, episode steps: 109, steps per second: 71, episode reward: 2.000, mean reward: 0.018 [0.000, 1.000], mean action: 12.083 [2.000, 20.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.008481, mean_absolute_error: 1.082611, mean_q: 1.170053\n",
            "  50768/150000: episode: 1242, duration: 0.518s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.433 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008727, mean_absolute_error: 1.077603, mean_q: 1.170966\n",
            "  50833/150000: episode: 1243, duration: 0.997s, episode steps: 65, steps per second: 65, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 5.846 [1.000, 19.000], mean observation: -0.614 [-1.017, 8.381], loss: 0.010857, mean_absolute_error: 1.085973, mean_q: 1.175804\n",
            "  50846/150000: episode: 1244, duration: 0.293s, episode steps: 13, steps per second: 44, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 5.538 [5.000, 12.000], mean observation: -0.636 [-1.043, 1.018], loss: 0.014925, mean_absolute_error: 1.091001, mean_q: 1.180940\n",
            "  50866/150000: episode: 1245, duration: 0.364s, episode steps: 20, steps per second: 55, episode reward: 2.000, mean reward: 0.100 [0.000, 1.000], mean action: 7.500 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.004876, mean_absolute_error: 1.085649, mean_q: 1.170439\n",
            "  50965/150000: episode: 1246, duration: 1.416s, episode steps: 99, steps per second: 70, episode reward: 2.000, mean reward: 0.020 [0.000, 1.100], mean action: 7.424 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.204], loss: 0.008751, mean_absolute_error: 1.085050, mean_q: 1.167660\n",
            "  51065/150000: episode: 1247, duration: 1.418s, episode steps: 100, steps per second: 70, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 6.870 [1.000, 19.000], mean observation: -0.635 [-1.009, 1.000], loss: 0.009008, mean_absolute_error: 1.073940, mean_q: 1.159437\n",
            "  51102/150000: episode: 1248, duration: 0.597s, episode steps: 37, steps per second: 62, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 4.811 [2.000, 5.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.007454, mean_absolute_error: 1.077160, mean_q: 1.166742\n",
            "  51219/150000: episode: 1249, duration: 1.660s, episode steps: 117, steps per second: 70, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 7.009 [1.000, 19.000], mean observation: -0.633 [-1.011, 1.075], loss: 0.007128, mean_absolute_error: 1.086353, mean_q: 1.169978\n",
            "  51250/150000: episode: 1250, duration: 0.526s, episode steps: 31, steps per second: 59, episode reward: 2.000, mean reward: 0.065 [0.000, 1.000], mean action: 5.581 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.009181, mean_absolute_error: 1.077185, mean_q: 1.161351\n",
            "  51356/150000: episode: 1251, duration: 1.549s, episode steps: 106, steps per second: 68, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 6.349 [0.000, 18.000], mean observation: -0.621 [-1.017, 8.381], loss: 0.007488, mean_absolute_error: 1.081652, mean_q: 1.165731\n",
            "  51422/150000: episode: 1252, duration: 0.969s, episode steps: 66, steps per second: 68, episode reward: 2.000, mean reward: 0.030 [0.000, 1.000], mean action: 9.879 [4.000, 19.000], mean observation: -0.631 [-1.011, 1.011], loss: 0.009503, mean_absolute_error: 1.072596, mean_q: 1.157590\n",
            "  51529/150000: episode: 1253, duration: 1.510s, episode steps: 107, steps per second: 71, episode reward: 2.000, mean reward: 0.019 [0.000, 1.000], mean action: 5.692 [1.000, 19.000], mean observation: -0.632 [-1.021, 1.509], loss: 0.007046, mean_absolute_error: 1.067626, mean_q: 1.147264\n",
            "  51541/150000: episode: 1254, duration: 0.277s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.005628, mean_absolute_error: 1.058567, mean_q: 1.133827\n",
            "  51584/150000: episode: 1255, duration: 0.680s, episode steps: 43, steps per second: 63, episode reward: 2.000, mean reward: 0.047 [0.000, 1.000], mean action: 16.349 [3.000, 19.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.004566, mean_absolute_error: 1.063314, mean_q: 1.140895\n",
            "  51730/150000: episode: 1256, duration: 2.025s, episode steps: 146, steps per second: 72, episode reward: 2.000, mean reward: 0.014 [0.000, 1.000], mean action: 11.014 [0.000, 20.000], mean observation: -0.630 [-1.011, 1.101], loss: 0.008948, mean_absolute_error: 1.051377, mean_q: 1.130969\n",
            "  51753/150000: episode: 1257, duration: 0.417s, episode steps: 23, steps per second: 55, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 12.217 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.008420, mean_absolute_error: 1.049684, mean_q: 1.135147\n",
            "  51866/150000: episode: 1258, duration: 1.625s, episode steps: 113, steps per second: 70, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 7.018 [0.000, 20.000], mean observation: -0.631 [-1.009, 1.503], loss: 0.008719, mean_absolute_error: 1.037442, mean_q: 1.113327\n",
            "  51878/150000: episode: 1259, duration: 0.279s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.013766, mean_absolute_error: 1.036281, mean_q: 1.109620\n",
            "  51895/150000: episode: 1260, duration: 0.334s, episode steps: 17, steps per second: 51, episode reward: 2.000, mean reward: 0.118 [0.000, 1.100], mean action: 11.412 [2.000, 12.000], mean observation: -0.633 [-1.011, 1.427], loss: 0.008609, mean_absolute_error: 1.034575, mean_q: 1.118575\n",
            "  51910/150000: episode: 1261, duration: 0.321s, episode steps: 15, steps per second: 47, episode reward: 2.000, mean reward: 0.133 [0.000, 2.000], mean action: 9.667 [0.000, 16.000], mean observation: -0.636 [-1.011, 1.011], loss: 0.008205, mean_absolute_error: 1.038971, mean_q: 1.125023\n",
            "  51920/150000: episode: 1262, duration: 0.258s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 5.800 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.017], loss: 0.006948, mean_absolute_error: 1.044213, mean_q: 1.121234\n",
            "  51951/150000: episode: 1263, duration: 0.545s, episode steps: 31, steps per second: 57, episode reward: 2.000, mean reward: 0.065 [0.000, 1.000], mean action: 5.903 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.015], loss: 0.006793, mean_absolute_error: 1.036117, mean_q: 1.119014\n",
            "  52084/150000: episode: 1264, duration: 1.898s, episode steps: 133, steps per second: 70, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 6.669 [0.000, 20.000], mean observation: -0.631 [-1.011, 1.130], loss: 0.008987, mean_absolute_error: 1.028536, mean_q: 1.107794\n",
            "  52108/150000: episode: 1265, duration: 0.442s, episode steps: 24, steps per second: 54, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 6.333 [2.000, 13.000], mean observation: -0.635 [-1.011, 1.005], loss: 0.009223, mean_absolute_error: 1.026283, mean_q: 1.102322\n",
            "  52508/150000: episode: 1266, duration: 5.343s, episode steps: 400, steps per second: 75, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 4.970 [0.000, 20.000], mean observation: -0.625 [-1.058, 9.490], loss: 0.007999, mean_absolute_error: 1.029589, mean_q: 1.110656\n",
            "  52533/150000: episode: 1267, duration: 0.445s, episode steps: 25, steps per second: 56, episode reward: 2.000, mean reward: 0.080 [0.000, 1.000], mean action: 5.680 [5.000, 15.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.008807, mean_absolute_error: 1.024051, mean_q: 1.101893\n",
            "  52602/150000: episode: 1268, duration: 1.012s, episode steps: 69, steps per second: 68, episode reward: 0.200, mean reward: 0.003 [0.000, 0.200], mean action: 8.710 [2.000, 17.000], mean observation: -0.638 [-1.011, 1.000], loss: 0.006069, mean_absolute_error: 1.024726, mean_q: 1.098853\n",
            "  52668/150000: episode: 1269, duration: 0.967s, episode steps: 66, steps per second: 68, episode reward: 2.000, mean reward: 0.030 [0.000, 1.000], mean action: 8.212 [1.000, 17.000], mean observation: -0.632 [-1.011, 1.012], loss: 0.007118, mean_absolute_error: 1.021762, mean_q: 1.097615\n",
            "  52705/150000: episode: 1270, duration: 0.624s, episode steps: 37, steps per second: 59, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 6.351 [2.000, 20.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.005429, mean_absolute_error: 1.018698, mean_q: 1.090940\n",
            "  52718/150000: episode: 1271, duration: 0.293s, episode steps: 13, steps per second: 44, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 5.385 [0.000, 12.000], mean observation: -0.636 [-1.036, 1.018], loss: 0.004373, mean_absolute_error: 1.019459, mean_q: 1.091104\n",
            "  52757/150000: episode: 1272, duration: 0.659s, episode steps: 39, steps per second: 59, episode reward: 1.000, mean reward: 0.026 [0.000, 0.900], mean action: 7.051 [3.000, 17.000], mean observation: -0.633 [-1.011, 1.009], loss: 0.006883, mean_absolute_error: 1.018322, mean_q: 1.093406\n",
            "  52787/150000: episode: 1273, duration: 0.501s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.267 [1.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008308, mean_absolute_error: 1.020617, mean_q: 1.100715\n",
            "  52875/150000: episode: 1274, duration: 1.311s, episode steps: 88, steps per second: 67, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 4.182 [0.000, 18.000], mean observation: -0.631 [-1.009, 1.013], loss: 0.006459, mean_absolute_error: 1.022452, mean_q: 1.103429\n",
            "  52887/150000: episode: 1275, duration: 0.290s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.003], loss: 0.012404, mean_absolute_error: 1.022095, mean_q: 1.103933\n",
            "  52902/150000: episode: 1276, duration: 0.347s, episode steps: 15, steps per second: 43, episode reward: 0.900, mean reward: 0.060 [0.000, 0.900], mean action: 11.600 [6.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.011290, mean_absolute_error: 1.009211, mean_q: 1.087109\n",
            "  52912/150000: episode: 1277, duration: 0.250s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 6.000 [1.000, 12.000], mean observation: -0.635 [-1.011, 1.012], loss: 0.005053, mean_absolute_error: 0.998237, mean_q: 1.074391\n",
            "  52933/150000: episode: 1278, duration: 0.430s, episode steps: 21, steps per second: 49, episode reward: 0.900, mean reward: 0.043 [0.000, 0.900], mean action: 11.667 [4.000, 15.000], mean observation: -0.629 [-1.011, 3.124], loss: 0.010559, mean_absolute_error: 1.006144, mean_q: 1.079414\n",
            "  52963/150000: episode: 1279, duration: 0.490s, episode steps: 30, steps per second: 61, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.700 [4.000, 17.000], mean observation: -0.633 [-1.009, 1.015], loss: 0.005079, mean_absolute_error: 1.008255, mean_q: 1.090046\n",
            "  52993/150000: episode: 1280, duration: 0.523s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008921, mean_absolute_error: 1.007698, mean_q: 1.086369\n",
            "  53023/150000: episode: 1281, duration: 0.507s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.100 [2.000, 11.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.004848, mean_absolute_error: 1.007659, mean_q: 1.083036\n",
            "  53158/150000: episode: 1282, duration: 1.895s, episode steps: 135, steps per second: 71, episode reward: 2.000, mean reward: 0.015 [0.000, 1.000], mean action: 5.719 [0.000, 17.000], mean observation: -0.632 [-1.011, 1.044], loss: 0.007345, mean_absolute_error: 1.011463, mean_q: 1.087337\n",
            "  53170/150000: episode: 1283, duration: 0.283s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 7.500 [4.000, 15.000], mean observation: -0.634 [-1.011, 1.113], loss: 0.009288, mean_absolute_error: 1.006917, mean_q: 1.084818\n",
            "  53192/150000: episode: 1284, duration: 0.423s, episode steps: 22, steps per second: 52, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 7.500 [0.000, 12.000], mean observation: -0.635 [-1.011, 1.015], loss: 0.005742, mean_absolute_error: 1.007073, mean_q: 1.088665\n",
            "  53334/150000: episode: 1285, duration: 2.014s, episode steps: 142, steps per second: 71, episode reward: 2.000, mean reward: 0.014 [0.000, 1.000], mean action: 6.563 [0.000, 20.000], mean observation: -0.633 [-1.011, 2.242], loss: 0.008273, mean_absolute_error: 1.010368, mean_q: 1.091137\n",
            "  53430/150000: episode: 1286, duration: 1.374s, episode steps: 96, steps per second: 70, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 8.927 [3.000, 20.000], mean observation: -0.633 [-1.009, 1.075], loss: 0.007909, mean_absolute_error: 1.005701, mean_q: 1.086478\n",
            "  53443/150000: episode: 1287, duration: 0.288s, episode steps: 13, steps per second: 45, episode reward: 2.000, mean reward: 0.154 [0.000, 2.000], mean action: 8.154 [4.000, 16.000], mean observation: -0.633 [-1.038, 1.359], loss: 0.004705, mean_absolute_error: 1.006132, mean_q: 1.090043\n",
            "  53453/150000: episode: 1288, duration: 0.242s, episode steps: 10, steps per second: 41, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.800 [5.000, 17.000], mean observation: -0.633 [-1.011, 1.191], loss: 0.007782, mean_absolute_error: 1.005674, mean_q: 1.075805\n",
            "  53463/150000: episode: 1289, duration: 0.264s, episode steps: 10, steps per second: 38, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.600 [5.000, 16.000], mean observation: -0.632 [-1.011, 1.439], loss: 0.011352, mean_absolute_error: 1.003220, mean_q: 1.074067\n",
            "  53472/150000: episode: 1290, duration: 0.214s, episode steps: 9, steps per second: 42, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 10.333 [5.000, 18.000], mean observation: -0.633 [-1.011, 1.022], loss: 0.006729, mean_absolute_error: 1.004938, mean_q: 1.079301\n",
            "  53564/150000: episode: 1291, duration: 1.287s, episode steps: 92, steps per second: 71, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 15.315 [1.000, 20.000], mean observation: -0.632 [-1.011, 1.241], loss: 0.008264, mean_absolute_error: 1.000368, mean_q: 1.075477\n",
            "  53652/150000: episode: 1292, duration: 1.263s, episode steps: 88, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 9.602 [2.000, 20.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.006482, mean_absolute_error: 0.992252, mean_q: 1.064589\n",
            "  53682/150000: episode: 1293, duration: 0.518s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.000 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006997, mean_absolute_error: 0.987508, mean_q: 1.063424\n",
            "  53894/150000: episode: 1294, duration: 2.910s, episode steps: 212, steps per second: 73, episode reward: 0.200, mean reward: 0.001 [0.000, 0.200], mean action: 10.910 [2.000, 20.000], mean observation: -0.637 [-1.009, 1.000], loss: 0.007776, mean_absolute_error: 0.992370, mean_q: 1.074959\n",
            "  53966/150000: episode: 1295, duration: 1.038s, episode steps: 72, steps per second: 69, episode reward: 2.000, mean reward: 0.028 [0.000, 1.000], mean action: 11.417 [5.000, 14.000], mean observation: -0.632 [-1.011, 1.010], loss: 0.007575, mean_absolute_error: 0.999611, mean_q: 1.084594\n",
            "  53986/150000: episode: 1296, duration: 0.384s, episode steps: 20, steps per second: 52, episode reward: 2.000, mean reward: 0.100 [0.000, 1.000], mean action: 5.350 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.005298, mean_absolute_error: 0.994110, mean_q: 1.084727\n",
            "  54033/150000: episode: 1297, duration: 0.754s, episode steps: 47, steps per second: 62, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 11.894 [5.000, 20.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.005784, mean_absolute_error: 0.997721, mean_q: 1.077257\n",
            "  54063/150000: episode: 1298, duration: 0.502s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010754, mean_absolute_error: 1.000120, mean_q: 1.084031\n",
            "  54099/150000: episode: 1299, duration: 0.587s, episode steps: 36, steps per second: 61, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 5.639 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.007131, mean_absolute_error: 0.995420, mean_q: 1.071483\n",
            "  54132/150000: episode: 1300, duration: 0.542s, episode steps: 33, steps per second: 61, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.879 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.004742, mean_absolute_error: 1.000360, mean_q: 1.076637\n",
            "  54205/150000: episode: 1301, duration: 1.077s, episode steps: 73, steps per second: 68, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 7.137 [1.000, 15.000], mean observation: -0.612 [-1.033, 8.448], loss: 0.008859, mean_absolute_error: 0.993362, mean_q: 1.064979\n",
            "  54235/150000: episode: 1302, duration: 0.507s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 9.567 [2.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004058, mean_absolute_error: 0.990683, mean_q: 1.058659\n",
            "  54345/150000: episode: 1303, duration: 1.523s, episode steps: 110, steps per second: 72, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 6.755 [3.000, 18.000], mean observation: -0.627 [-1.011, 1.019], loss: 0.007153, mean_absolute_error: 0.985346, mean_q: 1.058996\n",
            "  54481/150000: episode: 1304, duration: 1.875s, episode steps: 136, steps per second: 73, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 12.199 [0.000, 20.000], mean observation: -0.632 [-1.009, 1.000], loss: 0.007016, mean_absolute_error: 0.988630, mean_q: 1.067637\n",
            "  54583/150000: episode: 1305, duration: 1.449s, episode steps: 102, steps per second: 70, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 12.176 [1.000, 18.000], mean observation: -0.635 [-1.011, 1.380], loss: 0.004351, mean_absolute_error: 0.982196, mean_q: 1.053200\n",
            "  54686/150000: episode: 1306, duration: 1.474s, episode steps: 103, steps per second: 70, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 8.029 [0.000, 20.000], mean observation: -0.629 [-1.009, 1.008], loss: 0.006840, mean_absolute_error: 0.978245, mean_q: 1.053600\n",
            "  54698/150000: episode: 1307, duration: 0.274s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.333 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.285], loss: 0.012952, mean_absolute_error: 0.970909, mean_q: 1.057046\n",
            "  54713/150000: episode: 1308, duration: 0.331s, episode steps: 15, steps per second: 45, episode reward: 2.000, mean reward: 0.133 [0.000, 1.100], mean action: 10.733 [2.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.002972, mean_absolute_error: 0.969570, mean_q: 1.052295\n",
            "  54727/150000: episode: 1309, duration: 0.320s, episode steps: 14, steps per second: 44, episode reward: 2.000, mean reward: 0.143 [0.000, 2.000], mean action: 12.214 [12.000, 15.000], mean observation: -0.634 [-1.009, 1.019], loss: 0.004440, mean_absolute_error: 0.974595, mean_q: 1.059073\n",
            "  54796/150000: episode: 1310, duration: 1.014s, episode steps: 69, steps per second: 68, episode reward: 0.200, mean reward: 0.003 [0.000, 0.200], mean action: 9.493 [2.000, 18.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.011283, mean_absolute_error: 0.969213, mean_q: 1.043200\n",
            "  54921/150000: episode: 1311, duration: 1.744s, episode steps: 125, steps per second: 72, episode reward: 0.900, mean reward: 0.007 [0.000, 0.900], mean action: 10.744 [0.000, 19.000], mean observation: -0.636 [-1.011, 2.075], loss: 0.007187, mean_absolute_error: 0.962752, mean_q: 1.041533\n",
            "  54958/150000: episode: 1312, duration: 0.616s, episode steps: 37, steps per second: 60, episode reward: 1.000, mean reward: 0.027 [0.000, 0.900], mean action: 7.108 [3.000, 20.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.005586, mean_absolute_error: 0.962207, mean_q: 1.037721\n",
            "  54970/150000: episode: 1313, duration: 0.288s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.004], loss: 0.006978, mean_absolute_error: 0.965537, mean_q: 1.041220\n",
            "  54980/150000: episode: 1314, duration: 0.247s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 5.200 [3.000, 14.000], mean observation: -0.635 [-1.011, 1.013], loss: 0.001325, mean_absolute_error: 0.962262, mean_q: 1.041394\n",
            "  55002/150000: episode: 1315, duration: 0.399s, episode steps: 22, steps per second: 55, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 9.227 [0.000, 16.000], mean observation: -0.634 [-1.011, 1.494], loss: 0.004634, mean_absolute_error: 0.961842, mean_q: 1.026260\n",
            "  55013/150000: episode: 1316, duration: 0.256s, episode steps: 11, steps per second: 43, episode reward: 2.000, mean reward: 0.182 [0.000, 2.000], mean action: 7.273 [5.000, 16.000], mean observation: -0.635 [-1.039, 1.000], loss: 0.005639, mean_absolute_error: 0.957265, mean_q: 1.030416\n",
            "  55215/150000: episode: 1317, duration: 2.764s, episode steps: 202, steps per second: 73, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 11.262 [0.000, 19.000], mean observation: -0.628 [-1.011, 3.331], loss: 0.007420, mean_absolute_error: 0.952881, mean_q: 1.024633\n",
            "  55225/150000: episode: 1318, duration: 0.240s, episode steps: 10, steps per second: 42, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 8.500 [4.000, 15.000], mean observation: -0.633 [-1.011, 1.079], loss: 0.008814, mean_absolute_error: 0.949823, mean_q: 1.019633\n",
            "  55237/150000: episode: 1319, duration: 0.292s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 6.750 [5.000, 12.000], mean observation: -0.636 [-1.041, 1.012], loss: 0.009939, mean_absolute_error: 0.950882, mean_q: 1.021594\n",
            "  55250/150000: episode: 1320, duration: 0.292s, episode steps: 13, steps per second: 45, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 6.538 [5.000, 18.000], mean observation: -0.635 [-1.039, 1.000], loss: 0.005552, mean_absolute_error: 0.956184, mean_q: 1.033409\n",
            "  55348/150000: episode: 1321, duration: 1.386s, episode steps: 98, steps per second: 71, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 11.724 [1.000, 19.000], mean observation: -0.634 [-1.009, 1.000], loss: 0.009062, mean_absolute_error: 0.947761, mean_q: 1.025750\n",
            "  55385/150000: episode: 1322, duration: 0.596s, episode steps: 37, steps per second: 62, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 4.865 [2.000, 5.000], mean observation: -0.634 [-1.011, 1.007], loss: 0.008148, mean_absolute_error: 0.948157, mean_q: 1.030209\n",
            "  55415/150000: episode: 1323, duration: 0.520s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 13.800 [4.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007188, mean_absolute_error: 0.947710, mean_q: 1.017305\n",
            "  55445/150000: episode: 1324, duration: 0.514s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.933 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006928, mean_absolute_error: 0.951222, mean_q: 1.026338\n",
            "  55465/150000: episode: 1325, duration: 0.384s, episode steps: 20, steps per second: 52, episode reward: 2.000, mean reward: 0.100 [0.000, 1.000], mean action: 7.700 [5.000, 20.000], mean observation: -0.635 [-1.011, 1.005], loss: 0.008912, mean_absolute_error: 0.957045, mean_q: 1.033867\n",
            "  55539/150000: episode: 1326, duration: 1.072s, episode steps: 74, steps per second: 69, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 8.743 [2.000, 18.000], mean observation: -0.635 [-1.009, 1.000], loss: 0.004181, mean_absolute_error: 0.959479, mean_q: 1.033785\n",
            "  55551/150000: episode: 1327, duration: 0.291s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.083 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.195], loss: 0.009138, mean_absolute_error: 0.961382, mean_q: 1.032615\n",
            "  55585/150000: episode: 1328, duration: 0.549s, episode steps: 34, steps per second: 62, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 6.235 [2.000, 20.000], mean observation: -0.634 [-1.011, 1.003], loss: 0.007892, mean_absolute_error: 0.963093, mean_q: 1.032743\n",
            "  55615/150000: episode: 1329, duration: 0.523s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.733 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.003783, mean_absolute_error: 0.960014, mean_q: 1.030486\n",
            "  55631/150000: episode: 1330, duration: 0.328s, episode steps: 16, steps per second: 49, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 8.750 [2.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.006894, mean_absolute_error: 0.956851, mean_q: 1.023969\n",
            "  55643/150000: episode: 1331, duration: 0.272s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.128], loss: 0.002793, mean_absolute_error: 0.963484, mean_q: 1.030556\n",
            "  55655/150000: episode: 1332, duration: 0.279s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.917 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.003751, mean_absolute_error: 0.955773, mean_q: 1.034688\n",
            "  55667/150000: episode: 1333, duration: 0.285s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.917 [5.000, 18.000], mean observation: -0.635 [-1.011, 1.005], loss: 0.005079, mean_absolute_error: 0.952941, mean_q: 1.016413\n",
            "  55679/150000: episode: 1334, duration: 0.304s, episode steps: 12, steps per second: 40, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 11.667 [6.000, 15.000], mean observation: -0.628 [-1.011, 2.324], loss: 0.007995, mean_absolute_error: 0.955949, mean_q: 1.022261\n",
            "  55796/150000: episode: 1335, duration: 1.670s, episode steps: 117, steps per second: 70, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 4.923 [0.000, 12.000], mean observation: -0.633 [-1.011, 2.264], loss: 0.007031, mean_absolute_error: 0.955028, mean_q: 1.029918\n",
            "  55829/150000: episode: 1336, duration: 0.541s, episode steps: 33, steps per second: 61, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 1.727 [0.000, 5.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.006346, mean_absolute_error: 0.958952, mean_q: 1.027655\n",
            "  55922/150000: episode: 1337, duration: 1.314s, episode steps: 93, steps per second: 71, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 11.011 [0.000, 20.000], mean observation: -0.618 [-1.009, 7.920], loss: 0.006761, mean_absolute_error: 0.955881, mean_q: 1.032195\n",
            "  55950/150000: episode: 1338, duration: 0.503s, episode steps: 28, steps per second: 56, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 18.357 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008744, mean_absolute_error: 0.962172, mean_q: 1.043349\n",
            "  55960/150000: episode: 1339, duration: 0.261s, episode steps: 10, steps per second: 38, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 9.000 [5.000, 18.000], mean observation: -0.636 [-1.011, 1.016], loss: 0.010460, mean_absolute_error: 0.962322, mean_q: 1.037293\n",
            "  55974/150000: episode: 1340, duration: 0.314s, episode steps: 14, steps per second: 45, episode reward: 2.000, mean reward: 0.143 [0.000, 1.000], mean action: 5.500 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007511, mean_absolute_error: 0.954894, mean_q: 1.037154\n",
            "  56007/150000: episode: 1341, duration: 0.538s, episode steps: 33, steps per second: 61, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.636 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.003407, mean_absolute_error: 0.961110, mean_q: 1.040196\n",
            "  56019/150000: episode: 1342, duration: 0.295s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.003726, mean_absolute_error: 0.968778, mean_q: 1.038922\n",
            "  56032/150000: episode: 1343, duration: 0.299s, episode steps: 13, steps per second: 43, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 12.000 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.415], loss: 0.009874, mean_absolute_error: 0.969940, mean_q: 1.058996\n",
            "  56044/150000: episode: 1344, duration: 0.271s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.001], loss: 0.006301, mean_absolute_error: 0.964642, mean_q: 1.046432\n",
            "  56057/150000: episode: 1345, duration: 0.285s, episode steps: 13, steps per second: 46, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.538 [5.000, 16.000], mean observation: -0.635 [-1.011, 1.007], loss: 0.009867, mean_absolute_error: 0.963958, mean_q: 1.044535\n",
            "  56073/150000: episode: 1346, duration: 0.367s, episode steps: 16, steps per second: 44, episode reward: 0.900, mean reward: 0.056 [0.000, 0.900], mean action: 11.375 [3.000, 14.000], mean observation: -0.632 [-1.009, 1.637], loss: 0.009649, mean_absolute_error: 0.957785, mean_q: 1.033047\n",
            "  56088/150000: episode: 1347, duration: 0.318s, episode steps: 15, steps per second: 47, episode reward: 2.000, mean reward: 0.133 [0.000, 1.100], mean action: 8.933 [4.000, 16.000], mean observation: -0.633 [-1.011, 1.068], loss: 0.008912, mean_absolute_error: 0.955006, mean_q: 1.023023\n",
            "  56111/150000: episode: 1348, duration: 0.422s, episode steps: 23, steps per second: 55, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 12.087 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007127, mean_absolute_error: 0.960719, mean_q: 1.026973\n",
            "  56123/150000: episode: 1349, duration: 0.289s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.250 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.005682, mean_absolute_error: 0.964597, mean_q: 1.037668\n",
            "  56135/150000: episode: 1350, duration: 0.277s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 7.917 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.008911, mean_absolute_error: 0.964303, mean_q: 1.031291\n",
            "  56160/150000: episode: 1351, duration: 0.481s, episode steps: 25, steps per second: 52, episode reward: 1.000, mean reward: 0.040 [0.000, 0.900], mean action: 6.160 [3.000, 17.000], mean observation: -0.632 [-1.011, 2.454], loss: 0.009559, mean_absolute_error: 0.955905, mean_q: 1.026476\n",
            "  56232/150000: episode: 1352, duration: 1.092s, episode steps: 72, steps per second: 66, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 5.556 [0.000, 18.000], mean observation: -0.615 [-1.011, 8.366], loss: 0.006618, mean_absolute_error: 0.951444, mean_q: 1.022594\n",
            "  56244/150000: episode: 1353, duration: 0.289s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.167 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.005850, mean_absolute_error: 0.955379, mean_q: 1.030288\n",
            "  56256/150000: episode: 1354, duration: 0.287s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.667 [0.000, 15.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.001353, mean_absolute_error: 0.955893, mean_q: 1.037197\n",
            "  56381/150000: episode: 1355, duration: 1.763s, episode steps: 125, steps per second: 71, episode reward: 2.000, mean reward: 0.016 [0.000, 1.100], mean action: 9.544 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.308], loss: 0.006704, mean_absolute_error: 0.959142, mean_q: 1.033773\n",
            "  56411/150000: episode: 1356, duration: 0.509s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 3.333 [0.000, 15.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006218, mean_absolute_error: 0.957929, mean_q: 1.032128\n",
            "  56427/150000: episode: 1357, duration: 0.334s, episode steps: 16, steps per second: 48, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 4.875 [0.000, 12.000], mean observation: -0.636 [-1.040, 1.007], loss: 0.011265, mean_absolute_error: 0.958734, mean_q: 1.021998\n",
            "  56470/150000: episode: 1358, duration: 0.687s, episode steps: 43, steps per second: 63, episode reward: 2.000, mean reward: 0.047 [0.000, 1.000], mean action: 7.302 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.007240, mean_absolute_error: 0.949991, mean_q: 1.022854\n",
            "  56614/150000: episode: 1359, duration: 2.044s, episode steps: 144, steps per second: 70, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 10.021 [0.000, 20.000], mean observation: -0.629 [-1.009, 1.013], loss: 0.007176, mean_absolute_error: 0.951841, mean_q: 1.028297\n",
            "  56638/150000: episode: 1360, duration: 0.467s, episode steps: 24, steps per second: 51, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 7.583 [4.000, 12.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.004856, mean_absolute_error: 0.950240, mean_q: 1.028583\n",
            "  56650/150000: episode: 1361, duration: 0.285s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [0.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.005232, mean_absolute_error: 0.949789, mean_q: 1.026881\n",
            "  56659/150000: episode: 1362, duration: 0.220s, episode steps: 9, steps per second: 41, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.778 [4.000, 19.000], mean observation: -0.633 [-1.009, 1.022], loss: 0.011569, mean_absolute_error: 0.948978, mean_q: 1.015599\n",
            "  56671/150000: episode: 1363, duration: 0.284s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.000 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007452, mean_absolute_error: 0.947924, mean_q: 1.024602\n",
            "  56774/150000: episode: 1364, duration: 1.481s, episode steps: 103, steps per second: 70, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 15.058 [0.000, 19.000], mean observation: -0.637 [-1.009, 1.000], loss: 0.008018, mean_absolute_error: 0.944222, mean_q: 1.019687\n",
            "  56804/150000: episode: 1365, duration: 0.522s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.100], mean action: 4.367 [3.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.006404, mean_absolute_error: 0.950800, mean_q: 1.024403\n",
            "  56871/150000: episode: 1366, duration: 1.002s, episode steps: 67, steps per second: 67, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 5.194 [3.000, 20.000], mean observation: -0.612 [-1.061, 9.765], loss: 0.006441, mean_absolute_error: 0.941519, mean_q: 1.013514\n",
            "  56956/150000: episode: 1367, duration: 1.244s, episode steps: 85, steps per second: 68, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 6.259 [2.000, 18.000], mean observation: -0.618 [-1.049, 8.426], loss: 0.006363, mean_absolute_error: 0.944879, mean_q: 1.012949\n",
            "  57007/150000: episode: 1368, duration: 0.814s, episode steps: 51, steps per second: 63, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 7.824 [0.000, 19.000], mean observation: -0.611 [-1.011, 7.740], loss: 0.007088, mean_absolute_error: 0.943879, mean_q: 1.013526\n",
            "  57063/150000: episode: 1369, duration: 0.854s, episode steps: 56, steps per second: 66, episode reward: 2.000, mean reward: 0.036 [0.000, 1.000], mean action: 8.446 [1.000, 20.000], mean observation: -0.632 [-1.011, 1.013], loss: 0.006304, mean_absolute_error: 0.941545, mean_q: 1.013377\n",
            "  57086/150000: episode: 1370, duration: 0.428s, episode steps: 23, steps per second: 54, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 5.957 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.003], loss: 0.009970, mean_absolute_error: 0.942954, mean_q: 1.009247\n",
            "  57227/150000: episode: 1371, duration: 1.999s, episode steps: 141, steps per second: 71, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 9.206 [0.000, 20.000], mean observation: -0.629 [-1.009, 1.861], loss: 0.007006, mean_absolute_error: 0.935789, mean_q: 1.005841\n",
            "  57323/150000: episode: 1372, duration: 1.405s, episode steps: 96, steps per second: 68, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 8.854 [1.000, 20.000], mean observation: -0.632 [-1.011, 1.295], loss: 0.005085, mean_absolute_error: 0.939986, mean_q: 1.009365\n",
            "  57353/150000: episode: 1373, duration: 0.517s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 13.733 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.002429, mean_absolute_error: 0.937793, mean_q: 1.015526\n",
            "  57573/150000: episode: 1374, duration: 3.084s, episode steps: 220, steps per second: 71, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 11.855 [0.000, 20.000], mean observation: -0.629 [-1.009, 5.562], loss: 0.004976, mean_absolute_error: 0.931572, mean_q: 1.000881\n",
            "  57602/150000: episode: 1375, duration: 0.520s, episode steps: 29, steps per second: 56, episode reward: 2.000, mean reward: 0.069 [0.000, 1.000], mean action: 13.828 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.009297, mean_absolute_error: 0.934461, mean_q: 1.002615\n",
            "  57614/150000: episode: 1376, duration: 0.288s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.583 [0.000, 14.000], mean observation: -0.634 [-1.011, 1.134], loss: 0.004706, mean_absolute_error: 0.925700, mean_q: 1.001772\n",
            "  57630/150000: episode: 1377, duration: 0.333s, episode steps: 16, steps per second: 48, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 10.062 [5.000, 12.000], mean observation: -0.635 [-1.041, 1.010], loss: 0.006382, mean_absolute_error: 0.919925, mean_q: 0.990909\n",
            "  57640/150000: episode: 1378, duration: 0.265s, episode steps: 10, steps per second: 38, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.800 [3.000, 16.000], mean observation: -0.636 [-1.011, 1.013], loss: 0.005163, mean_absolute_error: 0.922642, mean_q: 0.992858\n",
            "  57650/150000: episode: 1379, duration: 0.252s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 5.700 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.023], loss: 0.012330, mean_absolute_error: 0.930268, mean_q: 0.995594\n",
            "  57668/150000: episode: 1380, duration: 0.359s, episode steps: 18, steps per second: 50, episode reward: 2.000, mean reward: 0.111 [0.000, 1.000], mean action: 9.444 [5.000, 15.000], mean observation: -0.634 [-1.011, 1.042], loss: 0.006820, mean_absolute_error: 0.926857, mean_q: 1.005904\n",
            "  57680/150000: episode: 1381, duration: 0.291s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.005], loss: 0.011484, mean_absolute_error: 0.925865, mean_q: 1.013152\n",
            "  57689/150000: episode: 1382, duration: 0.253s, episode steps: 9, steps per second: 36, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 12.111 [5.000, 17.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.006241, mean_absolute_error: 0.930704, mean_q: 1.024169\n",
            "  57789/150000: episode: 1383, duration: 1.419s, episode steps: 100, steps per second: 70, episode reward: 2.000, mean reward: 0.020 [0.000, 1.100], mean action: 13.520 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.169], loss: 0.005157, mean_absolute_error: 0.938208, mean_q: 1.017436\n",
            "  57855/150000: episode: 1384, duration: 0.985s, episode steps: 66, steps per second: 67, episode reward: 0.900, mean reward: 0.014 [0.000, 0.900], mean action: 13.076 [2.000, 19.000], mean observation: -0.622 [-1.023, 7.668], loss: 0.006149, mean_absolute_error: 0.931281, mean_q: 0.998905\n",
            "  57885/150000: episode: 1385, duration: 0.525s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.500 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007115, mean_absolute_error: 0.921365, mean_q: 0.989214\n",
            "  57928/150000: episode: 1386, duration: 0.677s, episode steps: 43, steps per second: 64, episode reward: 2.000, mean reward: 0.047 [0.000, 1.000], mean action: 12.256 [4.000, 19.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.005359, mean_absolute_error: 0.924127, mean_q: 0.991381\n",
            "  57958/150000: episode: 1387, duration: 0.552s, episode steps: 30, steps per second: 54, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.067 [5.000, 7.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008037, mean_absolute_error: 0.925821, mean_q: 0.994200\n",
            "  58037/150000: episode: 1388, duration: 1.166s, episode steps: 79, steps per second: 68, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 4.747 [2.000, 16.000], mean observation: -0.614 [-1.011, 9.701], loss: 0.006007, mean_absolute_error: 0.918810, mean_q: 0.999269\n",
            "  58065/150000: episode: 1389, duration: 0.483s, episode steps: 28, steps per second: 58, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 7.214 [3.000, 12.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.004574, mean_absolute_error: 0.914830, mean_q: 0.982044\n",
            "  58075/150000: episode: 1390, duration: 0.257s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 9.900 [5.000, 18.000], mean observation: -0.632 [-1.011, 1.398], loss: 0.002669, mean_absolute_error: 0.924410, mean_q: 1.000085\n",
            "  58193/150000: episode: 1391, duration: 1.663s, episode steps: 118, steps per second: 71, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 8.424 [0.000, 18.000], mean observation: -0.630 [-1.011, 1.000], loss: 0.006804, mean_absolute_error: 0.922694, mean_q: 0.994158\n",
            "  58264/150000: episode: 1392, duration: 1.058s, episode steps: 71, steps per second: 67, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 6.493 [0.000, 20.000], mean observation: -0.639 [-1.009, 1.255], loss: 0.006252, mean_absolute_error: 0.921586, mean_q: 0.996154\n",
            "  58360/150000: episode: 1393, duration: 1.363s, episode steps: 96, steps per second: 70, episode reward: 2.000, mean reward: 0.021 [0.000, 1.000], mean action: 13.344 [5.000, 19.000], mean observation: -0.629 [-1.011, 1.010], loss: 0.006206, mean_absolute_error: 0.915272, mean_q: 0.983163\n",
            "  58414/150000: episode: 1394, duration: 0.819s, episode steps: 54, steps per second: 66, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 5.648 [5.000, 19.000], mean observation: -0.610 [-1.017, 7.989], loss: 0.004276, mean_absolute_error: 0.919249, mean_q: 0.987929\n",
            "  58491/150000: episode: 1395, duration: 1.132s, episode steps: 77, steps per second: 68, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 8.364 [0.000, 20.000], mean observation: -0.634 [-1.009, 1.036], loss: 0.007111, mean_absolute_error: 0.914857, mean_q: 0.981327\n",
            "  58503/150000: episode: 1396, duration: 0.287s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.041, 1.010], loss: 0.006513, mean_absolute_error: 0.914803, mean_q: 0.976140\n",
            "  58574/150000: episode: 1397, duration: 1.068s, episode steps: 71, steps per second: 66, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 7.366 [2.000, 16.000], mean observation: -0.620 [-1.009, 8.189], loss: 0.005345, mean_absolute_error: 0.912964, mean_q: 0.988678\n",
            "  58712/150000: episode: 1398, duration: 1.993s, episode steps: 138, steps per second: 69, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 8.543 [0.000, 19.000], mean observation: -0.629 [-1.009, 7.213], loss: 0.005946, mean_absolute_error: 0.908229, mean_q: 0.977687\n",
            "  58752/150000: episode: 1399, duration: 0.613s, episode steps: 40, steps per second: 65, episode reward: 2.000, mean reward: 0.050 [0.000, 1.000], mean action: 6.825 [0.000, 16.000], mean observation: -0.632 [-1.011, 1.015], loss: 0.009919, mean_absolute_error: 0.903417, mean_q: 0.963889\n",
            "  58830/150000: episode: 1400, duration: 1.178s, episode steps: 78, steps per second: 66, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 4.692 [0.000, 20.000], mean observation: -0.616 [-1.011, 8.317], loss: 0.004713, mean_absolute_error: 0.897096, mean_q: 0.966077\n",
            "  58968/150000: episode: 1401, duration: 1.905s, episode steps: 138, steps per second: 72, episode reward: 0.900, mean reward: 0.007 [0.000, 0.900], mean action: 6.304 [0.000, 20.000], mean observation: -0.634 [-1.009, 1.310], loss: 0.005764, mean_absolute_error: 0.896143, mean_q: 0.965803\n",
            "  58998/150000: episode: 1402, duration: 0.516s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 8.467 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.002795, mean_absolute_error: 0.897511, mean_q: 0.966420\n",
            "  59119/150000: episode: 1403, duration: 1.730s, episode steps: 121, steps per second: 70, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 5.207 [2.000, 19.000], mean observation: -0.633 [-1.009, 1.081], loss: 0.007430, mean_absolute_error: 0.900958, mean_q: 0.970731\n",
            "  59299/150000: episode: 1404, duration: 2.614s, episode steps: 180, steps per second: 69, episode reward: 1.000, mean reward: 0.006 [0.000, 0.900], mean action: 4.806 [0.000, 20.000], mean observation: -0.627 [-1.090, 9.220], loss: 0.006363, mean_absolute_error: 0.900839, mean_q: 0.971478\n",
            "  59347/150000: episode: 1405, duration: 0.772s, episode steps: 48, steps per second: 62, episode reward: 2.000, mean reward: 0.042 [0.000, 1.000], mean action: 5.458 [2.000, 20.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.008120, mean_absolute_error: 0.899440, mean_q: 0.966370\n",
            "  59465/150000: episode: 1406, duration: 1.743s, episode steps: 118, steps per second: 68, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 7.119 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.362], loss: 0.007306, mean_absolute_error: 0.894071, mean_q: 0.958252\n",
            "  59495/150000: episode: 1407, duration: 0.509s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 7.900 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006166, mean_absolute_error: 0.888114, mean_q: 0.960403\n",
            "  59583/150000: episode: 1408, duration: 1.269s, episode steps: 88, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 10.705 [0.000, 20.000], mean observation: -0.632 [-1.011, 1.000], loss: 0.006217, mean_absolute_error: 0.899108, mean_q: 0.970890\n",
            "  59724/150000: episode: 1409, duration: 1.987s, episode steps: 141, steps per second: 71, episode reward: 2.000, mean reward: 0.014 [0.000, 1.000], mean action: 10.752 [2.000, 20.000], mean observation: -0.630 [-1.011, 1.010], loss: 0.005990, mean_absolute_error: 0.896384, mean_q: 0.966921\n",
            "  59733/150000: episode: 1410, duration: 0.242s, episode steps: 9, steps per second: 37, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.333 [5.000, 15.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.003069, mean_absolute_error: 0.896882, mean_q: 0.961226\n",
            "  59811/150000: episode: 1411, duration: 1.171s, episode steps: 78, steps per second: 67, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 10.397 [5.000, 16.000], mean observation: -0.634 [-1.009, 1.832], loss: 0.004766, mean_absolute_error: 0.890053, mean_q: 0.951762\n",
            "  59876/150000: episode: 1412, duration: 0.989s, episode steps: 65, steps per second: 66, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 5.123 [2.000, 20.000], mean observation: -0.632 [-1.011, 1.012], loss: 0.005262, mean_absolute_error: 0.891058, mean_q: 0.957107\n",
            "  59994/150000: episode: 1413, duration: 1.628s, episode steps: 118, steps per second: 72, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 5.949 [1.000, 20.000], mean observation: -0.639 [-1.011, 1.346], loss: 0.006315, mean_absolute_error: 0.887137, mean_q: 0.952402\n",
            "  60051/150000: episode: 1414, duration: 0.870s, episode steps: 57, steps per second: 66, episode reward: 2.000, mean reward: 0.035 [0.000, 1.000], mean action: 8.789 [0.000, 19.000], mean observation: -0.632 [-1.011, 1.013], loss: 0.004444, mean_absolute_error: 0.885789, mean_q: 0.954926\n",
            "  60168/150000: episode: 1415, duration: 1.679s, episode steps: 117, steps per second: 70, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 8.556 [1.000, 20.000], mean observation: -0.631 [-1.011, 1.000], loss: 0.005305, mean_absolute_error: 0.879676, mean_q: 0.942007\n",
            "  60178/150000: episode: 1416, duration: 0.254s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 7.600 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006818, mean_absolute_error: 0.873219, mean_q: 0.936709\n",
            "  60261/150000: episode: 1417, duration: 1.180s, episode steps: 83, steps per second: 70, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 13.289 [1.000, 20.000], mean observation: -0.635 [-1.009, 1.055], loss: 0.005944, mean_absolute_error: 0.869650, mean_q: 0.936953\n",
            "  60304/150000: episode: 1418, duration: 0.689s, episode steps: 43, steps per second: 62, episode reward: 2.000, mean reward: 0.047 [0.000, 1.000], mean action: 6.930 [2.000, 19.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.008043, mean_absolute_error: 0.867903, mean_q: 0.935202\n",
            "  60417/150000: episode: 1419, duration: 1.600s, episode steps: 113, steps per second: 71, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 3.938 [1.000, 19.000], mean observation: -0.639 [-1.009, 1.000], loss: 0.006846, mean_absolute_error: 0.864606, mean_q: 0.934831\n",
            "  60460/150000: episode: 1420, duration: 0.681s, episode steps: 43, steps per second: 63, episode reward: 2.000, mean reward: 0.047 [0.000, 1.000], mean action: 11.047 [3.000, 19.000], mean observation: -0.632 [-1.011, 1.013], loss: 0.005824, mean_absolute_error: 0.865823, mean_q: 0.929953\n",
            "  60478/150000: episode: 1421, duration: 0.358s, episode steps: 18, steps per second: 50, episode reward: 2.000, mean reward: 0.111 [0.000, 1.000], mean action: 12.333 [0.000, 15.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.008747, mean_absolute_error: 0.865565, mean_q: 0.930051\n",
            "  60527/150000: episode: 1422, duration: 0.739s, episode steps: 49, steps per second: 66, episode reward: 2.000, mean reward: 0.041 [0.000, 1.000], mean action: 8.408 [1.000, 18.000], mean observation: -0.633 [-1.011, 1.022], loss: 0.008067, mean_absolute_error: 0.862804, mean_q: 0.927940\n",
            "  60559/150000: episode: 1423, duration: 0.514s, episode steps: 32, steps per second: 62, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 5.000 [4.000, 6.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.005030, mean_absolute_error: 0.861415, mean_q: 0.925119\n",
            "  60671/150000: episode: 1424, duration: 1.615s, episode steps: 112, steps per second: 69, episode reward: 2.000, mean reward: 0.018 [0.000, 1.000], mean action: 10.580 [0.000, 18.000], mean observation: -0.634 [-1.027, 1.000], loss: 0.006630, mean_absolute_error: 0.866636, mean_q: 0.932030\n",
            "  60683/150000: episode: 1425, duration: 0.274s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.667 [3.000, 12.000], mean observation: -0.636 [-1.041, 1.010], loss: 0.004464, mean_absolute_error: 0.866617, mean_q: 0.934954\n",
            "  60695/150000: episode: 1426, duration: 0.289s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.083 [5.000, 17.000], mean observation: -0.636 [-1.041, 1.010], loss: 0.008686, mean_absolute_error: 0.864825, mean_q: 0.922367\n",
            "  60745/150000: episode: 1427, duration: 0.785s, episode steps: 50, steps per second: 64, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 9.780 [5.000, 18.000], mean observation: -0.611 [-1.011, 7.668], loss: 0.007062, mean_absolute_error: 0.859654, mean_q: 0.929820\n",
            "  60804/150000: episode: 1428, duration: 0.924s, episode steps: 59, steps per second: 64, episode reward: 0.900, mean reward: 0.015 [0.000, 0.900], mean action: 6.441 [0.000, 10.000], mean observation: -0.613 [-1.073, 9.173], loss: 0.006244, mean_absolute_error: 0.857594, mean_q: 0.928343\n",
            "  60914/150000: episode: 1429, duration: 1.568s, episode steps: 110, steps per second: 70, episode reward: 2.000, mean reward: 0.018 [0.000, 1.000], mean action: 8.545 [1.000, 15.000], mean observation: -0.634 [-1.011, 1.477], loss: 0.005058, mean_absolute_error: 0.856647, mean_q: 0.923400\n",
            "  60926/150000: episode: 1430, duration: 0.281s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.833 [5.000, 19.000], mean observation: -0.636 [-1.011, 1.007], loss: 0.009651, mean_absolute_error: 0.858837, mean_q: 0.932241\n",
            "  60964/150000: episode: 1431, duration: 0.620s, episode steps: 38, steps per second: 61, episode reward: 2.000, mean reward: 0.053 [0.000, 1.100], mean action: 12.921 [0.000, 19.000], mean observation: -0.634 [-1.011, 1.206], loss: 0.005234, mean_absolute_error: 0.864034, mean_q: 0.935285\n",
            "  61033/150000: episode: 1432, duration: 1.018s, episode steps: 69, steps per second: 68, episode reward: 0.200, mean reward: 0.003 [0.000, 0.200], mean action: 7.565 [5.000, 18.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.006129, mean_absolute_error: 0.865121, mean_q: 0.931211\n",
            "  61117/150000: episode: 1433, duration: 1.180s, episode steps: 84, steps per second: 71, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 5.940 [0.000, 19.000], mean observation: -0.636 [-1.011, 1.313], loss: 0.006330, mean_absolute_error: 0.862575, mean_q: 0.928146\n",
            "  61149/150000: episode: 1434, duration: 0.547s, episode steps: 32, steps per second: 59, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 5.344 [5.000, 13.000], mean observation: -0.634 [-1.011, 1.004], loss: 0.006251, mean_absolute_error: 0.864847, mean_q: 0.928629\n",
            "  61190/150000: episode: 1435, duration: 0.662s, episode steps: 41, steps per second: 62, episode reward: 2.000, mean reward: 0.049 [0.000, 1.000], mean action: 7.439 [1.000, 18.000], mean observation: -0.633 [-1.011, 1.009], loss: 0.005791, mean_absolute_error: 0.867472, mean_q: 0.937772\n",
            "  61218/150000: episode: 1436, duration: 0.483s, episode steps: 28, steps per second: 58, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 7.000 [2.000, 17.000], mean observation: -0.635 [-1.011, 1.013], loss: 0.005185, mean_absolute_error: 0.862303, mean_q: 0.929852\n",
            "  61240/150000: episode: 1437, duration: 0.402s, episode steps: 22, steps per second: 55, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 9.500 [4.000, 16.000], mean observation: -0.635 [-1.011, 1.024], loss: 0.008920, mean_absolute_error: 0.868181, mean_q: 0.930224\n",
            "  61322/150000: episode: 1438, duration: 1.191s, episode steps: 82, steps per second: 69, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 10.610 [0.000, 18.000], mean observation: -0.634 [-1.011, 1.404], loss: 0.007926, mean_absolute_error: 0.867453, mean_q: 0.934995\n",
            "  61413/150000: episode: 1439, duration: 1.317s, episode steps: 91, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 6.440 [1.000, 9.000], mean observation: -0.632 [-1.011, 1.389], loss: 0.005797, mean_absolute_error: 0.873864, mean_q: 0.947485\n",
            "  61540/150000: episode: 1440, duration: 1.809s, episode steps: 127, steps per second: 70, episode reward: 0.900, mean reward: 0.007 [0.000, 0.900], mean action: 6.709 [0.000, 18.000], mean observation: -0.633 [-1.009, 1.251], loss: 0.007102, mean_absolute_error: 0.892409, mean_q: 0.975232\n",
            "  61649/150000: episode: 1441, duration: 1.575s, episode steps: 109, steps per second: 69, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 12.771 [0.000, 19.000], mean observation: -0.635 [-1.009, 1.231], loss: 0.006721, mean_absolute_error: 0.897623, mean_q: 0.971829\n",
            "  61661/150000: episode: 1442, duration: 0.277s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.917 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.006595, mean_absolute_error: 0.899245, mean_q: 0.961634\n",
            "  61673/150000: episode: 1443, duration: 0.276s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.041, 1.010], loss: 0.006767, mean_absolute_error: 0.897325, mean_q: 0.963749\n",
            "  61688/150000: episode: 1444, duration: 0.320s, episode steps: 15, steps per second: 47, episode reward: 2.000, mean reward: 0.133 [0.000, 1.100], mean action: 8.667 [4.000, 12.000], mean observation: -0.632 [-1.011, 1.368], loss: 0.003572, mean_absolute_error: 0.894404, mean_q: 0.960305\n",
            "  61765/150000: episode: 1445, duration: 1.139s, episode steps: 77, steps per second: 68, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 9.221 [1.000, 20.000], mean observation: -0.636 [-1.009, 1.000], loss: 0.007683, mean_absolute_error: 0.904549, mean_q: 0.973130\n",
            "  61873/150000: episode: 1446, duration: 1.544s, episode steps: 108, steps per second: 70, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 6.167 [2.000, 19.000], mean observation: -0.638 [-1.009, 1.000], loss: 0.005710, mean_absolute_error: 0.903099, mean_q: 0.969496\n",
            "  61886/150000: episode: 1447, duration: 0.286s, episode steps: 13, steps per second: 45, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.538 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007533, mean_absolute_error: 0.900253, mean_q: 0.968520\n",
            "  61898/150000: episode: 1448, duration: 0.294s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.002], loss: 0.005511, mean_absolute_error: 0.904745, mean_q: 0.978713\n",
            "  61910/150000: episode: 1449, duration: 0.286s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 8.833 [1.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.008737, mean_absolute_error: 0.909401, mean_q: 0.978125\n",
            "  61940/150000: episode: 1450, duration: 0.499s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.267 [0.000, 15.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.005045, mean_absolute_error: 0.903281, mean_q: 0.969975\n",
            "  61962/150000: episode: 1451, duration: 0.425s, episode steps: 22, steps per second: 52, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 12.545 [0.000, 18.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007604, mean_absolute_error: 0.907137, mean_q: 0.983243\n",
            "  61972/150000: episode: 1452, duration: 0.251s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.100 [0.000, 12.000], mean observation: -0.632 [-1.011, 1.351], loss: 0.015632, mean_absolute_error: 0.901952, mean_q: 0.973217\n",
            "  61981/150000: episode: 1453, duration: 0.214s, episode steps: 9, steps per second: 42, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 10.444 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.022], loss: 0.007386, mean_absolute_error: 0.899286, mean_q: 0.972329\n",
            "  62081/150000: episode: 1454, duration: 1.504s, episode steps: 100, steps per second: 67, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 6.780 [3.000, 20.000], mean observation: -0.626 [-1.046, 8.078], loss: 0.005923, mean_absolute_error: 0.904391, mean_q: 0.979027\n",
            "  62123/150000: episode: 1455, duration: 0.668s, episode steps: 42, steps per second: 63, episode reward: 2.000, mean reward: 0.048 [0.000, 1.100], mean action: 8.786 [3.000, 12.000], mean observation: -0.631 [-1.011, 2.187], loss: 0.004605, mean_absolute_error: 0.906615, mean_q: 0.971348\n",
            "  62133/150000: episode: 1456, duration: 0.271s, episode steps: 10, steps per second: 37, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 12.700 [3.000, 17.000], mean observation: -0.632 [-1.041, 1.546], loss: 0.007811, mean_absolute_error: 0.906344, mean_q: 0.972375\n",
            "  62243/150000: episode: 1457, duration: 1.547s, episode steps: 110, steps per second: 71, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 10.864 [0.000, 20.000], mean observation: -0.630 [-1.009, 1.037], loss: 0.005469, mean_absolute_error: 0.910317, mean_q: 0.980165\n",
            "  62308/150000: episode: 1458, duration: 0.999s, episode steps: 65, steps per second: 65, episode reward: 0.900, mean reward: 0.014 [0.000, 0.900], mean action: 3.954 [3.000, 7.000], mean observation: -0.634 [-1.009, 1.014], loss: 0.004657, mean_absolute_error: 0.907835, mean_q: 0.972102\n",
            "  62456/150000: episode: 1459, duration: 2.092s, episode steps: 148, steps per second: 71, episode reward: 2.000, mean reward: 0.014 [0.000, 1.000], mean action: 7.101 [0.000, 20.000], mean observation: -0.630 [-1.026, 1.311], loss: 0.007296, mean_absolute_error: 0.907496, mean_q: 0.977160\n",
            "  62607/150000: episode: 1460, duration: 2.127s, episode steps: 151, steps per second: 71, episode reward: 2.000, mean reward: 0.013 [0.000, 1.000], mean action: 7.219 [0.000, 19.000], mean observation: -0.631 [-1.011, 1.270], loss: 0.007544, mean_absolute_error: 0.903680, mean_q: 0.973494\n",
            "  62648/150000: episode: 1461, duration: 0.834s, episode steps: 41, steps per second: 49, episode reward: 1.000, mean reward: 0.024 [0.000, 0.900], mean action: 13.780 [4.000, 19.000], mean observation: -0.634 [-1.011, 1.354], loss: 0.004878, mean_absolute_error: 0.906907, mean_q: 0.973239\n",
            "  62774/150000: episode: 1462, duration: 1.821s, episode steps: 126, steps per second: 69, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 12.230 [1.000, 19.000], mean observation: -0.628 [-1.046, 8.026], loss: 0.006791, mean_absolute_error: 0.897475, mean_q: 0.963581\n",
            "  62788/150000: episode: 1463, duration: 0.328s, episode steps: 14, steps per second: 43, episode reward: 2.000, mean reward: 0.143 [0.000, 1.000], mean action: 3.000 [0.000, 13.000], mean observation: -0.635 [-1.039, 1.000], loss: 0.007197, mean_absolute_error: 0.894662, mean_q: 0.955028\n",
            "  62870/150000: episode: 1464, duration: 1.191s, episode steps: 82, steps per second: 69, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 5.122 [0.000, 16.000], mean observation: -0.633 [-1.009, 1.093], loss: 0.005008, mean_absolute_error: 0.888322, mean_q: 0.949562\n",
            "  62976/150000: episode: 1465, duration: 1.543s, episode steps: 106, steps per second: 69, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 12.764 [2.000, 20.000], mean observation: -0.633 [-1.011, 2.853], loss: 0.006295, mean_absolute_error: 0.885035, mean_q: 0.948089\n",
            "  63015/150000: episode: 1466, duration: 0.637s, episode steps: 39, steps per second: 61, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 9.128 [5.000, 14.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.006929, mean_absolute_error: 0.882912, mean_q: 0.954233\n",
            "  63052/150000: episode: 1467, duration: 0.608s, episode steps: 37, steps per second: 61, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.514 [1.000, 18.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.009118, mean_absolute_error: 0.880932, mean_q: 0.946616\n",
            "  63114/150000: episode: 1468, duration: 0.947s, episode steps: 62, steps per second: 65, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 11.323 [3.000, 17.000], mean observation: -0.631 [-1.009, 1.139], loss: 0.004020, mean_absolute_error: 0.885268, mean_q: 0.950388\n",
            "  63144/150000: episode: 1469, duration: 0.506s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.700 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005326, mean_absolute_error: 0.881677, mean_q: 0.946059\n",
            "  63179/150000: episode: 1470, duration: 0.613s, episode steps: 35, steps per second: 57, episode reward: 1.000, mean reward: 0.029 [0.000, 0.900], mean action: 13.143 [5.000, 20.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.006822, mean_absolute_error: 0.877906, mean_q: 0.947116\n",
            "  63249/150000: episode: 1471, duration: 1.064s, episode steps: 70, steps per second: 66, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 13.271 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.016], loss: 0.004971, mean_absolute_error: 0.881370, mean_q: 0.947898\n",
            "  63324/150000: episode: 1472, duration: 1.144s, episode steps: 75, steps per second: 66, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 9.573 [0.000, 18.000], mean observation: -0.617 [-1.041, 8.159], loss: 0.006064, mean_absolute_error: 0.878703, mean_q: 0.944473\n",
            "  63354/150000: episode: 1473, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 10.467 [0.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007732, mean_absolute_error: 0.874062, mean_q: 0.935599\n",
            "  63384/150000: episode: 1474, duration: 0.497s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.333 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.006174, mean_absolute_error: 0.868896, mean_q: 0.941473\n",
            "  63784/150000: episode: 1475, duration: 5.464s, episode steps: 400, steps per second: 73, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 7.232 [0.000, 20.000], mean observation: -0.636 [-1.011, 7.733], loss: 0.004870, mean_absolute_error: 0.875116, mean_q: 0.939649\n",
            "  63870/150000: episode: 1476, duration: 1.286s, episode steps: 86, steps per second: 67, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 6.523 [2.000, 16.000], mean observation: -0.634 [-1.009, 1.503], loss: 0.007221, mean_absolute_error: 0.869154, mean_q: 0.937563\n",
            "  63892/150000: episode: 1477, duration: 0.451s, episode steps: 22, steps per second: 49, episode reward: 1.000, mean reward: 0.045 [0.000, 0.900], mean action: 5.773 [4.000, 19.000], mean observation: -0.634 [-1.009, 1.012], loss: 0.006212, mean_absolute_error: 0.863689, mean_q: 0.941096\n",
            "  63986/150000: episode: 1478, duration: 1.385s, episode steps: 94, steps per second: 68, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 6.479 [0.000, 16.000], mean observation: -0.632 [-1.011, 1.000], loss: 0.007563, mean_absolute_error: 0.863632, mean_q: 0.930533\n",
            "  64067/150000: episode: 1479, duration: 1.216s, episode steps: 81, steps per second: 67, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 10.605 [0.000, 20.000], mean observation: -0.636 [-1.009, 1.414], loss: 0.005741, mean_absolute_error: 0.865776, mean_q: 0.928051\n",
            "  64087/150000: episode: 1480, duration: 0.393s, episode steps: 20, steps per second: 51, episode reward: 2.000, mean reward: 0.100 [0.000, 1.000], mean action: 7.000 [0.000, 15.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007258, mean_absolute_error: 0.862846, mean_q: 0.927505\n",
            "  64099/150000: episode: 1481, duration: 0.278s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.041, 1.010], loss: 0.004548, mean_absolute_error: 0.853189, mean_q: 0.916721\n",
            "  64111/150000: episode: 1482, duration: 0.302s, episode steps: 12, steps per second: 40, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.667 [5.000, 15.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.005540, mean_absolute_error: 0.858323, mean_q: 0.925257\n",
            "  64123/150000: episode: 1483, duration: 0.293s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.003], loss: 0.004411, mean_absolute_error: 0.862336, mean_q: 0.927407\n",
            "  64133/150000: episode: 1484, duration: 0.251s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 7.800 [5.000, 19.000], mean observation: -0.631 [-1.011, 1.856], loss: 0.005398, mean_absolute_error: 0.863130, mean_q: 0.927271\n",
            "  64163/150000: episode: 1485, duration: 0.564s, episode steps: 30, steps per second: 53, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006480, mean_absolute_error: 0.857986, mean_q: 0.925141\n",
            "  64173/150000: episode: 1486, duration: 0.255s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 2.000], mean action: 6.300 [4.000, 12.000], mean observation: -0.632 [-1.011, 1.135], loss: 0.007669, mean_absolute_error: 0.858256, mean_q: 0.929513\n",
            "  64207/150000: episode: 1487, duration: 0.570s, episode steps: 34, steps per second: 60, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 5.471 [2.000, 13.000], mean observation: -0.634 [-1.011, 1.015], loss: 0.008625, mean_absolute_error: 0.859574, mean_q: 0.930712\n",
            "  64304/150000: episode: 1488, duration: 1.375s, episode steps: 97, steps per second: 71, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 11.505 [1.000, 19.000], mean observation: -0.632 [-1.011, 1.103], loss: 0.005349, mean_absolute_error: 0.854656, mean_q: 0.917665\n",
            "  64382/150000: episode: 1489, duration: 1.114s, episode steps: 78, steps per second: 70, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 2.641 [0.000, 14.000], mean observation: -0.615 [-1.059, 9.880], loss: 0.005590, mean_absolute_error: 0.858310, mean_q: 0.920873\n",
            "  64461/150000: episode: 1490, duration: 1.128s, episode steps: 79, steps per second: 70, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 9.354 [0.000, 20.000], mean observation: -0.637 [-1.009, 1.000], loss: 0.007160, mean_absolute_error: 0.851199, mean_q: 0.917719\n",
            "  64498/150000: episode: 1491, duration: 0.581s, episode steps: 37, steps per second: 64, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.081 [0.000, 16.000], mean observation: -0.634 [-1.011, 1.015], loss: 0.010861, mean_absolute_error: 0.846447, mean_q: 0.917009\n",
            "  64634/150000: episode: 1492, duration: 1.996s, episode steps: 136, steps per second: 68, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 7.382 [2.000, 19.000], mean observation: -0.613 [-1.009, 7.662], loss: 0.006039, mean_absolute_error: 0.854791, mean_q: 0.920917\n",
            "  64656/150000: episode: 1493, duration: 0.412s, episode steps: 22, steps per second: 53, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 11.727 [2.000, 17.000], mean observation: -0.635 [-1.011, 1.017], loss: 0.005340, mean_absolute_error: 0.856107, mean_q: 0.916340\n",
            "  64674/150000: episode: 1494, duration: 0.347s, episode steps: 18, steps per second: 52, episode reward: 2.000, mean reward: 0.111 [0.000, 1.100], mean action: 15.778 [2.000, 17.000], mean observation: -0.631 [-1.539, 2.361], loss: 0.005110, mean_absolute_error: 0.858089, mean_q: 0.923605\n",
            "  64711/150000: episode: 1495, duration: 0.640s, episode steps: 37, steps per second: 58, episode reward: 1.000, mean reward: 0.027 [0.000, 0.900], mean action: 7.027 [4.000, 17.000], mean observation: -0.633 [-1.009, 1.013], loss: 0.007180, mean_absolute_error: 0.853282, mean_q: 0.922215\n",
            "  64812/150000: episode: 1496, duration: 1.409s, episode steps: 101, steps per second: 72, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 9.485 [0.000, 19.000], mean observation: -0.632 [-1.011, 1.257], loss: 0.005790, mean_absolute_error: 0.853251, mean_q: 0.915549\n",
            "  64822/150000: episode: 1497, duration: 0.256s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 7.200 [5.000, 20.000], mean observation: -0.633 [-1.037, 1.109], loss: 0.001398, mean_absolute_error: 0.851184, mean_q: 0.915758\n",
            "  64852/150000: episode: 1498, duration: 0.531s, episode steps: 30, steps per second: 56, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.367 [0.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005590, mean_absolute_error: 0.855647, mean_q: 0.916291\n",
            "  64890/150000: episode: 1499, duration: 0.617s, episode steps: 38, steps per second: 62, episode reward: 2.000, mean reward: 0.053 [0.000, 1.000], mean action: 6.237 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.009], loss: 0.004522, mean_absolute_error: 0.853146, mean_q: 0.923152\n",
            "  65086/150000: episode: 1500, duration: 2.650s, episode steps: 196, steps per second: 74, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 7.913 [0.000, 20.000], mean observation: -0.629 [-1.011, 7.372], loss: 0.006000, mean_absolute_error: 0.853073, mean_q: 0.916828\n",
            "  65102/150000: episode: 1501, duration: 0.355s, episode steps: 16, steps per second: 45, episode reward: 0.900, mean reward: 0.056 [0.000, 0.900], mean action: 11.062 [3.000, 12.000], mean observation: -0.632 [-1.009, 2.173], loss: 0.008110, mean_absolute_error: 0.851883, mean_q: 0.925639\n",
            "  65114/150000: episode: 1502, duration: 0.289s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.002799, mean_absolute_error: 0.858089, mean_q: 0.924830\n",
            "  65126/150000: episode: 1503, duration: 0.270s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.833 [5.000, 17.000], mean observation: -0.633 [-1.011, 1.380], loss: 0.007548, mean_absolute_error: 0.863321, mean_q: 0.930943\n",
            "  65139/150000: episode: 1504, duration: 0.287s, episode steps: 13, steps per second: 45, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.308 [3.000, 19.000], mean observation: -0.636 [-1.011, 1.017], loss: 0.004090, mean_absolute_error: 0.864076, mean_q: 0.941049\n",
            "  65159/150000: episode: 1505, duration: 0.400s, episode steps: 20, steps per second: 50, episode reward: 2.000, mean reward: 0.100 [0.000, 1.000], mean action: 8.500 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.007], loss: 0.002735, mean_absolute_error: 0.860572, mean_q: 0.926237\n",
            "  65194/150000: episode: 1506, duration: 0.600s, episode steps: 35, steps per second: 58, episode reward: 0.900, mean reward: 0.026 [0.000, 0.900], mean action: 7.714 [3.000, 14.000], mean observation: -0.634 [-1.009, 1.001], loss: 0.007941, mean_absolute_error: 0.856704, mean_q: 0.925083\n",
            "  65294/150000: episode: 1507, duration: 1.436s, episode steps: 100, steps per second: 70, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 5.620 [1.000, 19.000], mean observation: -0.632 [-1.011, 1.039], loss: 0.006263, mean_absolute_error: 0.849241, mean_q: 0.926060\n",
            "  65314/150000: episode: 1508, duration: 0.412s, episode steps: 20, steps per second: 49, episode reward: 0.900, mean reward: 0.045 [0.000, 0.900], mean action: 4.200 [3.000, 16.000], mean observation: -0.633 [-1.009, 1.364], loss: 0.007154, mean_absolute_error: 0.852202, mean_q: 0.925446\n",
            "  65346/150000: episode: 1509, duration: 0.572s, episode steps: 32, steps per second: 56, episode reward: 0.900, mean reward: 0.028 [0.000, 0.900], mean action: 7.625 [3.000, 14.000], mean observation: -0.631 [-1.009, 3.548], loss: 0.003981, mean_absolute_error: 0.856620, mean_q: 0.923564\n",
            "  65364/150000: episode: 1510, duration: 0.353s, episode steps: 18, steps per second: 51, episode reward: 2.000, mean reward: 0.111 [0.000, 1.000], mean action: 9.389 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.004349, mean_absolute_error: 0.859397, mean_q: 0.919811\n",
            "  65382/150000: episode: 1511, duration: 0.359s, episode steps: 18, steps per second: 50, episode reward: 2.000, mean reward: 0.111 [0.000, 1.000], mean action: 9.667 [0.000, 19.000], mean observation: -0.635 [-1.011, 1.011], loss: 0.005934, mean_absolute_error: 0.860234, mean_q: 0.921905\n",
            "  65394/150000: episode: 1512, duration: 0.286s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.004379, mean_absolute_error: 0.856865, mean_q: 0.909684\n",
            "  65404/150000: episode: 1513, duration: 0.247s, episode steps: 10, steps per second: 41, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 5.700 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.012], loss: 0.005210, mean_absolute_error: 0.863743, mean_q: 0.925852\n",
            "  65434/150000: episode: 1514, duration: 0.507s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.933 [3.000, 5.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.006925, mean_absolute_error: 0.856877, mean_q: 0.924586\n",
            "  65468/150000: episode: 1515, duration: 0.585s, episode steps: 34, steps per second: 58, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 5.235 [2.000, 19.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.006176, mean_absolute_error: 0.857760, mean_q: 0.922730\n",
            "  65498/150000: episode: 1516, duration: 0.512s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.002328, mean_absolute_error: 0.853605, mean_q: 0.917982\n",
            "  65575/150000: episode: 1517, duration: 1.141s, episode steps: 77, steps per second: 67, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 13.481 [2.000, 19.000], mean observation: -0.630 [-1.011, 1.009], loss: 0.004118, mean_absolute_error: 0.853219, mean_q: 0.914787\n",
            "  65634/150000: episode: 1518, duration: 0.887s, episode steps: 59, steps per second: 67, episode reward: 2.000, mean reward: 0.034 [0.000, 1.000], mean action: 8.678 [0.000, 19.000], mean observation: -0.631 [-1.011, 1.012], loss: 0.006736, mean_absolute_error: 0.851000, mean_q: 0.918235\n",
            "  65664/150000: episode: 1519, duration: 0.508s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.833 [2.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004891, mean_absolute_error: 0.842707, mean_q: 0.918710\n",
            "  65676/150000: episode: 1520, duration: 0.278s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 14.000], mean observation: -0.635 [-1.011, 1.005], loss: 0.003381, mean_absolute_error: 0.848537, mean_q: 0.915468\n",
            "  65688/150000: episode: 1521, duration: 0.275s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.250 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.496], loss: 0.011136, mean_absolute_error: 0.853126, mean_q: 0.930084\n",
            "  65718/150000: episode: 1522, duration: 0.523s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.867 [1.000, 5.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.006045, mean_absolute_error: 0.849192, mean_q: 0.918481\n",
            "  65844/150000: episode: 1523, duration: 1.929s, episode steps: 126, steps per second: 65, episode reward: 0.900, mean reward: 0.007 [0.000, 0.900], mean action: 8.452 [0.000, 20.000], mean observation: -0.635 [-1.011, 1.757], loss: 0.005846, mean_absolute_error: 0.855085, mean_q: 0.923747\n",
            "  65874/150000: episode: 1524, duration: 0.507s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.433 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004819, mean_absolute_error: 0.855408, mean_q: 0.923627\n",
            "  66113/150000: episode: 1525, duration: 3.241s, episode steps: 239, steps per second: 74, episode reward: 0.000, mean reward: 0.000 [-1.000, 0.900], mean action: 8.992 [1.000, 20.000], mean observation: -0.632 [-1.015, 8.699], loss: 0.007429, mean_absolute_error: 0.855270, mean_q: 0.922083\n",
            "  66158/150000: episode: 1526, duration: 0.776s, episode steps: 45, steps per second: 58, episode reward: 1.000, mean reward: 0.022 [0.000, 0.900], mean action: 4.489 [0.000, 19.000], mean observation: -0.633 [-1.009, 1.013], loss: 0.004253, mean_absolute_error: 0.852010, mean_q: 0.939835\n",
            "  66195/150000: episode: 1527, duration: 0.601s, episode steps: 37, steps per second: 62, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.486 [3.000, 20.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.004664, mean_absolute_error: 0.857995, mean_q: 0.932946\n",
            "  66205/150000: episode: 1528, duration: 0.243s, episode steps: 10, steps per second: 41, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 13.100 [5.000, 17.000], mean observation: -0.632 [-1.011, 1.551], loss: 0.006583, mean_absolute_error: 0.857583, mean_q: 0.914754\n",
            "  66217/150000: episode: 1529, duration: 0.253s, episode steps: 12, steps per second: 47, episode reward: 2.000, mean reward: 0.167 [0.000, 1.000], mean action: 8.750 [5.000, 18.000], mean observation: -0.635 [-1.011, 1.020], loss: 0.009586, mean_absolute_error: 0.859474, mean_q: 0.926363\n",
            "  66247/150000: episode: 1530, duration: 0.522s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.667 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006092, mean_absolute_error: 0.857077, mean_q: 0.928144\n",
            "  66277/150000: episode: 1531, duration: 0.517s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.500 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004886, mean_absolute_error: 0.856447, mean_q: 0.922974\n",
            "  66297/150000: episode: 1532, duration: 0.402s, episode steps: 20, steps per second: 50, episode reward: 2.000, mean reward: 0.100 [0.000, 1.000], mean action: 10.000 [5.000, 16.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.004304, mean_absolute_error: 0.858794, mean_q: 0.915324\n",
            "  66324/150000: episode: 1533, duration: 0.476s, episode steps: 27, steps per second: 57, episode reward: 2.000, mean reward: 0.074 [0.000, 1.000], mean action: 5.111 [1.000, 13.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.009029, mean_absolute_error: 0.854665, mean_q: 0.911213\n",
            "  66376/150000: episode: 1534, duration: 0.803s, episode steps: 52, steps per second: 65, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 6.577 [0.000, 19.000], mean observation: -0.609 [-1.011, 8.189], loss: 0.004434, mean_absolute_error: 0.860239, mean_q: 0.925400\n",
            "  66388/150000: episode: 1535, duration: 0.272s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.167 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.286], loss: 0.000191, mean_absolute_error: 0.865405, mean_q: 0.930365\n",
            "  66400/150000: episode: 1536, duration: 0.270s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.583 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.255], loss: 0.004508, mean_absolute_error: 0.865351, mean_q: 0.930795\n",
            "  66439/150000: episode: 1537, duration: 0.634s, episode steps: 39, steps per second: 62, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 7.872 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.005367, mean_absolute_error: 0.859704, mean_q: 0.920374\n",
            "  66452/150000: episode: 1538, duration: 0.284s, episode steps: 13, steps per second: 46, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 10.077 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.023], loss: 0.004102, mean_absolute_error: 0.863543, mean_q: 0.934402\n",
            "  66464/150000: episode: 1539, duration: 0.301s, episode steps: 12, steps per second: 40, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.004470, mean_absolute_error: 0.865414, mean_q: 0.934697\n",
            "  66476/150000: episode: 1540, duration: 0.278s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.002271, mean_absolute_error: 0.863941, mean_q: 0.929210\n",
            "  66488/150000: episode: 1541, duration: 0.274s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [0.000, 12.000], mean observation: -0.634 [-1.011, 1.044], loss: 0.005488, mean_absolute_error: 0.864239, mean_q: 0.937115\n",
            "  66500/150000: episode: 1542, duration: 0.270s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.002], loss: 0.010820, mean_absolute_error: 0.862835, mean_q: 0.933611\n",
            "  66512/150000: episode: 1543, duration: 0.288s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.014], loss: 0.005437, mean_absolute_error: 0.863593, mean_q: 0.946554\n",
            "  66524/150000: episode: 1544, duration: 0.275s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.129], loss: 0.011368, mean_absolute_error: 0.865467, mean_q: 0.940363\n",
            "  66536/150000: episode: 1545, duration: 0.276s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.105], loss: 0.003376, mean_absolute_error: 0.856607, mean_q: 0.927157\n",
            "  66548/150000: episode: 1546, duration: 0.276s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.083 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.206], loss: 0.005494, mean_absolute_error: 0.861109, mean_q: 0.937456\n",
            "  66560/150000: episode: 1547, duration: 0.283s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [0.000, 12.000], mean observation: -0.636 [-1.041, 1.008], loss: 0.002190, mean_absolute_error: 0.867379, mean_q: 0.934379\n",
            "  66577/150000: episode: 1548, duration: 0.371s, episode steps: 17, steps per second: 46, episode reward: 0.900, mean reward: 0.053 [0.000, 0.900], mean action: 6.941 [3.000, 12.000], mean observation: -0.630 [-1.011, 2.585], loss: 0.005178, mean_absolute_error: 0.873997, mean_q: 0.942153\n",
            "  66614/150000: episode: 1549, duration: 0.596s, episode steps: 37, steps per second: 62, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 7.378 [5.000, 16.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.007874, mean_absolute_error: 0.872416, mean_q: 0.943220\n",
            "  66637/150000: episode: 1550, duration: 0.427s, episode steps: 23, steps per second: 54, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 13.957 [4.000, 15.000], mean observation: -0.635 [-1.011, 1.018], loss: 0.005176, mean_absolute_error: 0.866451, mean_q: 0.924804\n",
            "  66693/150000: episode: 1551, duration: 0.873s, episode steps: 56, steps per second: 64, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 11.607 [0.000, 15.000], mean observation: -0.609 [-1.017, 8.381], loss: 0.005863, mean_absolute_error: 0.868373, mean_q: 0.933176\n",
            "  66723/150000: episode: 1552, duration: 0.506s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.200 [0.000, 16.000], mean observation: -0.633 [-1.009, 1.015], loss: 0.005334, mean_absolute_error: 0.861490, mean_q: 0.932432\n",
            "  66753/150000: episode: 1553, duration: 0.509s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.500 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005653, mean_absolute_error: 0.866896, mean_q: 0.935775\n",
            "  66783/150000: episode: 1554, duration: 0.498s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.833 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006198, mean_absolute_error: 0.872144, mean_q: 0.933966\n",
            "  67152/150000: episode: 1555, duration: 4.925s, episode steps: 369, steps per second: 75, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 7.531 [0.000, 20.000], mean observation: -0.633 [-1.009, 8.346], loss: 0.006918, mean_absolute_error: 0.862164, mean_q: 0.927637\n",
            "  67161/150000: episode: 1556, duration: 0.245s, episode steps: 9, steps per second: 37, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 12.889 [5.000, 18.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.019092, mean_absolute_error: 0.862138, mean_q: 0.931247\n",
            "  67226/150000: episode: 1557, duration: 0.968s, episode steps: 65, steps per second: 67, episode reward: 2.000, mean reward: 0.031 [0.000, 2.000], mean action: 15.538 [10.000, 20.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.005697, mean_absolute_error: 0.863345, mean_q: 0.931957\n",
            "  67296/150000: episode: 1558, duration: 1.021s, episode steps: 70, steps per second: 69, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 8.343 [3.000, 15.000], mean observation: -0.637 [-1.011, 1.135], loss: 0.008341, mean_absolute_error: 0.858371, mean_q: 0.924406\n",
            "  67441/150000: episode: 1559, duration: 2.053s, episode steps: 145, steps per second: 71, episode reward: 0.300, mean reward: 0.002 [0.000, 0.200], mean action: 7.359 [0.000, 20.000], mean observation: -0.632 [-1.011, 1.067], loss: 0.007632, mean_absolute_error: 0.860800, mean_q: 0.930041\n",
            "  67453/150000: episode: 1560, duration: 0.290s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 7.333 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.305], loss: 0.008551, mean_absolute_error: 0.859240, mean_q: 0.922278\n",
            "  67465/150000: episode: 1561, duration: 0.290s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.000], mean action: 5.583 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.008900, mean_absolute_error: 0.864191, mean_q: 0.934198\n",
            "  67637/150000: episode: 1562, duration: 2.430s, episode steps: 172, steps per second: 71, episode reward: 1.000, mean reward: 0.006 [0.000, 0.900], mean action: 8.308 [0.000, 20.000], mean observation: -0.626 [-1.009, 8.189], loss: 0.006207, mean_absolute_error: 0.863037, mean_q: 0.931241\n",
            "  67741/150000: episode: 1563, duration: 1.465s, episode steps: 104, steps per second: 71, episode reward: 2.000, mean reward: 0.019 [0.000, 1.000], mean action: 13.769 [1.000, 19.000], mean observation: -0.630 [-1.011, 1.002], loss: 0.006326, mean_absolute_error: 0.862841, mean_q: 0.929637\n",
            "  67808/150000: episode: 1564, duration: 1.007s, episode steps: 67, steps per second: 67, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 8.134 [5.000, 20.000], mean observation: -0.611 [-1.009, 8.360], loss: 0.006049, mean_absolute_error: 0.862227, mean_q: 0.931101\n",
            "  67838/150000: episode: 1565, duration: 0.501s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.100 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.004016, mean_absolute_error: 0.859790, mean_q: 0.933599\n",
            "  67922/150000: episode: 1566, duration: 1.267s, episode steps: 84, steps per second: 66, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 6.107 [4.000, 20.000], mean observation: -0.615 [-1.024, 8.299], loss: 0.006641, mean_absolute_error: 0.863963, mean_q: 0.931359\n",
            "  67931/150000: episode: 1567, duration: 0.251s, episode steps: 9, steps per second: 36, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 10.444 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.013073, mean_absolute_error: 0.866157, mean_q: 0.936878\n",
            "  67944/150000: episode: 1568, duration: 0.291s, episode steps: 13, steps per second: 45, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 12.846 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.005], loss: 0.008013, mean_absolute_error: 0.870002, mean_q: 0.942450\n",
            "  67956/150000: episode: 1569, duration: 0.276s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.005], loss: 0.007707, mean_absolute_error: 0.871970, mean_q: 0.951967\n",
            "  67968/150000: episode: 1570, duration: 0.284s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.636 [-1.041, 1.010], loss: 0.005705, mean_absolute_error: 0.867288, mean_q: 0.948022\n",
            "  67978/150000: episode: 1571, duration: 0.253s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 12.300 [5.000, 19.000], mean observation: -0.636 [-1.011, 1.012], loss: 0.005235, mean_absolute_error: 0.866581, mean_q: 0.932986\n",
            "  67988/150000: episode: 1572, duration: 0.251s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 15.300 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.045], loss: 0.011960, mean_absolute_error: 0.871859, mean_q: 0.939536\n",
            "  68066/150000: episode: 1573, duration: 1.156s, episode steps: 78, steps per second: 67, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 11.487 [4.000, 20.000], mean observation: -0.618 [-1.011, 8.159], loss: 0.006620, mean_absolute_error: 0.868242, mean_q: 0.934024\n",
            "  68164/150000: episode: 1574, duration: 1.469s, episode steps: 98, steps per second: 67, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 8.663 [0.000, 20.000], mean observation: -0.632 [-1.009, 1.016], loss: 0.006434, mean_absolute_error: 0.874655, mean_q: 0.940683\n",
            "  68223/150000: episode: 1575, duration: 0.868s, episode steps: 59, steps per second: 68, episode reward: 0.900, mean reward: 0.015 [0.000, 0.900], mean action: 8.644 [3.000, 19.000], mean observation: -0.609 [-1.078, 9.710], loss: 0.005487, mean_absolute_error: 0.872939, mean_q: 0.934265\n",
            "  68236/150000: episode: 1576, duration: 0.301s, episode steps: 13, steps per second: 43, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 12.462 [5.000, 19.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.006072, mean_absolute_error: 0.870300, mean_q: 0.933289\n",
            "  68248/150000: episode: 1577, duration: 0.275s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.917 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.004341, mean_absolute_error: 0.872686, mean_q: 0.956324\n",
            "  68257/150000: episode: 1578, duration: 0.217s, episode steps: 9, steps per second: 41, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 12.222 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.022], loss: 0.007244, mean_absolute_error: 0.873316, mean_q: 0.944754\n",
            "  68287/150000: episode: 1579, duration: 0.520s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 7.433 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007480, mean_absolute_error: 0.875287, mean_q: 0.941056\n",
            "  68299/150000: episode: 1580, duration: 0.283s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 8.917 [3.000, 12.000], mean observation: -0.635 [-1.011, 1.001], loss: 0.010339, mean_absolute_error: 0.874123, mean_q: 0.930590\n",
            "  68360/150000: episode: 1581, duration: 0.971s, episode steps: 61, steps per second: 63, episode reward: 0.900, mean reward: 0.015 [0.000, 0.900], mean action: 4.262 [3.000, 13.000], mean observation: -0.636 [-1.011, 1.016], loss: 0.007543, mean_absolute_error: 0.870330, mean_q: 0.941130\n",
            "  68382/150000: episode: 1582, duration: 0.405s, episode steps: 22, steps per second: 54, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 12.227 [4.000, 19.000], mean observation: -0.635 [-1.011, 1.002], loss: 0.006600, mean_absolute_error: 0.861141, mean_q: 0.920452\n",
            "  68495/150000: episode: 1583, duration: 1.590s, episode steps: 113, steps per second: 71, episode reward: 2.000, mean reward: 0.018 [0.000, 1.000], mean action: 11.000 [2.000, 19.000], mean observation: -0.631 [-1.011, 1.093], loss: 0.006293, mean_absolute_error: 0.873004, mean_q: 0.946034\n",
            "  68556/150000: episode: 1584, duration: 0.904s, episode steps: 61, steps per second: 67, episode reward: 2.000, mean reward: 0.033 [0.000, 1.000], mean action: 5.016 [1.000, 19.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.006559, mean_absolute_error: 0.875904, mean_q: 0.944955\n",
            "  68566/150000: episode: 1585, duration: 0.254s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.200 [1.000, 18.000], mean observation: -0.635 [-1.011, 1.008], loss: 0.011909, mean_absolute_error: 0.878744, mean_q: 0.947095\n",
            "  68674/150000: episode: 1586, duration: 1.494s, episode steps: 108, steps per second: 72, episode reward: 2.000, mean reward: 0.019 [0.000, 1.000], mean action: 12.565 [1.000, 20.000], mean observation: -0.633 [-1.011, 1.387], loss: 0.004776, mean_absolute_error: 0.878515, mean_q: 0.944532\n",
            "  68686/150000: episode: 1587, duration: 0.289s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.167 [2.000, 15.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.005522, mean_absolute_error: 0.878903, mean_q: 0.947460\n",
            "  68696/150000: episode: 1588, duration: 0.281s, episode steps: 10, steps per second: 36, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 3.000 [2.000, 12.000], mean observation: -0.634 [-1.009, 1.003], loss: 0.009252, mean_absolute_error: 0.882045, mean_q: 0.944045\n",
            "  68728/150000: episode: 1589, duration: 0.582s, episode steps: 32, steps per second: 55, episode reward: 0.900, mean reward: 0.028 [0.000, 0.900], mean action: 4.906 [1.000, 18.000], mean observation: -0.631 [-1.011, 3.076], loss: 0.007435, mean_absolute_error: 0.881457, mean_q: 0.946023\n",
            "  68812/150000: episode: 1590, duration: 1.213s, episode steps: 84, steps per second: 69, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 11.548 [1.000, 20.000], mean observation: -0.619 [-1.009, 7.700], loss: 0.006770, mean_absolute_error: 0.880173, mean_q: 0.952203\n",
            "  68910/150000: episode: 1591, duration: 1.395s, episode steps: 98, steps per second: 70, episode reward: 2.000, mean reward: 0.020 [0.000, 1.800], mean action: 10.469 [0.000, 19.000], mean observation: -0.635 [-1.011, 2.271], loss: 0.008258, mean_absolute_error: 0.887230, mean_q: 0.961684\n",
            "  68945/150000: episode: 1592, duration: 0.584s, episode steps: 35, steps per second: 60, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 5.057 [1.000, 14.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.008021, mean_absolute_error: 0.884329, mean_q: 0.959576\n",
            "  68954/150000: episode: 1593, duration: 0.222s, episode steps: 9, steps per second: 40, episode reward: 2.000, mean reward: 0.222 [0.000, 2.000], mean action: 5.778 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.021], loss: 0.006194, mean_absolute_error: 0.883576, mean_q: 0.961096\n",
            "  68984/150000: episode: 1594, duration: 0.500s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.767 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005332, mean_absolute_error: 0.884468, mean_q: 0.957479\n",
            "  69098/150000: episode: 1595, duration: 1.626s, episode steps: 114, steps per second: 70, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 6.956 [0.000, 17.000], mean observation: -0.631 [-1.009, 1.414], loss: 0.006347, mean_absolute_error: 0.882210, mean_q: 0.950489\n",
            "  69148/150000: episode: 1596, duration: 0.799s, episode steps: 50, steps per second: 63, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 14.340 [5.000, 20.000], mean observation: -0.611 [-1.011, 7.668], loss: 0.004529, mean_absolute_error: 0.885949, mean_q: 0.955056\n",
            "  69161/150000: episode: 1597, duration: 0.289s, episode steps: 13, steps per second: 45, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.337], loss: 0.007217, mean_absolute_error: 0.886788, mean_q: 0.951209\n",
            "  69173/150000: episode: 1598, duration: 0.284s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 13.500 [5.000, 20.000], mean observation: -0.635 [-1.011, 1.003], loss: 0.006752, mean_absolute_error: 0.881961, mean_q: 0.940932\n",
            "  69195/150000: episode: 1599, duration: 0.422s, episode steps: 22, steps per second: 52, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 16.000 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.005502, mean_absolute_error: 0.880657, mean_q: 0.948766\n",
            "  69205/150000: episode: 1600, duration: 0.255s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 6.300 [4.000, 12.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.008155, mean_absolute_error: 0.881369, mean_q: 0.956904\n",
            "  69339/150000: episode: 1601, duration: 1.882s, episode steps: 134, steps per second: 71, episode reward: 2.000, mean reward: 0.015 [0.000, 1.000], mean action: 7.910 [1.000, 15.000], mean observation: -0.629 [-1.011, 1.329], loss: 0.006690, mean_absolute_error: 0.878106, mean_q: 0.950731\n",
            "  69522/150000: episode: 1602, duration: 2.554s, episode steps: 183, steps per second: 72, episode reward: 0.900, mean reward: 0.005 [0.000, 0.900], mean action: 6.279 [0.000, 18.000], mean observation: -0.633 [-1.009, 1.307], loss: 0.006106, mean_absolute_error: 0.891785, mean_q: 0.966359\n",
            "  69550/150000: episode: 1603, duration: 0.491s, episode steps: 28, steps per second: 57, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 5.000 [3.000, 13.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.003383, mean_absolute_error: 0.893290, mean_q: 0.967497\n",
            "  69631/150000: episode: 1604, duration: 1.150s, episode steps: 81, steps per second: 70, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 6.864 [0.000, 19.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.007361, mean_absolute_error: 0.893587, mean_q: 0.960028\n",
            "  69645/150000: episode: 1605, duration: 0.307s, episode steps: 14, steps per second: 46, episode reward: 2.000, mean reward: 0.143 [0.000, 1.000], mean action: 10.143 [5.000, 12.000], mean observation: -0.636 [-1.040, 1.011], loss: 0.004280, mean_absolute_error: 0.892450, mean_q: 0.976778\n",
            "  69657/150000: episode: 1606, duration: 0.275s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.003], loss: 0.009969, mean_absolute_error: 0.893995, mean_q: 0.957778\n",
            "  69692/150000: episode: 1607, duration: 0.577s, episode steps: 35, steps per second: 61, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 7.457 [0.000, 15.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.004286, mean_absolute_error: 0.893472, mean_q: 0.959415\n",
            "  69707/150000: episode: 1608, duration: 0.325s, episode steps: 15, steps per second: 46, episode reward: 2.000, mean reward: 0.133 [0.000, 1.100], mean action: 11.267 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.009175, mean_absolute_error: 0.890815, mean_q: 0.958221\n",
            "  69737/150000: episode: 1609, duration: 0.523s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.400 [0.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.009771, mean_absolute_error: 0.891092, mean_q: 0.974689\n",
            "  69962/150000: episode: 1610, duration: 3.069s, episode steps: 225, steps per second: 73, episode reward: 0.900, mean reward: 0.004 [0.000, 0.900], mean action: 9.364 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.386], loss: 0.007209, mean_absolute_error: 0.894863, mean_q: 0.968314\n",
            "  69974/150000: episode: 1611, duration: 0.283s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.833 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.496], loss: 0.009989, mean_absolute_error: 0.899663, mean_q: 0.963999\n",
            "  69986/150000: episode: 1612, duration: 0.287s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 7.000 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.003368, mean_absolute_error: 0.899368, mean_q: 0.965120\n",
            "  70022/150000: episode: 1613, duration: 0.644s, episode steps: 36, steps per second: 56, episode reward: 1.000, mean reward: 0.028 [0.000, 0.900], mean action: 5.361 [2.000, 19.000], mean observation: -0.633 [-1.009, 1.014], loss: 0.004931, mean_absolute_error: 0.892392, mean_q: 0.964759\n",
            "  70066/150000: episode: 1614, duration: 0.747s, episode steps: 44, steps per second: 59, episode reward: 1.000, mean reward: 0.023 [0.000, 0.900], mean action: 4.568 [0.000, 17.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.007134, mean_absolute_error: 0.900889, mean_q: 0.977336\n",
            "  70124/150000: episode: 1615, duration: 0.939s, episode steps: 58, steps per second: 62, episode reward: 1.000, mean reward: 0.017 [0.000, 0.900], mean action: 4.293 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.006201, mean_absolute_error: 0.912244, mean_q: 0.982543\n",
            "  70166/150000: episode: 1616, duration: 0.710s, episode steps: 42, steps per second: 59, episode reward: 1.000, mean reward: 0.024 [0.000, 0.900], mean action: 5.310 [4.000, 20.000], mean observation: -0.633 [-1.009, 1.010], loss: 0.005651, mean_absolute_error: 0.915256, mean_q: 0.978099\n",
            "  70183/150000: episode: 1617, duration: 0.397s, episode steps: 17, steps per second: 43, episode reward: 1.000, mean reward: 0.059 [0.000, 0.900], mean action: 6.118 [0.000, 13.000], mean observation: -0.635 [-1.011, 1.015], loss: 0.005553, mean_absolute_error: 0.912275, mean_q: 0.976961\n",
            "  70205/150000: episode: 1618, duration: 0.412s, episode steps: 22, steps per second: 53, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 8.955 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.017], loss: 0.008015, mean_absolute_error: 0.911949, mean_q: 0.976702\n",
            "  70313/150000: episode: 1619, duration: 1.549s, episode steps: 108, steps per second: 70, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 7.167 [1.000, 20.000], mean observation: -0.633 [-1.009, 1.000], loss: 0.005726, mean_absolute_error: 0.910751, mean_q: 0.981164\n",
            "  70338/150000: episode: 1620, duration: 0.440s, episode steps: 25, steps per second: 57, episode reward: 2.000, mean reward: 0.080 [0.000, 1.000], mean action: 5.320 [5.000, 13.000], mean observation: -0.634 [-1.011, 1.015], loss: 0.008592, mean_absolute_error: 0.911372, mean_q: 0.991448\n",
            "  70374/150000: episode: 1621, duration: 0.627s, episode steps: 36, steps per second: 57, episode reward: 1.000, mean reward: 0.028 [0.000, 0.900], mean action: 4.917 [0.000, 6.000], mean observation: -0.632 [-1.011, 1.014], loss: 0.006763, mean_absolute_error: 0.912661, mean_q: 0.977408\n",
            "  70522/150000: episode: 1622, duration: 2.108s, episode steps: 148, steps per second: 70, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 8.088 [0.000, 20.000], mean observation: -0.612 [-1.009, 13.276], loss: 0.005689, mean_absolute_error: 0.907887, mean_q: 0.971150\n",
            "  70599/150000: episode: 1623, duration: 1.091s, episode steps: 77, steps per second: 71, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 3.649 [1.000, 10.000], mean observation: -0.620 [-1.009, 8.189], loss: 0.004695, mean_absolute_error: 0.900463, mean_q: 0.967441\n",
            "  70684/150000: episode: 1624, duration: 1.222s, episode steps: 85, steps per second: 70, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 7.294 [0.000, 19.000], mean observation: -0.635 [-1.011, 1.300], loss: 0.006315, mean_absolute_error: 0.893837, mean_q: 0.961398\n",
            "  70793/150000: episode: 1625, duration: 1.614s, episode steps: 109, steps per second: 68, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 8.917 [0.000, 17.000], mean observation: -0.630 [-1.009, 1.278], loss: 0.007033, mean_absolute_error: 0.894329, mean_q: 0.974078\n",
            "  70805/150000: episode: 1626, duration: 0.293s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 8.750 [1.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.010020, mean_absolute_error: 0.900904, mean_q: 0.976365\n",
            "  70914/150000: episode: 1627, duration: 1.569s, episode steps: 109, steps per second: 69, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 8.404 [1.000, 20.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.005572, mean_absolute_error: 0.896981, mean_q: 0.964449\n",
            "  70928/150000: episode: 1628, duration: 0.326s, episode steps: 14, steps per second: 43, episode reward: 2.000, mean reward: 0.143 [0.000, 1.000], mean action: 6.929 [0.000, 12.000], mean observation: -0.636 [-1.041, 1.018], loss: 0.003834, mean_absolute_error: 0.893547, mean_q: 0.957896\n",
            "  70965/150000: episode: 1629, duration: 0.613s, episode steps: 37, steps per second: 60, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.432 [2.000, 13.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.006585, mean_absolute_error: 0.895842, mean_q: 0.970140\n",
            "  71015/150000: episode: 1630, duration: 0.773s, episode steps: 50, steps per second: 65, episode reward: 2.000, mean reward: 0.040 [0.000, 1.000], mean action: 7.320 [5.000, 14.000], mean observation: -0.632 [-1.011, 1.014], loss: 0.006481, mean_absolute_error: 0.895492, mean_q: 0.964523\n",
            "  71055/150000: episode: 1631, duration: 0.650s, episode steps: 40, steps per second: 62, episode reward: 2.000, mean reward: 0.050 [0.000, 1.000], mean action: 5.750 [1.000, 19.000], mean observation: -0.633 [-1.011, 1.009], loss: 0.004637, mean_absolute_error: 0.898054, mean_q: 0.962414\n",
            "  71147/150000: episode: 1632, duration: 1.331s, episode steps: 92, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.261 [1.000, 20.000], mean observation: -0.631 [-1.011, 1.345], loss: 0.004746, mean_absolute_error: 0.901100, mean_q: 0.969562\n",
            "  71277/150000: episode: 1633, duration: 1.826s, episode steps: 130, steps per second: 71, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 7.792 [0.000, 16.000], mean observation: -0.633 [-1.011, 1.030], loss: 0.007279, mean_absolute_error: 0.896787, mean_q: 0.961712\n",
            "  71417/150000: episode: 1634, duration: 1.938s, episode steps: 140, steps per second: 72, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 8.971 [0.000, 18.000], mean observation: -0.630 [-1.011, 1.000], loss: 0.007678, mean_absolute_error: 0.893378, mean_q: 0.963654\n",
            "  71520/150000: episode: 1635, duration: 1.482s, episode steps: 103, steps per second: 70, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 8.068 [3.000, 20.000], mean observation: -0.632 [-1.009, 1.014], loss: 0.006332, mean_absolute_error: 0.894194, mean_q: 0.964498\n",
            "  71561/150000: episode: 1636, duration: 0.720s, episode steps: 41, steps per second: 57, episode reward: 1.000, mean reward: 0.024 [0.000, 0.900], mean action: 4.366 [4.000, 18.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.005301, mean_absolute_error: 0.893802, mean_q: 0.961826\n",
            "  71594/150000: episode: 1637, duration: 0.544s, episode steps: 33, steps per second: 61, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 7.333 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.007529, mean_absolute_error: 0.895275, mean_q: 0.962923\n",
            "  71606/150000: episode: 1638, duration: 0.285s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.083 [2.000, 15.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.008308, mean_absolute_error: 0.888990, mean_q: 0.947231\n",
            "  71636/150000: episode: 1639, duration: 0.506s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 8.467 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010062, mean_absolute_error: 0.890400, mean_q: 0.953532\n",
            "  71732/150000: episode: 1640, duration: 1.367s, episode steps: 96, steps per second: 70, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 5.938 [0.000, 19.000], mean observation: -0.631 [-1.011, 1.418], loss: 0.008057, mean_absolute_error: 0.883498, mean_q: 0.956045\n",
            "  71782/150000: episode: 1641, duration: 0.813s, episode steps: 50, steps per second: 61, episode reward: 1.000, mean reward: 0.020 [0.000, 0.900], mean action: 3.720 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.006450, mean_absolute_error: 0.882216, mean_q: 0.950132\n",
            "  71855/150000: episode: 1642, duration: 1.082s, episode steps: 73, steps per second: 67, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 9.370 [5.000, 20.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.005795, mean_absolute_error: 0.882400, mean_q: 0.953145\n",
            "  71928/150000: episode: 1643, duration: 1.062s, episode steps: 73, steps per second: 69, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 10.164 [2.000, 20.000], mean observation: -0.635 [-1.009, 1.129], loss: 0.006508, mean_absolute_error: 0.886004, mean_q: 0.948577\n",
            "  71965/150000: episode: 1644, duration: 0.645s, episode steps: 37, steps per second: 57, episode reward: 1.000, mean reward: 0.027 [0.000, 0.900], mean action: 16.351 [4.000, 20.000], mean observation: -0.633 [-1.009, 1.019], loss: 0.007638, mean_absolute_error: 0.877120, mean_q: 0.943025\n",
            "  71974/150000: episode: 1645, duration: 0.219s, episode steps: 9, steps per second: 41, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 12.222 [5.000, 18.000], mean observation: -0.636 [-1.011, 1.021], loss: 0.011435, mean_absolute_error: 0.877972, mean_q: 0.945734\n",
            "  72007/150000: episode: 1646, duration: 0.557s, episode steps: 33, steps per second: 59, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 7.545 [1.000, 16.000], mean observation: -0.633 [-1.011, 1.004], loss: 0.006064, mean_absolute_error: 0.879306, mean_q: 0.957340\n",
            "  72107/150000: episode: 1647, duration: 1.412s, episode steps: 100, steps per second: 71, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 10.080 [0.000, 20.000], mean observation: -0.631 [-1.011, 1.000], loss: 0.005735, mean_absolute_error: 0.880541, mean_q: 0.946669\n",
            "  72198/150000: episode: 1648, duration: 1.295s, episode steps: 91, steps per second: 70, episode reward: 2.000, mean reward: 0.022 [0.000, 1.000], mean action: 5.527 [3.000, 20.000], mean observation: -0.632 [-1.011, 1.116], loss: 0.006476, mean_absolute_error: 0.875634, mean_q: 0.946642\n",
            "  72228/150000: episode: 1649, duration: 0.532s, episode steps: 30, steps per second: 56, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 3.033 [0.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008269, mean_absolute_error: 0.873344, mean_q: 0.946199\n",
            "  72326/150000: episode: 1650, duration: 1.401s, episode steps: 98, steps per second: 70, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 11.724 [0.000, 19.000], mean observation: -0.636 [-1.011, 1.115], loss: 0.007178, mean_absolute_error: 0.878205, mean_q: 0.958742\n",
            "  72523/150000: episode: 1651, duration: 2.881s, episode steps: 197, steps per second: 68, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 9.553 [1.000, 19.000], mean observation: -0.636 [-1.011, 3.027], loss: 0.006733, mean_absolute_error: 0.884364, mean_q: 0.956252\n",
            "  72553/150000: episode: 1652, duration: 0.501s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004874, mean_absolute_error: 0.884016, mean_q: 0.964428\n",
            "  72639/150000: episode: 1653, duration: 1.236s, episode steps: 86, steps per second: 70, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 7.872 [1.000, 20.000], mean observation: -0.632 [-1.009, 1.243], loss: 0.007118, mean_absolute_error: 0.892083, mean_q: 0.965701\n",
            "  72667/150000: episode: 1654, duration: 0.482s, episode steps: 28, steps per second: 58, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 6.214 [4.000, 13.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.004816, mean_absolute_error: 0.893942, mean_q: 0.961970\n",
            "  73067/150000: episode: 1655, duration: 5.337s, episode steps: 400, steps per second: 75, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 9.150 [1.000, 20.000], mean observation: -0.636 [-1.011, 8.071], loss: 0.006708, mean_absolute_error: 0.893221, mean_q: 0.964256\n",
            "  73091/150000: episode: 1656, duration: 0.439s, episode steps: 24, steps per second: 55, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 12.958 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.011], loss: 0.005994, mean_absolute_error: 0.896499, mean_q: 0.961904\n",
            "  73266/150000: episode: 1657, duration: 2.393s, episode steps: 175, steps per second: 73, episode reward: 1.000, mean reward: 0.006 [0.000, 0.900], mean action: 5.709 [0.000, 20.000], mean observation: -0.631 [-1.009, 8.234], loss: 0.006966, mean_absolute_error: 0.892787, mean_q: 0.963076\n",
            "  73330/150000: episode: 1658, duration: 0.989s, episode steps: 64, steps per second: 65, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 5.625 [0.000, 13.000], mean observation: -0.615 [-1.021, 8.240], loss: 0.006250, mean_absolute_error: 0.894233, mean_q: 0.960239\n",
            "  73372/150000: episode: 1659, duration: 0.697s, episode steps: 42, steps per second: 60, episode reward: 1.000, mean reward: 0.024 [0.000, 0.900], mean action: 5.024 [1.000, 17.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.006436, mean_absolute_error: 0.884073, mean_q: 0.956379\n",
            "  73408/150000: episode: 1660, duration: 0.569s, episode steps: 36, steps per second: 63, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 6.028 [1.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006304, mean_absolute_error: 0.888853, mean_q: 0.957926\n",
            "  73425/150000: episode: 1661, duration: 0.344s, episode steps: 17, steps per second: 49, episode reward: 2.000, mean reward: 0.118 [0.000, 1.100], mean action: 6.000 [3.000, 17.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.005259, mean_absolute_error: 0.890137, mean_q: 0.963500\n",
            "  73608/150000: episode: 1662, duration: 2.488s, episode steps: 183, steps per second: 74, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 4.634 [0.000, 20.000], mean observation: -0.629 [-1.011, 1.274], loss: 0.006053, mean_absolute_error: 0.886692, mean_q: 0.954618\n",
            "  73695/150000: episode: 1663, duration: 1.236s, episode steps: 87, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 3.517 [0.000, 20.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.006250, mean_absolute_error: 0.890913, mean_q: 0.964067\n",
            "  73724/150000: episode: 1664, duration: 0.529s, episode steps: 29, steps per second: 55, episode reward: 2.000, mean reward: 0.069 [0.000, 1.000], mean action: 5.241 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.006], loss: 0.005675, mean_absolute_error: 0.890105, mean_q: 0.958022\n",
            "  73748/150000: episode: 1665, duration: 0.426s, episode steps: 24, steps per second: 56, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 6.375 [5.000, 18.000], mean observation: -0.635 [-1.011, 1.009], loss: 0.005967, mean_absolute_error: 0.895058, mean_q: 0.962002\n",
            "  73833/150000: episode: 1666, duration: 1.213s, episode steps: 85, steps per second: 70, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 7.247 [0.000, 19.000], mean observation: -0.633 [-1.009, 1.000], loss: 0.005009, mean_absolute_error: 0.900879, mean_q: 0.964220\n",
            "  73846/150000: episode: 1667, duration: 0.298s, episode steps: 13, steps per second: 44, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.692 [5.000, 15.000], mean observation: -0.635 [-1.011, 1.009], loss: 0.007430, mean_absolute_error: 0.894936, mean_q: 0.952581\n",
            "  73858/150000: episode: 1668, duration: 0.276s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.005], loss: 0.004488, mean_absolute_error: 0.891458, mean_q: 0.953937\n",
            "  73868/150000: episode: 1669, duration: 0.252s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 6.500 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.004131, mean_absolute_error: 0.891767, mean_q: 0.951468\n",
            "  73898/150000: episode: 1670, duration: 0.505s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.600 [4.000, 16.000], mean observation: -0.634 [-1.011, 1.015], loss: 0.006083, mean_absolute_error: 0.895432, mean_q: 0.968498\n",
            "  74024/150000: episode: 1671, duration: 1.741s, episode steps: 126, steps per second: 72, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 5.214 [0.000, 18.000], mean observation: -0.633 [-1.009, 1.252], loss: 0.006440, mean_absolute_error: 0.896739, mean_q: 0.964611\n",
            "  74047/150000: episode: 1672, duration: 0.437s, episode steps: 23, steps per second: 53, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 8.304 [0.000, 18.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.003777, mean_absolute_error: 0.899686, mean_q: 0.965942\n",
            "  74082/150000: episode: 1673, duration: 0.562s, episode steps: 35, steps per second: 62, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 5.343 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.006956, mean_absolute_error: 0.897125, mean_q: 0.966476\n",
            "  74168/150000: episode: 1674, duration: 1.221s, episode steps: 86, steps per second: 70, episode reward: 2.000, mean reward: 0.023 [0.000, 1.000], mean action: 13.558 [0.000, 16.000], mean observation: -0.630 [-1.011, 1.008], loss: 0.007499, mean_absolute_error: 0.891175, mean_q: 0.964977\n",
            "  74217/150000: episode: 1675, duration: 0.744s, episode steps: 49, steps per second: 66, episode reward: 2.000, mean reward: 0.041 [0.000, 1.000], mean action: 6.327 [1.000, 19.000], mean observation: -0.632 [-1.011, 1.012], loss: 0.008674, mean_absolute_error: 0.898815, mean_q: 0.969436\n",
            "  74247/150000: episode: 1676, duration: 0.512s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.800 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008471, mean_absolute_error: 0.901892, mean_q: 0.975839\n",
            "  74273/150000: episode: 1677, duration: 0.449s, episode steps: 26, steps per second: 58, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 5.692 [3.000, 17.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.007874, mean_absolute_error: 0.903788, mean_q: 0.973433\n",
            "  74320/150000: episode: 1678, duration: 0.761s, episode steps: 47, steps per second: 62, episode reward: 1.000, mean reward: 0.021 [0.000, 0.900], mean action: 5.766 [1.000, 20.000], mean observation: -0.631 [-1.011, 2.770], loss: 0.006397, mean_absolute_error: 0.907237, mean_q: 0.973391\n",
            "  74350/150000: episode: 1679, duration: 0.501s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.367 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006506, mean_absolute_error: 0.904441, mean_q: 0.964718\n",
            "  74388/150000: episode: 1680, duration: 0.626s, episode steps: 38, steps per second: 61, episode reward: 2.000, mean reward: 0.053 [0.000, 1.000], mean action: 8.579 [3.000, 19.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.005501, mean_absolute_error: 0.900848, mean_q: 0.963168\n",
            "  74418/150000: episode: 1681, duration: 0.529s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 13.300 [0.000, 15.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.005169, mean_absolute_error: 0.898086, mean_q: 0.966364\n",
            "  74432/150000: episode: 1682, duration: 0.287s, episode steps: 14, steps per second: 49, episode reward: 2.000, mean reward: 0.143 [0.000, 2.000], mean action: 11.929 [8.000, 15.000], mean observation: -0.635 [-1.011, 1.021], loss: 0.010554, mean_absolute_error: 0.899701, mean_q: 0.979673\n",
            "  74444/150000: episode: 1683, duration: 0.267s, episode steps: 12, steps per second: 45, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.005], loss: 0.003613, mean_absolute_error: 0.894461, mean_q: 0.958795\n",
            "  74456/150000: episode: 1684, duration: 0.276s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 8.500 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.008821, mean_absolute_error: 0.891660, mean_q: 0.951029\n",
            "  74469/150000: episode: 1685, duration: 0.288s, episode steps: 13, steps per second: 45, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 5.923 [5.000, 12.000], mean observation: -0.636 [-1.039, 1.000], loss: 0.004945, mean_absolute_error: 0.894480, mean_q: 0.961600\n",
            "  74592/150000: episode: 1686, duration: 1.747s, episode steps: 123, steps per second: 70, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 7.902 [0.000, 20.000], mean observation: -0.635 [-1.009, 1.264], loss: 0.006107, mean_absolute_error: 0.891731, mean_q: 0.958983\n",
            "  74694/150000: episode: 1687, duration: 1.449s, episode steps: 102, steps per second: 70, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 6.588 [1.000, 20.000], mean observation: -0.636 [-1.009, 1.000], loss: 0.005566, mean_absolute_error: 0.897020, mean_q: 0.965938\n",
            "  74724/150000: episode: 1688, duration: 0.500s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.833 [0.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007052, mean_absolute_error: 0.888188, mean_q: 0.959538\n",
            "  74754/150000: episode: 1689, duration: 0.528s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006255, mean_absolute_error: 0.892036, mean_q: 0.964235\n",
            "  74806/150000: episode: 1690, duration: 0.786s, episode steps: 52, steps per second: 66, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 7.308 [0.000, 16.000], mean observation: -0.610 [-1.011, 8.170], loss: 0.006951, mean_absolute_error: 0.894998, mean_q: 0.958576\n",
            "  74876/150000: episode: 1691, duration: 1.011s, episode steps: 70, steps per second: 69, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 8.900 [1.000, 16.000], mean observation: -0.609 [-1.072, 9.576], loss: 0.007243, mean_absolute_error: 0.890521, mean_q: 0.952234\n",
            "  75133/150000: episode: 1692, duration: 3.498s, episode steps: 257, steps per second: 73, episode reward: 1.000, mean reward: 0.004 [0.000, 0.900], mean action: 6.128 [0.000, 20.000], mean observation: -0.637 [-1.011, 1.004], loss: 0.006560, mean_absolute_error: 0.885926, mean_q: 0.955899\n",
            "  75241/150000: episode: 1693, duration: 1.536s, episode steps: 108, steps per second: 70, episode reward: 2.000, mean reward: 0.019 [0.000, 1.400], mean action: 8.907 [1.000, 20.000], mean observation: -0.635 [-1.009, 1.018], loss: 0.006231, mean_absolute_error: 0.885684, mean_q: 0.955492\n",
            "  75271/150000: episode: 1694, duration: 0.520s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 12.267 [4.000, 18.000], mean observation: -0.634 [-1.011, 1.015], loss: 0.006412, mean_absolute_error: 0.886467, mean_q: 0.960992\n",
            "  75374/150000: episode: 1695, duration: 1.498s, episode steps: 103, steps per second: 69, episode reward: 2.000, mean reward: 0.019 [0.000, 1.000], mean action: 7.602 [0.000, 17.000], mean observation: -0.618 [-1.042, 8.358], loss: 0.006327, mean_absolute_error: 0.891002, mean_q: 0.960095\n",
            "  75547/150000: episode: 1696, duration: 2.440s, episode steps: 173, steps per second: 71, episode reward: 2.000, mean reward: 0.012 [0.000, 1.100], mean action: 6.884 [0.000, 19.000], mean observation: -0.631 [-1.011, 1.014], loss: 0.006554, mean_absolute_error: 0.890586, mean_q: 0.959292\n",
            "  75575/150000: episode: 1697, duration: 0.481s, episode steps: 28, steps per second: 58, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 5.571 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.007467, mean_absolute_error: 0.893448, mean_q: 0.968032\n",
            "  75614/150000: episode: 1698, duration: 0.635s, episode steps: 39, steps per second: 61, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 7.564 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.008074, mean_absolute_error: 0.899327, mean_q: 0.974890\n",
            "  75830/150000: episode: 1699, duration: 2.960s, episode steps: 216, steps per second: 73, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 9.662 [0.000, 20.000], mean observation: -0.629 [-1.009, 9.478], loss: 0.007371, mean_absolute_error: 0.898881, mean_q: 0.969010\n",
            "  75844/150000: episode: 1700, duration: 0.310s, episode steps: 14, steps per second: 45, episode reward: 2.000, mean reward: 0.143 [0.000, 1.000], mean action: 15.786 [5.000, 17.000], mean observation: -0.636 [-1.011, 1.002], loss: 0.008971, mean_absolute_error: 0.897474, mean_q: 0.961695\n",
            "  75856/150000: episode: 1701, duration: 0.277s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 13.000], mean observation: -0.636 [-1.040, 1.011], loss: 0.010301, mean_absolute_error: 0.894125, mean_q: 0.952452\n",
            "  75868/150000: episode: 1702, duration: 0.286s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.917 [5.000, 14.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.004580, mean_absolute_error: 0.896981, mean_q: 0.968392\n",
            "  75925/150000: episode: 1703, duration: 0.864s, episode steps: 57, steps per second: 66, episode reward: 0.900, mean reward: 0.016 [0.000, 0.900], mean action: 10.877 [2.000, 16.000], mean observation: -0.608 [-1.080, 9.598], loss: 0.005428, mean_absolute_error: 0.894226, mean_q: 0.960390\n",
            "  75980/150000: episode: 1704, duration: 0.840s, episode steps: 55, steps per second: 65, episode reward: 2.000, mean reward: 0.036 [0.000, 1.000], mean action: 5.000 [1.000, 8.000], mean observation: -0.632 [-1.011, 1.012], loss: 0.006578, mean_absolute_error: 0.893927, mean_q: 0.962371\n",
            "  76016/150000: episode: 1705, duration: 0.633s, episode steps: 36, steps per second: 57, episode reward: 1.000, mean reward: 0.028 [0.000, 0.900], mean action: 5.750 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.008131, mean_absolute_error: 0.893984, mean_q: 0.959578\n",
            "  76029/150000: episode: 1706, duration: 0.288s, episode steps: 13, steps per second: 45, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 5.538 [5.000, 12.000], mean observation: -0.636 [-1.040, 1.000], loss: 0.002957, mean_absolute_error: 0.900311, mean_q: 0.976250\n",
            "  76062/150000: episode: 1707, duration: 0.545s, episode steps: 33, steps per second: 61, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 10.424 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.007586, mean_absolute_error: 0.897123, mean_q: 0.968180\n",
            "  76117/150000: episode: 1708, duration: 0.853s, episode steps: 55, steps per second: 64, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 5.655 [5.000, 19.000], mean observation: -0.611 [-1.011, 8.189], loss: 0.006851, mean_absolute_error: 0.900062, mean_q: 0.975332\n",
            "  76188/150000: episode: 1709, duration: 1.077s, episode steps: 71, steps per second: 66, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 5.099 [0.000, 20.000], mean observation: -0.613 [-1.015, 8.397], loss: 0.005463, mean_absolute_error: 0.906016, mean_q: 0.974544\n",
            "  76326/150000: episode: 1710, duration: 1.908s, episode steps: 138, steps per second: 72, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 12.022 [0.000, 17.000], mean observation: -0.631 [-1.011, 1.275], loss: 0.007476, mean_absolute_error: 0.901006, mean_q: 0.970984\n",
            "  76428/150000: episode: 1711, duration: 1.437s, episode steps: 102, steps per second: 71, episode reward: 2.000, mean reward: 0.020 [0.000, 1.000], mean action: 10.216 [1.000, 19.000], mean observation: -0.631 [-1.011, 1.007], loss: 0.005048, mean_absolute_error: 0.904368, mean_q: 0.976957\n",
            "  76467/150000: episode: 1712, duration: 0.613s, episode steps: 39, steps per second: 64, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 6.897 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.005898, mean_absolute_error: 0.910722, mean_q: 0.979491\n",
            "  76578/150000: episode: 1713, duration: 1.614s, episode steps: 111, steps per second: 69, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 10.432 [4.000, 19.000], mean observation: -0.638 [-1.011, 1.963], loss: 0.005430, mean_absolute_error: 0.900101, mean_q: 0.963619\n",
            "  76839/150000: episode: 1714, duration: 3.521s, episode steps: 261, steps per second: 74, episode reward: 1.000, mean reward: 0.004 [0.000, 0.900], mean action: 7.870 [0.000, 20.000], mean observation: -0.635 [-1.009, 2.620], loss: 0.007018, mean_absolute_error: 0.903124, mean_q: 0.974262\n",
            "  76956/150000: episode: 1715, duration: 1.669s, episode steps: 117, steps per second: 70, episode reward: 2.000, mean reward: 0.017 [0.000, 1.000], mean action: 11.068 [0.000, 20.000], mean observation: -0.631 [-1.011, 1.099], loss: 0.004951, mean_absolute_error: 0.895133, mean_q: 0.962142\n",
            "  76986/150000: episode: 1716, duration: 0.504s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.400 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008649, mean_absolute_error: 0.885339, mean_q: 0.962678\n",
            "  77004/150000: episode: 1717, duration: 0.361s, episode steps: 18, steps per second: 50, episode reward: 2.000, mean reward: 0.111 [0.000, 1.000], mean action: 9.444 [5.000, 14.000], mean observation: -0.635 [-1.011, 1.014], loss: 0.008887, mean_absolute_error: 0.890875, mean_q: 0.957915\n",
            "  77016/150000: episode: 1718, duration: 0.271s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.917 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.006698, mean_absolute_error: 0.893735, mean_q: 0.963684\n",
            "  77040/150000: episode: 1719, duration: 0.429s, episode steps: 24, steps per second: 56, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 8.375 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.006325, mean_absolute_error: 0.894951, mean_q: 0.964126\n",
            "  77057/150000: episode: 1720, duration: 0.345s, episode steps: 17, steps per second: 49, episode reward: 2.000, mean reward: 0.118 [0.000, 1.100], mean action: 7.647 [1.000, 16.000], mean observation: -0.635 [-1.011, 1.013], loss: 0.003991, mean_absolute_error: 0.896022, mean_q: 0.964107\n",
            "  77072/150000: episode: 1721, duration: 0.316s, episode steps: 15, steps per second: 47, episode reward: 2.000, mean reward: 0.133 [0.000, 2.000], mean action: 12.467 [11.000, 20.000], mean observation: -0.631 [-1.040, 2.127], loss: 0.007403, mean_absolute_error: 0.895058, mean_q: 0.968258\n",
            "  77084/150000: episode: 1722, duration: 0.287s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.583 [2.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.002266, mean_absolute_error: 0.893568, mean_q: 0.970811\n",
            "  77096/150000: episode: 1723, duration: 0.277s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.500 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.121], loss: 0.010347, mean_absolute_error: 0.898779, mean_q: 0.977939\n",
            "  77108/150000: episode: 1724, duration: 0.283s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.833 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.004], loss: 0.012605, mean_absolute_error: 0.903710, mean_q: 0.979448\n",
            "  77120/150000: episode: 1725, duration: 0.277s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.750 [4.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.002504, mean_absolute_error: 0.900831, mean_q: 0.967059\n",
            "  77132/150000: episode: 1726, duration: 0.272s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.481], loss: 0.008083, mean_absolute_error: 0.901496, mean_q: 0.960395\n",
            "  77142/150000: episode: 1727, duration: 0.264s, episode steps: 10, steps per second: 38, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.000 [5.000, 12.000], mean observation: -0.631 [-1.047, 1.781], loss: 0.006862, mean_absolute_error: 0.902405, mean_q: 0.969043\n",
            "  77151/150000: episode: 1728, duration: 0.233s, episode steps: 9, steps per second: 39, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 10.222 [5.000, 17.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.002910, mean_absolute_error: 0.903564, mean_q: 0.968392\n",
            "  77185/150000: episode: 1729, duration: 0.603s, episode steps: 34, steps per second: 56, episode reward: 1.000, mean reward: 0.029 [0.000, 0.900], mean action: 7.647 [5.000, 18.000], mean observation: -0.633 [-1.009, 1.019], loss: 0.006782, mean_absolute_error: 0.902996, mean_q: 0.966478\n",
            "  77224/150000: episode: 1730, duration: 0.645s, episode steps: 39, steps per second: 60, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 5.846 [4.000, 17.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.005622, mean_absolute_error: 0.897569, mean_q: 0.963895\n",
            "  77624/150000: episode: 1731, duration: 5.429s, episode steps: 400, steps per second: 74, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 7.688 [0.000, 20.000], mean observation: -0.633 [-1.030, 9.669], loss: 0.006704, mean_absolute_error: 0.900451, mean_q: 0.973334\n",
            "  77699/150000: episode: 1732, duration: 1.099s, episode steps: 75, steps per second: 68, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 8.853 [2.000, 14.000], mean observation: -0.634 [-1.011, 1.216], loss: 0.006132, mean_absolute_error: 0.905196, mean_q: 0.974896\n",
            "  77807/150000: episode: 1733, duration: 1.519s, episode steps: 108, steps per second: 71, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 5.907 [3.000, 20.000], mean observation: -0.636 [-1.009, 1.013], loss: 0.008652, mean_absolute_error: 0.898392, mean_q: 0.971464\n",
            "  77900/150000: episode: 1734, duration: 1.329s, episode steps: 93, steps per second: 70, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 7.258 [1.000, 18.000], mean observation: -0.635 [-1.009, 1.000], loss: 0.007673, mean_absolute_error: 0.902873, mean_q: 0.974362\n",
            "  77941/150000: episode: 1735, duration: 0.707s, episode steps: 41, steps per second: 58, episode reward: 1.000, mean reward: 0.024 [0.000, 0.900], mean action: 8.439 [3.000, 19.000], mean observation: -0.633 [-1.009, 1.014], loss: 0.005712, mean_absolute_error: 0.906970, mean_q: 0.974671\n",
            "  78125/150000: episode: 1736, duration: 2.530s, episode steps: 184, steps per second: 73, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 6.120 [3.000, 20.000], mean observation: -0.635 [-1.011, 1.047], loss: 0.006958, mean_absolute_error: 0.909229, mean_q: 0.981572\n",
            "  78138/150000: episode: 1737, duration: 0.303s, episode steps: 13, steps per second: 43, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 9.846 [5.000, 19.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.004191, mean_absolute_error: 0.912630, mean_q: 0.974223\n",
            "  78168/150000: episode: 1738, duration: 0.493s, episode steps: 30, steps per second: 61, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.333 [5.000, 15.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.003996, mean_absolute_error: 0.914353, mean_q: 0.983073\n",
            "  78199/150000: episode: 1739, duration: 0.516s, episode steps: 31, steps per second: 60, episode reward: 2.000, mean reward: 0.065 [0.000, 1.000], mean action: 6.032 [0.000, 20.000], mean observation: -0.634 [-1.009, 1.015], loss: 0.006044, mean_absolute_error: 0.913502, mean_q: 0.983476\n",
            "  78278/150000: episode: 1740, duration: 1.157s, episode steps: 79, steps per second: 68, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 3.392 [0.000, 18.000], mean observation: -0.635 [-1.009, 1.000], loss: 0.004751, mean_absolute_error: 0.916113, mean_q: 0.990768\n",
            "  78297/150000: episode: 1741, duration: 0.394s, episode steps: 19, steps per second: 48, episode reward: 0.900, mean reward: 0.047 [0.000, 0.900], mean action: 1.789 [0.000, 16.000], mean observation: -0.633 [-1.011, 1.290], loss: 0.006721, mean_absolute_error: 0.919872, mean_q: 1.003943\n",
            "  78408/150000: episode: 1742, duration: 1.576s, episode steps: 111, steps per second: 70, episode reward: 2.000, mean reward: 0.018 [0.000, 1.000], mean action: 9.261 [0.000, 17.000], mean observation: -0.630 [-1.011, 1.189], loss: 0.007914, mean_absolute_error: 0.922034, mean_q: 0.995778\n",
            "  78420/150000: episode: 1743, duration: 0.277s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.006530, mean_absolute_error: 0.923964, mean_q: 0.999332\n",
            "  78450/150000: episode: 1744, duration: 0.496s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.500 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008832, mean_absolute_error: 0.917607, mean_q: 1.001821\n",
            "  78540/150000: episode: 1745, duration: 1.307s, episode steps: 90, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 7.822 [3.000, 20.000], mean observation: -0.631 [-1.011, 1.037], loss: 0.006490, mean_absolute_error: 0.926845, mean_q: 1.011415\n",
            "  78595/150000: episode: 1746, duration: 0.851s, episode steps: 55, steps per second: 65, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 3.855 [0.000, 19.000], mean observation: -0.634 [-1.009, 1.010], loss: 0.006105, mean_absolute_error: 0.934439, mean_q: 1.003979\n",
            "  78664/150000: episode: 1747, duration: 1.064s, episode steps: 69, steps per second: 65, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 7.275 [2.000, 15.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.005030, mean_absolute_error: 0.929761, mean_q: 0.997175\n",
            "  78814/150000: episode: 1748, duration: 2.105s, episode steps: 150, steps per second: 71, episode reward: 2.000, mean reward: 0.013 [0.000, 1.000], mean action: 7.340 [0.000, 19.000], mean observation: -0.632 [-1.010, 1.011], loss: 0.006739, mean_absolute_error: 0.917825, mean_q: 0.983420\n",
            "  78902/150000: episode: 1749, duration: 1.270s, episode steps: 88, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 9.330 [0.000, 19.000], mean observation: -0.635 [-1.011, 1.057], loss: 0.006278, mean_absolute_error: 0.919965, mean_q: 0.991876\n",
            "  78963/150000: episode: 1750, duration: 0.945s, episode steps: 61, steps per second: 65, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 5.033 [2.000, 20.000], mean observation: -0.634 [-1.009, 1.011], loss: 0.008866, mean_absolute_error: 0.922988, mean_q: 1.002087\n",
            "  78993/150000: episode: 1751, duration: 0.522s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008507, mean_absolute_error: 0.929666, mean_q: 1.005726\n",
            "  79023/150000: episode: 1752, duration: 0.529s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 8.533 [0.000, 15.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.007543, mean_absolute_error: 0.931949, mean_q: 1.011231\n",
            "  79120/150000: episode: 1753, duration: 1.403s, episode steps: 97, steps per second: 69, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 12.660 [0.000, 20.000], mean observation: -0.631 [-1.011, 1.000], loss: 0.007350, mean_absolute_error: 0.934230, mean_q: 1.008752\n",
            "  79223/150000: episode: 1754, duration: 1.551s, episode steps: 103, steps per second: 66, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 8.427 [0.000, 20.000], mean observation: -0.632 [-1.009, 1.015], loss: 0.005131, mean_absolute_error: 0.933354, mean_q: 1.003899\n",
            "  79253/150000: episode: 1755, duration: 0.529s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.833 [0.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004051, mean_absolute_error: 0.927669, mean_q: 0.991788\n",
            "  79271/150000: episode: 1756, duration: 0.361s, episode steps: 18, steps per second: 50, episode reward: 2.000, mean reward: 0.111 [0.000, 1.100], mean action: 10.944 [5.000, 14.000], mean observation: -0.631 [-1.291, 2.186], loss: 0.008889, mean_absolute_error: 0.933075, mean_q: 1.005655\n",
            "  79284/150000: episode: 1757, duration: 0.304s, episode steps: 13, steps per second: 43, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.077 [5.000, 12.000], mean observation: -0.634 [-1.046, 1.175], loss: 0.002453, mean_absolute_error: 0.926403, mean_q: 0.999437\n",
            "  79296/150000: episode: 1758, duration: 0.313s, episode steps: 12, steps per second: 38, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 16.083 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.006], loss: 0.005747, mean_absolute_error: 0.924869, mean_q: 0.989112\n",
            "  79400/150000: episode: 1759, duration: 1.496s, episode steps: 104, steps per second: 70, episode reward: 2.000, mean reward: 0.019 [0.000, 1.000], mean action: 11.990 [0.000, 19.000], mean observation: -0.632 [-1.011, 1.087], loss: 0.006636, mean_absolute_error: 0.923665, mean_q: 0.988140\n",
            "  79449/150000: episode: 1760, duration: 0.744s, episode steps: 49, steps per second: 66, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 10.082 [3.000, 20.000], mean observation: -0.602 [-1.043, 9.589], loss: 0.004864, mean_absolute_error: 0.921610, mean_q: 0.989017\n",
            "  79488/150000: episode: 1761, duration: 0.628s, episode steps: 39, steps per second: 62, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 6.179 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.006285, mean_absolute_error: 0.915896, mean_q: 0.979067\n",
            "  79642/150000: episode: 1762, duration: 2.165s, episode steps: 154, steps per second: 71, episode reward: 0.900, mean reward: 0.006 [0.000, 0.900], mean action: 7.597 [0.000, 20.000], mean observation: -0.632 [-1.011, 1.146], loss: 0.006919, mean_absolute_error: 0.914288, mean_q: 0.985221\n",
            "  79672/150000: episode: 1763, duration: 0.502s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006819, mean_absolute_error: 0.906768, mean_q: 0.978156\n",
            "  79709/150000: episode: 1764, duration: 0.609s, episode steps: 37, steps per second: 61, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.568 [3.000, 19.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.006786, mean_absolute_error: 0.909723, mean_q: 0.989284\n",
            "  79829/150000: episode: 1765, duration: 1.657s, episode steps: 120, steps per second: 72, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 5.442 [2.000, 19.000], mean observation: -0.630 [-1.011, 1.000], loss: 0.006147, mean_absolute_error: 0.918470, mean_q: 0.990656\n",
            "  79959/150000: episode: 1766, duration: 1.775s, episode steps: 130, steps per second: 73, episode reward: 0.000, mean reward: 0.000 [-1.000, 0.900], mean action: 7.062 [0.000, 20.000], mean observation: -0.620 [-1.112, 9.611], loss: 0.007438, mean_absolute_error: 0.917646, mean_q: 0.986656\n",
            "  79989/150000: episode: 1767, duration: 0.523s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005103, mean_absolute_error: 0.919036, mean_q: 0.991162\n",
            "  80036/150000: episode: 1768, duration: 0.779s, episode steps: 47, steps per second: 60, episode reward: 1.000, mean reward: 0.021 [0.000, 0.900], mean action: 3.064 [0.000, 19.000], mean observation: -0.630 [-1.009, 2.506], loss: 0.008310, mean_absolute_error: 0.921716, mean_q: 0.999151\n",
            "  80052/150000: episode: 1769, duration: 0.330s, episode steps: 16, steps per second: 48, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 1.062 [0.000, 12.000], mean observation: -0.635 [-1.017, 1.001], loss: 0.007182, mean_absolute_error: 0.920030, mean_q: 0.994987\n",
            "  80122/150000: episode: 1770, duration: 1.034s, episode steps: 70, steps per second: 68, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 3.943 [0.000, 18.000], mean observation: -0.636 [-1.009, 1.000], loss: 0.007139, mean_absolute_error: 0.928681, mean_q: 0.993079\n",
            "  80183/150000: episode: 1771, duration: 0.928s, episode steps: 61, steps per second: 66, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 9.311 [0.000, 17.000], mean observation: -0.611 [-1.011, 8.266], loss: 0.004926, mean_absolute_error: 0.923705, mean_q: 0.982889\n",
            "  80213/150000: episode: 1772, duration: 0.493s, episode steps: 30, steps per second: 61, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004597, mean_absolute_error: 0.923793, mean_q: 0.983210\n",
            "  80287/150000: episode: 1773, duration: 1.069s, episode steps: 74, steps per second: 69, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 6.135 [1.000, 19.000], mean observation: -0.614 [-1.059, 9.067], loss: 0.006730, mean_absolute_error: 0.919197, mean_q: 0.990839\n",
            "  80297/150000: episode: 1774, duration: 0.259s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.300 [5.000, 13.000], mean observation: -0.633 [-1.011, 1.080], loss: 0.006562, mean_absolute_error: 0.912202, mean_q: 0.966859\n",
            "  80309/150000: episode: 1775, duration: 0.282s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.667 [5.000, 16.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007765, mean_absolute_error: 0.916769, mean_q: 0.982730\n",
            "  80319/150000: episode: 1776, duration: 0.250s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.800 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.008], loss: 0.005512, mean_absolute_error: 0.917949, mean_q: 0.983804\n",
            "  80332/150000: episode: 1777, duration: 0.321s, episode steps: 13, steps per second: 41, episode reward: 0.900, mean reward: 0.069 [0.000, 0.900], mean action: 11.692 [5.000, 20.000], mean observation: -0.632 [-1.009, 1.340], loss: 0.006526, mean_absolute_error: 0.916769, mean_q: 0.990614\n",
            "  80344/150000: episode: 1778, duration: 0.301s, episode steps: 12, steps per second: 40, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 14.583 [5.000, 20.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.006941, mean_absolute_error: 0.920680, mean_q: 0.999702\n",
            "  80378/150000: episode: 1779, duration: 0.552s, episode steps: 34, steps per second: 62, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 16.676 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.001258, mean_absolute_error: 0.921676, mean_q: 1.001939\n",
            "  80469/150000: episode: 1780, duration: 1.320s, episode steps: 91, steps per second: 69, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 13.615 [0.000, 17.000], mean observation: -0.637 [-1.009, 1.000], loss: 0.006435, mean_absolute_error: 0.925096, mean_q: 0.993131\n",
            "  80869/150000: episode: 1781, duration: 5.455s, episode steps: 400, steps per second: 73, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 6.763 [0.000, 20.000], mean observation: -0.638 [-1.011, 3.667], loss: 0.007286, mean_absolute_error: 0.921950, mean_q: 0.993883\n",
            "  80935/150000: episode: 1782, duration: 1.005s, episode steps: 66, steps per second: 66, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 5.485 [1.000, 20.000], mean observation: -0.632 [-1.011, 1.011], loss: 0.006910, mean_absolute_error: 0.926835, mean_q: 0.996563\n",
            "  80964/150000: episode: 1783, duration: 0.497s, episode steps: 29, steps per second: 58, episode reward: 2.000, mean reward: 0.069 [0.000, 1.000], mean action: 7.034 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.007850, mean_absolute_error: 0.926875, mean_q: 0.996018\n",
            "  81047/150000: episode: 1784, duration: 1.205s, episode steps: 83, steps per second: 69, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 8.458 [5.000, 20.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.006997, mean_absolute_error: 0.925626, mean_q: 0.993465\n",
            "  81057/150000: episode: 1785, duration: 0.253s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 5.700 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.013], loss: 0.004038, mean_absolute_error: 0.929255, mean_q: 1.005190\n",
            "  81087/150000: episode: 1786, duration: 0.500s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.000 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006302, mean_absolute_error: 0.923175, mean_q: 0.994904\n",
            "  81126/150000: episode: 1787, duration: 0.632s, episode steps: 39, steps per second: 62, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 5.359 [1.000, 19.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.007350, mean_absolute_error: 0.922785, mean_q: 0.992286\n",
            "  81180/150000: episode: 1788, duration: 0.813s, episode steps: 54, steps per second: 66, episode reward: 2.000, mean reward: 0.037 [0.000, 1.000], mean action: 8.796 [0.000, 15.000], mean observation: -0.630 [-1.011, 2.220], loss: 0.006322, mean_absolute_error: 0.916228, mean_q: 0.981523\n",
            "  81216/150000: episode: 1789, duration: 0.575s, episode steps: 36, steps per second: 63, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 8.389 [5.000, 17.000], mean observation: -0.633 [-1.011, 1.007], loss: 0.006245, mean_absolute_error: 0.915337, mean_q: 0.987047\n",
            "  81462/150000: episode: 1790, duration: 3.403s, episode steps: 246, steps per second: 72, episode reward: 1.000, mean reward: 0.004 [0.000, 0.900], mean action: 13.963 [0.000, 20.000], mean observation: -0.624 [-1.009, 7.155], loss: 0.006733, mean_absolute_error: 0.922736, mean_q: 0.995706\n",
            "  81534/150000: episode: 1791, duration: 1.053s, episode steps: 72, steps per second: 68, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 9.361 [1.000, 18.000], mean observation: -0.621 [-1.011, 8.189], loss: 0.006318, mean_absolute_error: 0.923255, mean_q: 0.985701\n",
            "  81612/150000: episode: 1792, duration: 1.167s, episode steps: 78, steps per second: 67, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 5.423 [1.000, 20.000], mean observation: -0.619 [-1.040, 8.378], loss: 0.005829, mean_absolute_error: 0.919882, mean_q: 0.985429\n",
            "  81702/150000: episode: 1793, duration: 1.332s, episode steps: 90, steps per second: 68, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.178 [0.000, 19.000], mean observation: -0.634 [-1.009, 1.016], loss: 0.003836, mean_absolute_error: 0.915917, mean_q: 0.986181\n",
            "  81781/150000: episode: 1794, duration: 1.146s, episode steps: 79, steps per second: 69, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 8.392 [1.000, 20.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.005940, mean_absolute_error: 0.907547, mean_q: 0.978145\n",
            "  81899/150000: episode: 1795, duration: 1.692s, episode steps: 118, steps per second: 70, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 4.661 [0.000, 16.000], mean observation: -0.630 [-1.011, 2.643], loss: 0.008075, mean_absolute_error: 0.909891, mean_q: 0.983102\n",
            "  81929/150000: episode: 1796, duration: 0.507s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.533 [0.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006811, mean_absolute_error: 0.916976, mean_q: 0.989416\n",
            "  81982/150000: episode: 1797, duration: 0.848s, episode steps: 53, steps per second: 62, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 8.094 [5.000, 19.000], mean observation: -0.610 [-1.020, 8.212], loss: 0.007134, mean_absolute_error: 0.916798, mean_q: 0.984266\n",
            "  82056/150000: episode: 1798, duration: 1.079s, episode steps: 74, steps per second: 69, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 6.784 [0.000, 18.000], mean observation: -0.636 [-1.011, 1.228], loss: 0.005575, mean_absolute_error: 0.916352, mean_q: 0.979003\n",
            "  82089/150000: episode: 1799, duration: 0.580s, episode steps: 33, steps per second: 57, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 6.758 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010782, mean_absolute_error: 0.917222, mean_q: 0.995489\n",
            "  82113/150000: episode: 1800, duration: 0.439s, episode steps: 24, steps per second: 55, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 11.625 [5.000, 20.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.004197, mean_absolute_error: 0.915479, mean_q: 0.981365\n",
            "  82175/150000: episode: 1801, duration: 0.960s, episode steps: 62, steps per second: 65, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 12.097 [2.000, 18.000], mean observation: -0.633 [-1.009, 1.015], loss: 0.003878, mean_absolute_error: 0.918741, mean_q: 0.987964\n",
            "  82191/150000: episode: 1802, duration: 0.326s, episode steps: 16, steps per second: 49, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 5.438 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.000885, mean_absolute_error: 0.917117, mean_q: 0.992898\n",
            "  82291/150000: episode: 1803, duration: 1.428s, episode steps: 100, steps per second: 70, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 6.970 [0.000, 19.000], mean observation: -0.631 [-1.009, 1.319], loss: 0.006599, mean_absolute_error: 0.915139, mean_q: 0.981041\n",
            "  82365/150000: episode: 1804, duration: 1.127s, episode steps: 74, steps per second: 66, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 3.243 [2.000, 14.000], mean observation: -0.620 [-1.020, 8.093], loss: 0.005807, mean_absolute_error: 0.914392, mean_q: 0.981173\n",
            "  82449/150000: episode: 1805, duration: 1.215s, episode steps: 84, steps per second: 69, episode reward: 2.000, mean reward: 0.024 [0.000, 1.000], mean action: 9.905 [2.000, 19.000], mean observation: -0.631 [-1.011, 1.015], loss: 0.005331, mean_absolute_error: 0.909335, mean_q: 0.971280\n",
            "  82458/150000: episode: 1806, duration: 0.239s, episode steps: 9, steps per second: 38, episode reward: 2.000, mean reward: 0.222 [0.000, 2.000], mean action: 4.889 [4.000, 12.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.007445, mean_absolute_error: 0.910961, mean_q: 0.977672\n",
            "  82500/150000: episode: 1807, duration: 0.705s, episode steps: 42, steps per second: 60, episode reward: 1.000, mean reward: 0.024 [0.000, 0.900], mean action: 5.214 [4.000, 20.000], mean observation: -0.634 [-1.011, 1.007], loss: 0.009214, mean_absolute_error: 0.906705, mean_q: 0.972922\n",
            "  82613/150000: episode: 1808, duration: 1.577s, episode steps: 113, steps per second: 72, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 4.858 [0.000, 20.000], mean observation: -0.632 [-1.009, 1.233], loss: 0.006025, mean_absolute_error: 0.907861, mean_q: 0.984546\n",
            "  82737/150000: episode: 1809, duration: 1.755s, episode steps: 124, steps per second: 71, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 5.298 [2.000, 20.000], mean observation: -0.632 [-1.009, 1.894], loss: 0.006897, mean_absolute_error: 0.915614, mean_q: 0.993591\n",
            "  82767/150000: episode: 1810, duration: 0.523s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 12.400 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005627, mean_absolute_error: 0.926310, mean_q: 0.995792\n",
            "  82779/150000: episode: 1811, duration: 0.306s, episode steps: 12, steps per second: 39, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.005834, mean_absolute_error: 0.921771, mean_q: 0.986089\n",
            "  82791/150000: episode: 1812, duration: 0.287s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 6.500 [2.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.011433, mean_absolute_error: 0.923190, mean_q: 1.000733\n",
            "  82825/150000: episode: 1813, duration: 0.589s, episode steps: 34, steps per second: 58, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 5.441 [0.000, 18.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.005251, mean_absolute_error: 0.921466, mean_q: 0.995676\n",
            "  82878/150000: episode: 1814, duration: 0.820s, episode steps: 53, steps per second: 65, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 5.868 [0.000, 19.000], mean observation: -0.609 [-1.009, 8.114], loss: 0.006670, mean_absolute_error: 0.929114, mean_q: 0.999898\n",
            "  83068/150000: episode: 1815, duration: 2.813s, episode steps: 190, steps per second: 68, episode reward: 0.900, mean reward: 0.005 [0.000, 0.900], mean action: 4.079 [0.000, 20.000], mean observation: -0.638 [-1.011, 6.412], loss: 0.006378, mean_absolute_error: 0.924017, mean_q: 0.993537\n",
            "  83166/150000: episode: 1816, duration: 1.414s, episode steps: 98, steps per second: 69, episode reward: 2.000, mean reward: 0.020 [0.000, 1.000], mean action: 4.827 [1.000, 18.000], mean observation: -0.634 [-1.011, 1.017], loss: 0.008784, mean_absolute_error: 0.919907, mean_q: 0.991826\n",
            "  83196/150000: episode: 1817, duration: 0.505s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.733 [0.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007391, mean_absolute_error: 0.922914, mean_q: 1.004171\n",
            "  83224/150000: episode: 1818, duration: 0.504s, episode steps: 28, steps per second: 56, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 5.286 [5.000, 13.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.002668, mean_absolute_error: 0.930320, mean_q: 1.014951\n",
            "  83254/150000: episode: 1819, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005349, mean_absolute_error: 0.932229, mean_q: 0.994512\n",
            "  83327/150000: episode: 1820, duration: 1.052s, episode steps: 73, steps per second: 69, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 4.671 [3.000, 17.000], mean observation: -0.618 [-1.009, 8.058], loss: 0.006199, mean_absolute_error: 0.930540, mean_q: 0.997915\n",
            "  83526/150000: episode: 1821, duration: 2.753s, episode steps: 199, steps per second: 72, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 7.613 [0.000, 20.000], mean observation: -0.625 [-1.040, 8.367], loss: 0.006655, mean_absolute_error: 0.921275, mean_q: 0.989157\n",
            "  83559/150000: episode: 1822, duration: 0.577s, episode steps: 33, steps per second: 57, episode reward: 1.000, mean reward: 0.030 [0.000, 0.900], mean action: 4.727 [3.000, 19.000], mean observation: -0.634 [-1.009, 1.013], loss: 0.006094, mean_absolute_error: 0.915594, mean_q: 0.993614\n",
            "  83569/150000: episode: 1823, duration: 0.267s, episode steps: 10, steps per second: 37, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 4.900 [4.000, 12.000], mean observation: -0.635 [-1.036, 1.003], loss: 0.006922, mean_absolute_error: 0.915926, mean_q: 0.993069\n",
            "  83604/150000: episode: 1824, duration: 0.607s, episode steps: 35, steps per second: 58, episode reward: 1.000, mean reward: 0.029 [0.000, 0.900], mean action: 4.857 [3.000, 16.000], mean observation: -0.634 [-1.011, 1.006], loss: 0.008735, mean_absolute_error: 0.928077, mean_q: 1.003376\n",
            "  83722/150000: episode: 1825, duration: 1.645s, episode steps: 118, steps per second: 72, episode reward: 2.000, mean reward: 0.017 [0.000, 1.000], mean action: 6.847 [0.000, 18.000], mean observation: -0.631 [-1.011, 1.116], loss: 0.007185, mean_absolute_error: 0.924823, mean_q: 0.998733\n",
            "  83752/150000: episode: 1826, duration: 0.499s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 13.967 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010467, mean_absolute_error: 0.918811, mean_q: 0.991988\n",
            "  83824/150000: episode: 1827, duration: 1.063s, episode steps: 72, steps per second: 68, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 8.097 [2.000, 18.000], mean observation: -0.637 [-1.009, 1.000], loss: 0.005048, mean_absolute_error: 0.922059, mean_q: 0.989758\n",
            "  83863/150000: episode: 1828, duration: 0.643s, episode steps: 39, steps per second: 61, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 5.615 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.006864, mean_absolute_error: 0.921694, mean_q: 0.995534\n",
            "  83893/150000: episode: 1829, duration: 0.506s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.967 [4.000, 5.000], mean observation: -0.633 [-1.009, 1.015], loss: 0.010997, mean_absolute_error: 0.921880, mean_q: 0.999198\n",
            "  83984/150000: episode: 1830, duration: 1.359s, episode steps: 91, steps per second: 67, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 12.549 [0.000, 19.000], mean observation: -0.632 [-1.011, 1.262], loss: 0.008004, mean_absolute_error: 0.928506, mean_q: 1.009103\n",
            "  84014/150000: episode: 1831, duration: 0.525s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 15.767 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005327, mean_absolute_error: 0.935585, mean_q: 1.016378\n",
            "  84126/150000: episode: 1832, duration: 1.634s, episode steps: 112, steps per second: 69, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 7.723 [1.000, 20.000], mean observation: -0.631 [-1.009, 1.000], loss: 0.006436, mean_absolute_error: 0.945357, mean_q: 1.017717\n",
            "  84156/150000: episode: 1833, duration: 0.552s, episode steps: 30, steps per second: 54, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.267 [5.000, 9.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007975, mean_absolute_error: 0.944431, mean_q: 1.009441\n",
            "  84556/150000: episode: 1834, duration: 5.482s, episode steps: 400, steps per second: 73, episode reward: 0.900, mean reward: 0.002 [0.000, 0.900], mean action: 5.402 [0.000, 20.000], mean observation: -0.634 [-1.009, 1.213], loss: 0.005781, mean_absolute_error: 0.939517, mean_q: 1.012139\n",
            "  84612/150000: episode: 1835, duration: 0.848s, episode steps: 56, steps per second: 66, episode reward: 2.000, mean reward: 0.036 [0.000, 1.000], mean action: 8.018 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.008], loss: 0.006360, mean_absolute_error: 0.942981, mean_q: 1.009368\n",
            "  84694/150000: episode: 1836, duration: 1.173s, episode steps: 82, steps per second: 70, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 7.976 [1.000, 19.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.005031, mean_absolute_error: 0.938642, mean_q: 1.011343\n",
            "  84862/150000: episode: 1837, duration: 2.317s, episode steps: 168, steps per second: 73, episode reward: 1.000, mean reward: 0.006 [0.000, 0.900], mean action: 6.696 [0.000, 20.000], mean observation: -0.635 [-1.011, 1.193], loss: 0.006205, mean_absolute_error: 0.941397, mean_q: 1.009997\n",
            "  84947/150000: episode: 1838, duration: 1.237s, episode steps: 85, steps per second: 69, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 10.094 [3.000, 20.000], mean observation: -0.637 [-1.011, 1.194], loss: 0.004901, mean_absolute_error: 0.941900, mean_q: 1.008825\n",
            "  84983/150000: episode: 1839, duration: 0.572s, episode steps: 36, steps per second: 63, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 4.833 [2.000, 5.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.008655, mean_absolute_error: 0.942727, mean_q: 1.017704\n",
            "  85140/150000: episode: 1840, duration: 2.174s, episode steps: 157, steps per second: 72, episode reward: 1.000, mean reward: 0.006 [0.000, 0.900], mean action: 7.338 [0.000, 19.000], mean observation: -0.638 [-1.011, 1.280], loss: 0.005582, mean_absolute_error: 0.943288, mean_q: 1.017315\n",
            "  85232/150000: episode: 1841, duration: 1.313s, episode steps: 92, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 7.848 [0.000, 18.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.006619, mean_absolute_error: 0.939808, mean_q: 1.014702\n",
            "  85262/150000: episode: 1842, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.333 [1.000, 17.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.002762, mean_absolute_error: 0.945538, mean_q: 1.015565\n",
            "  85343/150000: episode: 1843, duration: 1.213s, episode steps: 81, steps per second: 67, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 4.988 [2.000, 20.000], mean observation: -0.631 [-1.009, 1.013], loss: 0.004123, mean_absolute_error: 0.941905, mean_q: 1.010709\n",
            "  85379/150000: episode: 1844, duration: 0.556s, episode steps: 36, steps per second: 65, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 5.583 [2.000, 17.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.007339, mean_absolute_error: 0.943284, mean_q: 1.017849\n",
            "  85437/150000: episode: 1845, duration: 0.911s, episode steps: 58, steps per second: 64, episode reward: 1.000, mean reward: 0.017 [0.000, 0.900], mean action: 5.190 [2.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007461, mean_absolute_error: 0.942131, mean_q: 1.009625\n",
            "  85473/150000: episode: 1846, duration: 0.629s, episode steps: 36, steps per second: 57, episode reward: 1.000, mean reward: 0.028 [0.000, 0.900], mean action: 9.167 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.008488, mean_absolute_error: 0.941617, mean_q: 1.014231\n",
            "  85523/150000: episode: 1847, duration: 0.777s, episode steps: 50, steps per second: 64, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 6.200 [5.000, 20.000], mean observation: -0.611 [-1.009, 7.668], loss: 0.007037, mean_absolute_error: 0.937913, mean_q: 1.023630\n",
            "  85552/150000: episode: 1848, duration: 0.494s, episode steps: 29, steps per second: 59, episode reward: 2.000, mean reward: 0.069 [0.000, 1.000], mean action: 11.690 [3.000, 19.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.007212, mean_absolute_error: 0.938010, mean_q: 1.008109\n",
            "  85640/150000: episode: 1849, duration: 1.279s, episode steps: 88, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 7.398 [3.000, 19.000], mean observation: -0.636 [-1.009, 1.000], loss: 0.007457, mean_absolute_error: 0.939749, mean_q: 1.012340\n",
            "  85744/150000: episode: 1850, duration: 1.469s, episode steps: 104, steps per second: 71, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 8.106 [1.000, 17.000], mean observation: -0.630 [-1.011, 1.000], loss: 0.007039, mean_absolute_error: 0.945516, mean_q: 1.019939\n",
            "  85814/150000: episode: 1851, duration: 1.029s, episode steps: 70, steps per second: 68, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 5.400 [1.000, 20.000], mean observation: -0.637 [-1.009, 1.184], loss: 0.005339, mean_absolute_error: 0.951781, mean_q: 1.030082\n",
            "  85914/150000: episode: 1852, duration: 1.408s, episode steps: 100, steps per second: 71, episode reward: 2.000, mean reward: 0.020 [0.000, 1.100], mean action: 10.970 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.936], loss: 0.006285, mean_absolute_error: 0.961682, mean_q: 1.035459\n",
            "  85951/150000: episode: 1853, duration: 0.603s, episode steps: 37, steps per second: 61, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 12.324 [5.000, 20.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.009651, mean_absolute_error: 0.960163, mean_q: 1.039551\n",
            "  85988/150000: episode: 1854, duration: 0.600s, episode steps: 37, steps per second: 62, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 4.676 [0.000, 5.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.009354, mean_absolute_error: 0.965716, mean_q: 1.055593\n",
            "  86101/150000: episode: 1855, duration: 1.618s, episode steps: 113, steps per second: 70, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 8.425 [0.000, 20.000], mean observation: -0.636 [-1.009, 1.000], loss: 0.006351, mean_absolute_error: 0.975782, mean_q: 1.048601\n",
            "  86161/150000: episode: 1856, duration: 0.912s, episode steps: 60, steps per second: 66, episode reward: 1.000, mean reward: 0.017 [0.000, 0.900], mean action: 7.683 [2.000, 20.000], mean observation: -0.611 [-1.011, 8.115], loss: 0.005554, mean_absolute_error: 0.971471, mean_q: 1.040618\n",
            "  86214/150000: episode: 1857, duration: 0.805s, episode steps: 53, steps per second: 66, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 4.340 [0.000, 14.000], mean observation: -0.609 [-1.011, 8.114], loss: 0.007185, mean_absolute_error: 0.963882, mean_q: 1.036423\n",
            "  86260/150000: episode: 1858, duration: 0.736s, episode steps: 46, steps per second: 62, episode reward: 1.000, mean reward: 0.022 [0.000, 0.900], mean action: 4.935 [3.000, 20.000], mean observation: -0.634 [-1.009, 1.012], loss: 0.005187, mean_absolute_error: 0.962528, mean_q: 1.033335\n",
            "  86321/150000: episode: 1859, duration: 0.981s, episode steps: 61, steps per second: 62, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 6.459 [0.000, 17.000], mean observation: -0.613 [-1.011, 8.114], loss: 0.008299, mean_absolute_error: 0.963475, mean_q: 1.028710\n",
            "  86403/150000: episode: 1860, duration: 1.191s, episode steps: 82, steps per second: 69, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 5.171 [0.000, 15.000], mean observation: -0.634 [-1.009, 1.000], loss: 0.007064, mean_absolute_error: 0.960245, mean_q: 1.034692\n",
            "  86502/150000: episode: 1861, duration: 1.416s, episode steps: 99, steps per second: 70, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 5.707 [0.000, 15.000], mean observation: -0.632 [-1.011, 1.000], loss: 0.008204, mean_absolute_error: 0.959376, mean_q: 1.029031\n",
            "  86532/150000: episode: 1862, duration: 0.498s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.867 [1.000, 5.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.008106, mean_absolute_error: 0.949642, mean_q: 1.026170\n",
            "  86587/150000: episode: 1863, duration: 0.845s, episode steps: 55, steps per second: 65, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 8.109 [5.000, 19.000], mean observation: -0.609 [-1.017, 8.381], loss: 0.007429, mean_absolute_error: 0.949802, mean_q: 1.021797\n",
            "  86628/150000: episode: 1864, duration: 0.698s, episode steps: 41, steps per second: 59, episode reward: 1.000, mean reward: 0.024 [0.000, 0.900], mean action: 10.805 [0.000, 17.000], mean observation: -0.631 [-1.011, 1.783], loss: 0.005683, mean_absolute_error: 0.951944, mean_q: 1.027930\n",
            "  86814/150000: episode: 1865, duration: 2.540s, episode steps: 186, steps per second: 73, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 6.742 [0.000, 20.000], mean observation: -0.634 [-1.009, 3.855], loss: 0.006955, mean_absolute_error: 0.948538, mean_q: 1.025203\n",
            "  86867/150000: episode: 1866, duration: 0.843s, episode steps: 53, steps per second: 63, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 7.283 [1.000, 17.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.008133, mean_absolute_error: 0.945003, mean_q: 1.019291\n",
            "  86911/150000: episode: 1867, duration: 0.696s, episode steps: 44, steps per second: 63, episode reward: 2.000, mean reward: 0.045 [0.000, 1.000], mean action: 10.977 [5.000, 15.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.005878, mean_absolute_error: 0.949171, mean_q: 1.021951\n",
            "  87025/150000: episode: 1868, duration: 1.612s, episode steps: 114, steps per second: 71, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 9.535 [2.000, 15.000], mean observation: -0.635 [-1.009, 1.256], loss: 0.007539, mean_absolute_error: 0.944647, mean_q: 1.018653\n",
            "  87060/150000: episode: 1869, duration: 0.581s, episode steps: 35, steps per second: 60, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 5.800 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.015], loss: 0.007260, mean_absolute_error: 0.941160, mean_q: 1.012426\n",
            "  87178/150000: episode: 1870, duration: 1.667s, episode steps: 118, steps per second: 71, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 7.958 [0.000, 19.000], mean observation: -0.631 [-1.011, 1.406], loss: 0.007152, mean_absolute_error: 0.950526, mean_q: 1.026402\n",
            "  87189/150000: episode: 1871, duration: 0.284s, episode steps: 11, steps per second: 39, episode reward: 2.000, mean reward: 0.182 [0.000, 1.100], mean action: 12.000 [5.000, 16.000], mean observation: -0.636 [-1.042, 1.014], loss: 0.006661, mean_absolute_error: 0.947279, mean_q: 1.010214\n",
            "  87201/150000: episode: 1872, duration: 0.272s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.333 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.153], loss: 0.004894, mean_absolute_error: 0.948459, mean_q: 1.012256\n",
            "  87232/150000: episode: 1873, duration: 0.507s, episode steps: 31, steps per second: 61, episode reward: 2.000, mean reward: 0.065 [0.000, 1.000], mean action: 6.290 [4.000, 15.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.005832, mean_absolute_error: 0.949225, mean_q: 1.016795\n",
            "  87342/150000: episode: 1874, duration: 1.529s, episode steps: 110, steps per second: 72, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 12.573 [1.000, 19.000], mean observation: -0.632 [-1.011, 1.000], loss: 0.005553, mean_absolute_error: 0.945403, mean_q: 1.015837\n",
            "  87381/150000: episode: 1875, duration: 0.622s, episode steps: 39, steps per second: 63, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 7.462 [5.000, 20.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.008350, mean_absolute_error: 0.945142, mean_q: 1.012568\n",
            "  87411/150000: episode: 1876, duration: 0.501s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.200 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004950, mean_absolute_error: 0.935392, mean_q: 1.012424\n",
            "  87522/150000: episode: 1877, duration: 1.574s, episode steps: 111, steps per second: 71, episode reward: 2.000, mean reward: 0.018 [0.000, 1.000], mean action: 9.216 [1.000, 19.000], mean observation: -0.630 [-1.011, 1.712], loss: 0.005601, mean_absolute_error: 0.940942, mean_q: 1.008802\n",
            "  87635/150000: episode: 1878, duration: 1.598s, episode steps: 113, steps per second: 71, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 7.204 [0.000, 20.000], mean observation: -0.631 [-1.011, 1.015], loss: 0.007497, mean_absolute_error: 0.934692, mean_q: 1.004702\n",
            "  87770/150000: episode: 1879, duration: 1.884s, episode steps: 135, steps per second: 72, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 8.511 [3.000, 19.000], mean observation: -0.633 [-1.009, 1.000], loss: 0.005821, mean_absolute_error: 0.937934, mean_q: 1.008231\n",
            "  87802/150000: episode: 1880, duration: 0.520s, episode steps: 32, steps per second: 61, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 4.969 [4.000, 5.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.008561, mean_absolute_error: 0.932289, mean_q: 0.999114\n",
            "  87840/150000: episode: 1881, duration: 0.618s, episode steps: 38, steps per second: 61, episode reward: 2.000, mean reward: 0.053 [0.000, 1.000], mean action: 5.868 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.009], loss: 0.007928, mean_absolute_error: 0.937736, mean_q: 1.010908\n",
            "  87870/150000: episode: 1882, duration: 0.516s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.833 [0.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005864, mean_absolute_error: 0.930922, mean_q: 1.008316\n",
            "  87949/150000: episode: 1883, duration: 1.139s, episode steps: 79, steps per second: 69, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 5.190 [3.000, 19.000], mean observation: -0.638 [-1.009, 1.000], loss: 0.007478, mean_absolute_error: 0.934036, mean_q: 1.008014\n",
            "  88051/150000: episode: 1884, duration: 1.465s, episode steps: 102, steps per second: 70, episode reward: 2.000, mean reward: 0.020 [0.000, 1.100], mean action: 6.784 [1.000, 20.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.005776, mean_absolute_error: 0.932779, mean_q: 1.007129\n",
            "  88085/150000: episode: 1885, duration: 0.570s, episode steps: 34, steps per second: 60, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 9.882 [2.000, 17.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.008102, mean_absolute_error: 0.935792, mean_q: 1.001111\n",
            "  88115/150000: episode: 1886, duration: 0.513s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 16.700 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005562, mean_absolute_error: 0.936767, mean_q: 1.016416\n",
            "  88216/150000: episode: 1887, duration: 1.432s, episode steps: 101, steps per second: 71, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 9.703 [0.000, 20.000], mean observation: -0.632 [-1.009, 1.000], loss: 0.009748, mean_absolute_error: 0.927293, mean_q: 0.997355\n",
            "  88333/150000: episode: 1888, duration: 1.628s, episode steps: 117, steps per second: 72, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 7.419 [0.000, 18.000], mean observation: -0.632 [-1.009, 1.000], loss: 0.006806, mean_absolute_error: 0.929515, mean_q: 1.001902\n",
            "  88363/150000: episode: 1889, duration: 0.502s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 8.533 [0.000, 19.000], mean observation: -0.634 [-1.011, 1.021], loss: 0.007839, mean_absolute_error: 0.930765, mean_q: 0.998407\n",
            "  88375/150000: episode: 1890, duration: 0.295s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 5.417 [0.000, 12.000], mean observation: -0.635 [-1.011, 1.003], loss: 0.008887, mean_absolute_error: 0.929587, mean_q: 0.997316\n",
            "  88429/150000: episode: 1891, duration: 0.874s, episode steps: 54, steps per second: 62, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 5.796 [4.000, 16.000], mean observation: -0.611 [-1.009, 8.189], loss: 0.007373, mean_absolute_error: 0.927341, mean_q: 1.002875\n",
            "  88523/150000: episode: 1892, duration: 1.333s, episode steps: 94, steps per second: 71, episode reward: 2.000, mean reward: 0.021 [0.000, 1.000], mean action: 4.340 [1.000, 20.000], mean observation: -0.631 [-1.011, 1.012], loss: 0.007025, mean_absolute_error: 0.935079, mean_q: 1.006136\n",
            "  88570/150000: episode: 1893, duration: 0.730s, episode steps: 47, steps per second: 64, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 5.830 [2.000, 20.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.005877, mean_absolute_error: 0.933652, mean_q: 0.998226\n",
            "  88600/150000: episode: 1894, duration: 0.498s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.500 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010851, mean_absolute_error: 0.935527, mean_q: 1.024268\n",
            "  88670/150000: episode: 1895, duration: 1.017s, episode steps: 70, steps per second: 69, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 4.214 [1.000, 12.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.008550, mean_absolute_error: 0.941747, mean_q: 1.016665\n",
            "  88755/150000: episode: 1896, duration: 1.233s, episode steps: 85, steps per second: 69, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 14.835 [1.000, 20.000], mean observation: -0.637 [-1.009, 1.000], loss: 0.005265, mean_absolute_error: 0.942180, mean_q: 1.015850\n",
            "  88785/150000: episode: 1897, duration: 0.523s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 12.300 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005971, mean_absolute_error: 0.946004, mean_q: 1.010762\n",
            "  88818/150000: episode: 1898, duration: 0.545s, episode steps: 33, steps per second: 61, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.667 [4.000, 17.000], mean observation: -0.634 [-1.011, 1.015], loss: 0.006928, mean_absolute_error: 0.941254, mean_q: 1.012859\n",
            "  88933/150000: episode: 1899, duration: 1.630s, episode steps: 115, steps per second: 71, episode reward: 2.000, mean reward: 0.017 [0.000, 1.000], mean action: 6.348 [4.000, 20.000], mean observation: -0.631 [-1.011, 1.050], loss: 0.008757, mean_absolute_error: 0.941650, mean_q: 1.018123\n",
            "  89084/150000: episode: 1900, duration: 2.120s, episode steps: 151, steps per second: 71, episode reward: 1.000, mean reward: 0.007 [0.000, 0.800], mean action: 6.073 [0.000, 19.000], mean observation: -0.631 [-1.009, 1.021], loss: 0.006773, mean_absolute_error: 0.946928, mean_q: 1.017530\n",
            "  89169/150000: episode: 1901, duration: 1.239s, episode steps: 85, steps per second: 69, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 6.047 [5.000, 18.000], mean observation: -0.635 [-1.011, 1.432], loss: 0.006557, mean_absolute_error: 0.943085, mean_q: 1.011565\n",
            "  89276/150000: episode: 1902, duration: 1.531s, episode steps: 107, steps per second: 70, episode reward: 2.000, mean reward: 0.019 [0.000, 1.000], mean action: 2.729 [1.000, 16.000], mean observation: -0.633 [-1.011, 1.007], loss: 0.008725, mean_absolute_error: 0.941357, mean_q: 1.023640\n",
            "  89301/150000: episode: 1903, duration: 0.459s, episode steps: 25, steps per second: 55, episode reward: 2.000, mean reward: 0.080 [0.000, 1.100], mean action: 7.080 [1.000, 19.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.005397, mean_absolute_error: 0.942963, mean_q: 1.011264\n",
            "  89341/150000: episode: 1904, duration: 0.684s, episode steps: 40, steps per second: 59, episode reward: 1.000, mean reward: 0.025 [0.000, 0.900], mean action: 6.250 [4.000, 18.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.005556, mean_absolute_error: 0.949487, mean_q: 1.025770\n",
            "  89379/150000: episode: 1905, duration: 0.642s, episode steps: 38, steps per second: 59, episode reward: 1.000, mean reward: 0.026 [0.000, 0.900], mean action: 4.842 [0.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005869, mean_absolute_error: 0.945207, mean_q: 1.014010\n",
            "  89512/150000: episode: 1906, duration: 1.885s, episode steps: 133, steps per second: 71, episode reward: 2.000, mean reward: 0.015 [0.000, 1.100], mean action: 8.466 [0.000, 20.000], mean observation: -0.633 [-1.017, 1.364], loss: 0.008042, mean_absolute_error: 0.944982, mean_q: 1.017935\n",
            "  89524/150000: episode: 1907, duration: 0.284s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.083 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.014108, mean_absolute_error: 0.935853, mean_q: 1.010771\n",
            "  89674/150000: episode: 1908, duration: 2.097s, episode steps: 150, steps per second: 72, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 6.920 [0.000, 20.000], mean observation: -0.614 [-1.107, 11.850], loss: 0.005258, mean_absolute_error: 0.943943, mean_q: 1.017114\n",
            "  89704/150000: episode: 1909, duration: 0.501s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.700 [0.000, 19.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.006032, mean_absolute_error: 0.944270, mean_q: 1.019688\n",
            "  89734/150000: episode: 1910, duration: 0.514s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 2.367 [0.000, 6.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006096, mean_absolute_error: 0.946849, mean_q: 1.025230\n",
            "  89826/150000: episode: 1911, duration: 1.342s, episode steps: 92, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 8.087 [0.000, 18.000], mean observation: -0.630 [-1.011, 1.010], loss: 0.006445, mean_absolute_error: 0.951349, mean_q: 1.025896\n",
            "  89849/150000: episode: 1912, duration: 0.440s, episode steps: 23, steps per second: 52, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 9.261 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.007042, mean_absolute_error: 0.951205, mean_q: 1.022393\n",
            "  89931/150000: episode: 1913, duration: 1.168s, episode steps: 82, steps per second: 70, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 3.341 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.007692, mean_absolute_error: 0.942133, mean_q: 1.009928\n",
            "  89960/150000: episode: 1914, duration: 0.523s, episode steps: 29, steps per second: 55, episode reward: 2.000, mean reward: 0.069 [0.000, 1.000], mean action: 5.552 [4.000, 16.000], mean observation: -0.634 [-1.011, 1.004], loss: 0.005035, mean_absolute_error: 0.939423, mean_q: 1.006459\n",
            "  89998/150000: episode: 1915, duration: 0.667s, episode steps: 38, steps per second: 57, episode reward: 1.000, mean reward: 0.026 [0.000, 0.900], mean action: 10.026 [2.000, 17.000], mean observation: -0.635 [-1.009, 1.007], loss: 0.006177, mean_absolute_error: 0.938613, mean_q: 1.007291\n",
            "  90034/150000: episode: 1916, duration: 0.590s, episode steps: 36, steps per second: 61, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 13.778 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.008], loss: 0.007547, mean_absolute_error: 0.938841, mean_q: 1.018771\n",
            "  90047/150000: episode: 1917, duration: 0.317s, episode steps: 13, steps per second: 41, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 5.538 [5.000, 12.000], mean observation: -0.637 [-1.046, 1.012], loss: 0.002238, mean_absolute_error: 0.943089, mean_q: 1.011324\n",
            "  90106/150000: episode: 1918, duration: 0.910s, episode steps: 59, steps per second: 65, episode reward: 1.000, mean reward: 0.017 [0.000, 0.900], mean action: 5.186 [2.000, 12.000], mean observation: -0.613 [-1.011, 8.189], loss: 0.007230, mean_absolute_error: 0.940934, mean_q: 1.013179\n",
            "  90136/150000: episode: 1919, duration: 0.514s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 7.133 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.006391, mean_absolute_error: 0.942333, mean_q: 1.013879\n",
            "  90150/150000: episode: 1920, duration: 0.280s, episode steps: 14, steps per second: 50, episode reward: 2.000, mean reward: 0.143 [0.000, 2.000], mean action: 12.714 [12.000, 18.000], mean observation: -0.632 [-1.009, 1.145], loss: 0.003244, mean_absolute_error: 0.953109, mean_q: 1.029395\n",
            "  90160/150000: episode: 1921, duration: 0.254s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 12.100 [5.000, 20.000], mean observation: -0.631 [-1.041, 1.875], loss: 0.005613, mean_absolute_error: 0.943236, mean_q: 1.011864\n",
            "  90170/150000: episode: 1922, duration: 0.274s, episode steps: 10, steps per second: 36, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 5.700 [4.000, 12.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.006726, mean_absolute_error: 0.939279, mean_q: 1.015897\n",
            "  90216/150000: episode: 1923, duration: 0.714s, episode steps: 46, steps per second: 64, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 6.674 [1.000, 13.000], mean observation: -0.633 [-1.011, 1.009], loss: 0.006243, mean_absolute_error: 0.938697, mean_q: 1.019641\n",
            "  90242/150000: episode: 1924, duration: 0.501s, episode steps: 26, steps per second: 52, episode reward: 1.000, mean reward: 0.038 [0.000, 0.900], mean action: 12.346 [4.000, 13.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.007817, mean_absolute_error: 0.943422, mean_q: 1.030253\n",
            "  90264/150000: episode: 1925, duration: 0.409s, episode steps: 22, steps per second: 54, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 12.773 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.005], loss: 0.005434, mean_absolute_error: 0.945098, mean_q: 1.014100\n",
            "  90276/150000: episode: 1926, duration: 0.284s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 8.500 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.003784, mean_absolute_error: 0.951986, mean_q: 1.020658\n",
            "  90288/150000: episode: 1927, duration: 0.280s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.167 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.003], loss: 0.013486, mean_absolute_error: 0.945899, mean_q: 1.016261\n",
            "  90351/150000: episode: 1928, duration: 0.981s, episode steps: 63, steps per second: 64, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 10.048 [0.000, 19.000], mean observation: -0.612 [-1.036, 8.427], loss: 0.007163, mean_absolute_error: 0.938651, mean_q: 1.006910\n",
            "  90363/150000: episode: 1929, duration: 0.274s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.136], loss: 0.007139, mean_absolute_error: 0.936743, mean_q: 1.015193\n",
            "  90393/150000: episode: 1930, duration: 0.516s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.167 [0.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006575, mean_absolute_error: 0.933411, mean_q: 1.004682\n",
            "  90517/150000: episode: 1931, duration: 1.758s, episode steps: 124, steps per second: 71, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 5.298 [2.000, 19.000], mean observation: -0.619 [-1.015, 8.016], loss: 0.007389, mean_absolute_error: 0.932420, mean_q: 1.009589\n",
            "  90612/150000: episode: 1932, duration: 1.334s, episode steps: 95, steps per second: 71, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 9.737 [3.000, 20.000], mean observation: -0.633 [-1.011, 1.524], loss: 0.007342, mean_absolute_error: 0.936432, mean_q: 1.010768\n",
            "  90642/150000: episode: 1933, duration: 0.495s, episode steps: 30, steps per second: 61, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.700 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007004, mean_absolute_error: 0.935149, mean_q: 1.017959\n",
            "  90686/150000: episode: 1934, duration: 0.729s, episode steps: 44, steps per second: 60, episode reward: 1.000, mean reward: 0.023 [0.000, 0.900], mean action: 5.000 [3.000, 20.000], mean observation: -0.635 [-1.011, 1.011], loss: 0.007897, mean_absolute_error: 0.938807, mean_q: 1.012189\n",
            "  90776/150000: episode: 1935, duration: 1.270s, episode steps: 90, steps per second: 71, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 6.711 [1.000, 14.000], mean observation: -0.635 [-1.009, 1.000], loss: 0.005273, mean_absolute_error: 0.944896, mean_q: 1.020533\n",
            "  90794/150000: episode: 1936, duration: 0.370s, episode steps: 18, steps per second: 49, episode reward: 2.000, mean reward: 0.111 [0.000, 1.000], mean action: 7.889 [5.000, 18.000], mean observation: -0.636 [-1.011, 1.014], loss: 0.006613, mean_absolute_error: 0.939033, mean_q: 1.003392\n",
            "  90830/150000: episode: 1937, duration: 0.591s, episode steps: 36, steps per second: 61, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 5.806 [2.000, 15.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.006210, mean_absolute_error: 0.938258, mean_q: 1.008288\n",
            "  90856/150000: episode: 1938, duration: 0.457s, episode steps: 26, steps per second: 57, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 7.538 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.007737, mean_absolute_error: 0.934177, mean_q: 1.002110\n",
            "  90868/150000: episode: 1939, duration: 0.288s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.750 [5.000, 16.000], mean observation: -0.633 [-1.011, 1.570], loss: 0.009805, mean_absolute_error: 0.934058, mean_q: 1.014907\n",
            "  90878/150000: episode: 1940, duration: 0.247s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 6.600 [4.000, 13.000], mean observation: -0.636 [-1.011, 1.013], loss: 0.013845, mean_absolute_error: 0.931761, mean_q: 1.004759\n",
            "  90921/150000: episode: 1941, duration: 0.725s, episode steps: 43, steps per second: 59, episode reward: 1.000, mean reward: 0.023 [0.000, 0.900], mean action: 4.930 [4.000, 19.000], mean observation: -0.634 [-1.009, 1.008], loss: 0.005231, mean_absolute_error: 0.932708, mean_q: 1.008950\n",
            "  91018/150000: episode: 1942, duration: 1.353s, episode steps: 97, steps per second: 72, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 8.722 [4.000, 15.000], mean observation: -0.636 [-1.011, 1.127], loss: 0.006902, mean_absolute_error: 0.934879, mean_q: 1.003116\n",
            "  91120/150000: episode: 1943, duration: 1.424s, episode steps: 102, steps per second: 72, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 7.569 [2.000, 19.000], mean observation: -0.631 [-1.009, 1.000], loss: 0.008535, mean_absolute_error: 0.930325, mean_q: 1.005645\n",
            "  91210/150000: episode: 1944, duration: 1.312s, episode steps: 90, steps per second: 69, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 6.189 [2.000, 19.000], mean observation: -0.638 [-1.011, 1.000], loss: 0.007116, mean_absolute_error: 0.935916, mean_q: 1.010322\n",
            "  91240/150000: episode: 1945, duration: 0.513s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.533 [5.000, 15.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006571, mean_absolute_error: 0.932207, mean_q: 1.007313\n",
            "  91270/150000: episode: 1946, duration: 0.504s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008995, mean_absolute_error: 0.933518, mean_q: 1.005094\n",
            "  91308/150000: episode: 1947, duration: 0.626s, episode steps: 38, steps per second: 61, episode reward: 1.000, mean reward: 0.026 [0.000, 0.900], mean action: 4.553 [1.000, 13.000], mean observation: -0.632 [-1.009, 1.000], loss: 0.007758, mean_absolute_error: 0.936022, mean_q: 1.004537\n",
            "  91407/150000: episode: 1948, duration: 1.389s, episode steps: 99, steps per second: 71, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 4.596 [3.000, 20.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.007896, mean_absolute_error: 0.932461, mean_q: 1.004667\n",
            "  91500/150000: episode: 1949, duration: 1.329s, episode steps: 93, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.645 [1.000, 20.000], mean observation: -0.632 [-1.011, 1.171], loss: 0.007079, mean_absolute_error: 0.931299, mean_q: 1.003751\n",
            "  91590/150000: episode: 1950, duration: 1.314s, episode steps: 90, steps per second: 68, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 4.611 [1.000, 20.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.006535, mean_absolute_error: 0.923960, mean_q: 0.997324\n",
            "  91662/150000: episode: 1951, duration: 1.049s, episode steps: 72, steps per second: 69, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 4.889 [1.000, 20.000], mean observation: -0.638 [-1.009, 1.000], loss: 0.007274, mean_absolute_error: 0.924093, mean_q: 1.000523\n",
            "  91770/150000: episode: 1952, duration: 1.514s, episode steps: 108, steps per second: 71, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 6.944 [2.000, 19.000], mean observation: -0.629 [-1.009, 1.000], loss: 0.005718, mean_absolute_error: 0.922308, mean_q: 0.992092\n",
            "  91807/150000: episode: 1953, duration: 0.595s, episode steps: 37, steps per second: 62, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 6.568 [1.000, 18.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.008247, mean_absolute_error: 0.923623, mean_q: 0.999425\n",
            "  91888/150000: episode: 1954, duration: 1.184s, episode steps: 81, steps per second: 68, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 7.926 [1.000, 20.000], mean observation: -0.635 [-1.011, 1.153], loss: 0.006475, mean_absolute_error: 0.926523, mean_q: 0.996795\n",
            "  92010/150000: episode: 1955, duration: 1.773s, episode steps: 122, steps per second: 69, episode reward: 0.900, mean reward: 0.007 [0.000, 0.900], mean action: 9.172 [1.000, 20.000], mean observation: -0.632 [-1.009, 1.165], loss: 0.007134, mean_absolute_error: 0.917233, mean_q: 0.992828\n",
            "  92053/150000: episode: 1956, duration: 0.715s, episode steps: 43, steps per second: 60, episode reward: 1.000, mean reward: 0.023 [0.000, 0.900], mean action: 15.116 [3.000, 16.000], mean observation: -0.632 [-1.011, 1.000], loss: 0.006808, mean_absolute_error: 0.926753, mean_q: 1.000202\n",
            "  92124/150000: episode: 1957, duration: 1.018s, episode steps: 71, steps per second: 70, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 5.296 [2.000, 16.000], mean observation: -0.637 [-1.009, 1.000], loss: 0.006645, mean_absolute_error: 0.928040, mean_q: 1.001603\n",
            "  92165/150000: episode: 1958, duration: 0.668s, episode steps: 41, steps per second: 61, episode reward: 1.000, mean reward: 0.024 [0.000, 0.900], mean action: 4.537 [4.000, 18.000], mean observation: -0.633 [-1.009, 1.008], loss: 0.005127, mean_absolute_error: 0.922159, mean_q: 0.993573\n",
            "  92315/150000: episode: 1959, duration: 2.096s, episode steps: 150, steps per second: 72, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 8.440 [0.000, 20.000], mean observation: -0.632 [-1.009, 1.045], loss: 0.006536, mean_absolute_error: 0.921918, mean_q: 0.998489\n",
            "  92349/150000: episode: 1960, duration: 0.624s, episode steps: 34, steps per second: 54, episode reward: 1.000, mean reward: 0.029 [0.000, 0.900], mean action: 11.676 [1.000, 20.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.007510, mean_absolute_error: 0.927253, mean_q: 1.001555\n",
            "  92380/150000: episode: 1961, duration: 0.568s, episode steps: 31, steps per second: 55, episode reward: 1.000, mean reward: 0.032 [0.000, 0.900], mean action: 5.903 [4.000, 19.000], mean observation: -0.634 [-1.011, 1.036], loss: 0.008557, mean_absolute_error: 0.920396, mean_q: 0.992660\n",
            "  92483/150000: episode: 1962, duration: 1.525s, episode steps: 103, steps per second: 68, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 3.680 [0.000, 19.000], mean observation: -0.624 [-1.046, 9.380], loss: 0.006918, mean_absolute_error: 0.919298, mean_q: 0.993278\n",
            "  92589/150000: episode: 1963, duration: 1.547s, episode steps: 106, steps per second: 69, episode reward: 2.000, mean reward: 0.019 [0.000, 1.700], mean action: 5.415 [1.000, 19.000], mean observation: -0.636 [-1.011, 1.009], loss: 0.006597, mean_absolute_error: 0.915738, mean_q: 0.989485\n",
            "  92678/150000: episode: 1964, duration: 1.286s, episode steps: 89, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 7.281 [5.000, 20.000], mean observation: -0.632 [-1.009, 1.245], loss: 0.008191, mean_absolute_error: 0.917983, mean_q: 0.992381\n",
            "  92804/150000: episode: 1965, duration: 1.770s, episode steps: 126, steps per second: 71, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 5.937 [4.000, 19.000], mean observation: -0.634 [-1.009, 1.190], loss: 0.006559, mean_absolute_error: 0.924272, mean_q: 0.999973\n",
            "  92932/150000: episode: 1966, duration: 1.800s, episode steps: 128, steps per second: 71, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 11.289 [0.000, 20.000], mean observation: -0.626 [-1.009, 1.000], loss: 0.007771, mean_absolute_error: 0.930274, mean_q: 1.005611\n",
            "  93017/150000: episode: 1967, duration: 1.200s, episode steps: 85, steps per second: 71, episode reward: 0.200, mean reward: 0.002 [0.000, 0.200], mean action: 8.765 [2.000, 20.000], mean observation: -0.636 [-1.009, 1.000], loss: 0.008479, mean_absolute_error: 0.933583, mean_q: 0.997148\n",
            "  93054/150000: episode: 1968, duration: 0.616s, episode steps: 37, steps per second: 60, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 6.595 [4.000, 17.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.006555, mean_absolute_error: 0.929637, mean_q: 0.990044\n",
            "  93091/150000: episode: 1969, duration: 0.619s, episode steps: 37, steps per second: 60, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 6.378 [2.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007885, mean_absolute_error: 0.930964, mean_q: 0.996913\n",
            "  93103/150000: episode: 1970, duration: 0.274s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.006200, mean_absolute_error: 0.926722, mean_q: 1.003883\n",
            "  93116/150000: episode: 1971, duration: 0.296s, episode steps: 13, steps per second: 44, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 10.923 [1.000, 16.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.006875, mean_absolute_error: 0.920120, mean_q: 1.005831\n",
            "  93125/150000: episode: 1972, duration: 0.239s, episode steps: 9, steps per second: 38, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 13.556 [5.000, 15.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.002930, mean_absolute_error: 0.921087, mean_q: 0.996190\n",
            "  93157/150000: episode: 1973, duration: 0.548s, episode steps: 32, steps per second: 58, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 9.719 [4.000, 17.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.008326, mean_absolute_error: 0.930379, mean_q: 1.003986\n",
            "  93250/150000: episode: 1974, duration: 1.348s, episode steps: 93, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 6.753 [0.000, 19.000], mean observation: -0.632 [-1.011, 1.195], loss: 0.006914, mean_absolute_error: 0.928183, mean_q: 0.998652\n",
            "  93386/150000: episode: 1975, duration: 1.868s, episode steps: 136, steps per second: 73, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 5.169 [1.000, 15.000], mean observation: -0.630 [-1.009, 1.169], loss: 0.007388, mean_absolute_error: 0.932681, mean_q: 1.009164\n",
            "  93520/150000: episode: 1976, duration: 1.848s, episode steps: 134, steps per second: 73, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 8.149 [0.000, 19.000], mean observation: -0.631 [-1.011, 1.002], loss: 0.006359, mean_absolute_error: 0.934775, mean_q: 1.004060\n",
            "  93591/150000: episode: 1977, duration: 1.033s, episode steps: 71, steps per second: 69, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 4.183 [2.000, 18.000], mean observation: -0.638 [-1.011, 1.000], loss: 0.005896, mean_absolute_error: 0.934135, mean_q: 0.999415\n",
            "  93726/150000: episode: 1978, duration: 1.885s, episode steps: 135, steps per second: 72, episode reward: 2.000, mean reward: 0.015 [0.000, 1.100], mean action: 10.763 [0.000, 20.000], mean observation: -0.627 [-1.011, 3.981], loss: 0.010227, mean_absolute_error: 0.924280, mean_q: 0.993935\n",
            "  93875/150000: episode: 1979, duration: 2.049s, episode steps: 149, steps per second: 73, episode reward: 2.000, mean reward: 0.013 [0.000, 1.000], mean action: 9.463 [0.000, 20.000], mean observation: -0.629 [-1.011, 1.192], loss: 0.006543, mean_absolute_error: 0.917980, mean_q: 0.992832\n",
            "  93894/150000: episode: 1980, duration: 0.369s, episode steps: 19, steps per second: 51, episode reward: 2.000, mean reward: 0.105 [0.000, 1.000], mean action: 6.263 [5.000, 14.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.008369, mean_absolute_error: 0.918928, mean_q: 1.001021\n",
            "  93924/150000: episode: 1981, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 11.400 [4.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005912, mean_absolute_error: 0.923682, mean_q: 0.993080\n",
            "  94028/150000: episode: 1982, duration: 1.687s, episode steps: 104, steps per second: 62, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 8.567 [1.000, 19.000], mean observation: -0.619 [-1.031, 9.522], loss: 0.007185, mean_absolute_error: 0.926081, mean_q: 0.998357\n",
            "  94091/150000: episode: 1983, duration: 0.979s, episode steps: 63, steps per second: 64, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 8.762 [1.000, 20.000], mean observation: -0.613 [-1.011, 8.138], loss: 0.006315, mean_absolute_error: 0.930181, mean_q: 1.003176\n",
            "  94128/150000: episode: 1984, duration: 0.598s, episode steps: 37, steps per second: 62, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 12.568 [5.000, 17.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.007676, mean_absolute_error: 0.926145, mean_q: 0.998306\n",
            "  94178/150000: episode: 1985, duration: 0.803s, episode steps: 50, steps per second: 62, episode reward: 1.000, mean reward: 0.020 [0.000, 0.900], mean action: 12.180 [5.000, 17.000], mean observation: -0.631 [-1.009, 1.012], loss: 0.007912, mean_absolute_error: 0.926418, mean_q: 0.991341\n",
            "  94213/150000: episode: 1986, duration: 0.581s, episode steps: 35, steps per second: 60, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 6.457 [0.000, 19.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.004507, mean_absolute_error: 0.920196, mean_q: 0.983063\n",
            "  94287/150000: episode: 1987, duration: 1.072s, episode steps: 74, steps per second: 69, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 8.541 [5.000, 18.000], mean observation: -0.635 [-1.011, 1.161], loss: 0.005163, mean_absolute_error: 0.923429, mean_q: 1.000442\n",
            "  94406/150000: episode: 1988, duration: 1.675s, episode steps: 119, steps per second: 71, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 9.311 [2.000, 20.000], mean observation: -0.633 [-1.011, 1.422], loss: 0.007294, mean_absolute_error: 0.920359, mean_q: 0.996148\n",
            "  94428/150000: episode: 1989, duration: 0.413s, episode steps: 22, steps per second: 53, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 5.727 [2.000, 17.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.003878, mean_absolute_error: 0.931067, mean_q: 0.999034\n",
            "  94484/150000: episode: 1990, duration: 0.851s, episode steps: 56, steps per second: 66, episode reward: 0.900, mean reward: 0.016 [0.000, 0.900], mean action: 3.429 [2.000, 20.000], mean observation: -0.614 [-1.011, 8.892], loss: 0.004757, mean_absolute_error: 0.926787, mean_q: 0.997873\n",
            "  94884/150000: episode: 1991, duration: 5.355s, episode steps: 400, steps per second: 75, episode reward: 0.900, mean reward: 0.002 [0.000, 0.900], mean action: 6.312 [0.000, 20.000], mean observation: -0.640 [-1.009, 2.500], loss: 0.006980, mean_absolute_error: 0.918188, mean_q: 0.989310\n",
            "  94914/150000: episode: 1992, duration: 0.524s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.633 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.007611, mean_absolute_error: 0.913819, mean_q: 0.981513\n",
            "  94938/150000: episode: 1993, duration: 0.428s, episode steps: 24, steps per second: 56, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 7.458 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.006], loss: 0.004582, mean_absolute_error: 0.914908, mean_q: 0.983693\n",
            "  94965/150000: episode: 1994, duration: 0.481s, episode steps: 27, steps per second: 56, episode reward: 2.000, mean reward: 0.074 [0.000, 1.000], mean action: 13.222 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.006688, mean_absolute_error: 0.913440, mean_q: 0.973394\n",
            "  94995/150000: episode: 1995, duration: 0.502s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 10.433 [3.000, 15.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.004405, mean_absolute_error: 0.913912, mean_q: 0.985729\n",
            "  95282/150000: episode: 1996, duration: 3.879s, episode steps: 287, steps per second: 74, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 8.571 [0.000, 20.000], mean observation: -0.634 [-1.011, 5.099], loss: 0.007122, mean_absolute_error: 0.913162, mean_q: 0.986455\n",
            "  95298/150000: episode: 1997, duration: 0.328s, episode steps: 16, steps per second: 49, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 9.438 [5.000, 18.000], mean observation: -0.635 [-1.037, 1.000], loss: 0.002703, mean_absolute_error: 0.903715, mean_q: 0.972714\n",
            "  95307/150000: episode: 1998, duration: 0.229s, episode steps: 9, steps per second: 39, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 7.222 [5.000, 17.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.007275, mean_absolute_error: 0.907056, mean_q: 0.968894\n",
            "  95340/150000: episode: 1999, duration: 0.558s, episode steps: 33, steps per second: 59, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.909 [4.000, 20.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.008161, mean_absolute_error: 0.907966, mean_q: 0.976529\n",
            "  95452/150000: episode: 2000, duration: 1.573s, episode steps: 112, steps per second: 71, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 4.679 [0.000, 19.000], mean observation: -0.636 [-1.011, 1.402], loss: 0.004781, mean_absolute_error: 0.915413, mean_q: 0.986433\n",
            "  95482/150000: episode: 2001, duration: 0.519s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.033 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.009641, mean_absolute_error: 0.919557, mean_q: 0.989619\n",
            "  95569/150000: episode: 2002, duration: 1.222s, episode steps: 87, steps per second: 71, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 10.092 [2.000, 19.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007917, mean_absolute_error: 0.920304, mean_q: 0.989778\n",
            "  95599/150000: episode: 2003, duration: 0.523s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 10.700 [3.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004174, mean_absolute_error: 0.913569, mean_q: 0.984330\n",
            "  95629/150000: episode: 2004, duration: 0.506s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 11.000 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005661, mean_absolute_error: 0.920104, mean_q: 0.987414\n",
            "  95681/150000: episode: 2005, duration: 0.806s, episode steps: 52, steps per second: 65, episode reward: 2.000, mean reward: 0.038 [0.000, 1.000], mean action: 8.885 [3.000, 17.000], mean observation: -0.631 [-1.011, 1.000], loss: 0.007657, mean_absolute_error: 0.920302, mean_q: 0.983917\n",
            "  95725/150000: episode: 2006, duration: 0.691s, episode steps: 44, steps per second: 64, episode reward: 0.900, mean reward: 0.020 [0.000, 0.900], mean action: 7.932 [2.000, 19.000], mean observation: -0.600 [-1.059, 9.532], loss: 0.007866, mean_absolute_error: 0.917147, mean_q: 0.992440\n",
            "  95743/150000: episode: 2007, duration: 0.378s, episode steps: 18, steps per second: 48, episode reward: 0.900, mean reward: 0.050 [0.000, 0.900], mean action: 7.889 [4.000, 19.000], mean observation: -0.630 [-1.009, 1.997], loss: 0.007581, mean_absolute_error: 0.914290, mean_q: 0.991567\n",
            "  95755/150000: episode: 2008, duration: 0.270s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 8.333 [5.000, 20.000], mean observation: -0.633 [-1.041, 1.164], loss: 0.009014, mean_absolute_error: 0.912444, mean_q: 0.991046\n",
            "  95813/150000: episode: 2009, duration: 1.048s, episode steps: 58, steps per second: 55, episode reward: 0.900, mean reward: 0.016 [0.000, 0.900], mean action: 11.983 [3.000, 20.000], mean observation: -0.629 [-1.011, 1.426], loss: 0.008712, mean_absolute_error: 0.912637, mean_q: 0.989592\n",
            "  95836/150000: episode: 2010, duration: 0.429s, episode steps: 23, steps per second: 54, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 12.957 [5.000, 18.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.005924, mean_absolute_error: 0.919688, mean_q: 0.987587\n",
            "  95852/150000: episode: 2011, duration: 0.328s, episode steps: 16, steps per second: 49, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 5.250 [2.000, 12.000], mean observation: -0.635 [-1.017, 1.000], loss: 0.006012, mean_absolute_error: 0.921480, mean_q: 1.003834\n",
            "  95976/150000: episode: 2012, duration: 1.727s, episode steps: 124, steps per second: 72, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 10.863 [1.000, 20.000], mean observation: -0.625 [-1.009, 1.071], loss: 0.008133, mean_absolute_error: 0.916031, mean_q: 0.992965\n",
            "  96039/150000: episode: 2013, duration: 0.990s, episode steps: 63, steps per second: 64, episode reward: 0.900, mean reward: 0.014 [0.000, 0.900], mean action: 11.730 [2.000, 20.000], mean observation: -0.618 [-1.011, 7.668], loss: 0.007407, mean_absolute_error: 0.923559, mean_q: 1.001568\n",
            "  96077/150000: episode: 2014, duration: 0.666s, episode steps: 38, steps per second: 57, episode reward: 1.000, mean reward: 0.026 [0.000, 0.900], mean action: 5.868 [0.000, 20.000], mean observation: -0.632 [-1.011, 1.014], loss: 0.009521, mean_absolute_error: 0.926897, mean_q: 0.999480\n",
            "  96094/150000: episode: 2015, duration: 0.374s, episode steps: 17, steps per second: 45, episode reward: 0.900, mean reward: 0.053 [0.000, 0.900], mean action: 13.706 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.013431, mean_absolute_error: 0.922785, mean_q: 0.994001\n",
            "  96211/150000: episode: 2016, duration: 1.627s, episode steps: 117, steps per second: 72, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 5.872 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.125], loss: 0.007354, mean_absolute_error: 0.922773, mean_q: 0.996453\n",
            "  96279/150000: episode: 2017, duration: 1.025s, episode steps: 68, steps per second: 66, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 4.838 [0.000, 15.000], mean observation: -0.613 [-1.009, 8.366], loss: 0.006296, mean_absolute_error: 0.928853, mean_q: 1.010975\n",
            "  96398/150000: episode: 2018, duration: 1.682s, episode steps: 119, steps per second: 71, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 7.277 [0.000, 19.000], mean observation: -0.631 [-1.009, 1.275], loss: 0.005935, mean_absolute_error: 0.932482, mean_q: 1.003351\n",
            "  96431/150000: episode: 2019, duration: 0.537s, episode steps: 33, steps per second: 61, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 1.576 [0.000, 19.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.005450, mean_absolute_error: 0.932380, mean_q: 1.005930\n",
            "  96457/150000: episode: 2020, duration: 0.467s, episode steps: 26, steps per second: 56, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 4.038 [0.000, 16.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.008069, mean_absolute_error: 0.927730, mean_q: 0.994960\n",
            "  96466/150000: episode: 2021, duration: 0.213s, episode steps: 9, steps per second: 42, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 6.556 [5.000, 12.000], mean observation: -0.634 [-1.009, 1.024], loss: 0.006181, mean_absolute_error: 0.936046, mean_q: 1.014071\n",
            "  96508/150000: episode: 2022, duration: 0.672s, episode steps: 42, steps per second: 62, episode reward: 2.000, mean reward: 0.048 [0.000, 1.000], mean action: 6.881 [4.000, 20.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.005942, mean_absolute_error: 0.933018, mean_q: 1.011023\n",
            "  96518/150000: episode: 2023, duration: 0.256s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 5.700 [4.000, 12.000], mean observation: -0.636 [-1.011, 1.018], loss: 0.005410, mean_absolute_error: 0.937480, mean_q: 1.006318\n",
            "  96532/150000: episode: 2024, duration: 0.308s, episode steps: 14, steps per second: 45, episode reward: 2.000, mean reward: 0.143 [0.000, 1.000], mean action: 9.571 [4.000, 17.000], mean observation: -0.634 [-1.039, 1.173], loss: 0.007127, mean_absolute_error: 0.937595, mean_q: 1.010639\n",
            "  96542/150000: episode: 2025, duration: 0.250s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.200 [4.000, 17.000], mean observation: -0.637 [-1.036, 1.008], loss: 0.017794, mean_absolute_error: 0.930663, mean_q: 1.002431\n",
            "  96661/150000: episode: 2026, duration: 1.685s, episode steps: 119, steps per second: 71, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 9.597 [1.000, 18.000], mean observation: -0.633 [-1.009, 1.734], loss: 0.006432, mean_absolute_error: 0.920938, mean_q: 0.997052\n",
            "  96790/150000: episode: 2027, duration: 1.829s, episode steps: 129, steps per second: 71, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 9.341 [2.000, 19.000], mean observation: -0.631 [-1.009, 2.937], loss: 0.006556, mean_absolute_error: 0.926129, mean_q: 1.002597\n",
            "  96859/150000: episode: 2028, duration: 0.995s, episode steps: 69, steps per second: 69, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 12.623 [0.000, 14.000], mean observation: -0.611 [-1.084, 9.583], loss: 0.006958, mean_absolute_error: 0.931355, mean_q: 1.004242\n",
            "  96883/150000: episode: 2029, duration: 0.440s, episode steps: 24, steps per second: 55, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 12.125 [4.000, 14.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007241, mean_absolute_error: 0.937849, mean_q: 1.011953\n",
            "  96897/150000: episode: 2030, duration: 0.331s, episode steps: 14, steps per second: 42, episode reward: 2.000, mean reward: 0.143 [0.000, 1.000], mean action: 13.143 [5.000, 20.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.004209, mean_absolute_error: 0.932369, mean_q: 1.017444\n",
            "  96944/150000: episode: 2031, duration: 0.757s, episode steps: 47, steps per second: 62, episode reward: 0.900, mean reward: 0.019 [0.000, 0.900], mean action: 12.234 [0.000, 17.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.005863, mean_absolute_error: 0.933479, mean_q: 1.000669\n",
            "  96956/150000: episode: 2032, duration: 0.302s, episode steps: 12, steps per second: 40, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.005], loss: 0.010978, mean_absolute_error: 0.931381, mean_q: 1.001476\n",
            "  96968/150000: episode: 2033, duration: 0.274s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.030], loss: 0.004991, mean_absolute_error: 0.921183, mean_q: 0.992955\n",
            "  96981/150000: episode: 2034, duration: 0.290s, episode steps: 13, steps per second: 45, episode reward: 2.000, mean reward: 0.154 [0.000, 2.000], mean action: 12.000 [12.000, 12.000], mean observation: -0.634 [-1.038, 1.000], loss: 0.003420, mean_absolute_error: 0.914941, mean_q: 0.986937\n",
            "  96996/150000: episode: 2035, duration: 0.327s, episode steps: 15, steps per second: 46, episode reward: 2.000, mean reward: 0.133 [0.000, 1.100], mean action: 6.800 [2.000, 14.000], mean observation: -0.634 [-1.011, 1.015], loss: 0.014313, mean_absolute_error: 0.920826, mean_q: 0.993367\n",
            "  97012/150000: episode: 2036, duration: 0.337s, episode steps: 16, steps per second: 48, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 5.438 [5.000, 12.000], mean observation: -0.635 [-1.040, 1.000], loss: 0.007327, mean_absolute_error: 0.929878, mean_q: 1.013228\n",
            "  97048/150000: episode: 2037, duration: 0.585s, episode steps: 36, steps per second: 61, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 5.306 [3.000, 16.000], mean observation: -0.633 [-1.011, 1.007], loss: 0.009436, mean_absolute_error: 0.932594, mean_q: 1.015200\n",
            "  97076/150000: episode: 2038, duration: 0.496s, episode steps: 28, steps per second: 56, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 4.607 [0.000, 13.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007756, mean_absolute_error: 0.935126, mean_q: 1.009207\n",
            "  97157/150000: episode: 2039, duration: 1.187s, episode steps: 81, steps per second: 68, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 7.358 [0.000, 19.000], mean observation: -0.633 [-1.009, 1.000], loss: 0.006232, mean_absolute_error: 0.933299, mean_q: 1.003239\n",
            "  97220/150000: episode: 2040, duration: 0.948s, episode steps: 63, steps per second: 66, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 6.429 [0.000, 18.000], mean observation: -0.613 [-1.022, 7.952], loss: 0.006504, mean_absolute_error: 0.938669, mean_q: 1.010643\n",
            "  97251/150000: episode: 2041, duration: 0.520s, episode steps: 31, steps per second: 60, episode reward: 2.000, mean reward: 0.065 [0.000, 1.000], mean action: 8.774 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.008015, mean_absolute_error: 0.935727, mean_q: 1.015365\n",
            "  97394/150000: episode: 2042, duration: 1.996s, episode steps: 143, steps per second: 72, episode reward: 2.000, mean reward: 0.014 [0.000, 1.000], mean action: 6.448 [0.000, 20.000], mean observation: -0.630 [-1.011, 1.085], loss: 0.006509, mean_absolute_error: 0.934703, mean_q: 1.010021\n",
            "  97515/150000: episode: 2043, duration: 1.703s, episode steps: 121, steps per second: 71, episode reward: 2.000, mean reward: 0.017 [0.000, 1.000], mean action: 6.050 [3.000, 18.000], mean observation: -0.631 [-1.015, 1.113], loss: 0.006501, mean_absolute_error: 0.929527, mean_q: 1.005212\n",
            "  97528/150000: episode: 2044, duration: 0.294s, episode steps: 13, steps per second: 44, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 8.000 [2.000, 12.000], mean observation: -0.633 [-1.011, 1.443], loss: 0.009444, mean_absolute_error: 0.933111, mean_q: 1.003161\n",
            "  97558/150000: episode: 2045, duration: 0.531s, episode steps: 30, steps per second: 56, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.367 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007212, mean_absolute_error: 0.938890, mean_q: 1.023130\n",
            "  97790/150000: episode: 2046, duration: 3.144s, episode steps: 232, steps per second: 74, episode reward: 1.000, mean reward: 0.004 [0.000, 0.900], mean action: 6.483 [0.000, 20.000], mean observation: -0.631 [-1.011, 1.334], loss: 0.006292, mean_absolute_error: 0.939758, mean_q: 1.013117\n",
            "  97825/150000: episode: 2047, duration: 0.572s, episode steps: 35, steps per second: 61, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 7.171 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.005], loss: 0.006256, mean_absolute_error: 0.942634, mean_q: 1.013592\n",
            "  97837/150000: episode: 2048, duration: 0.288s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.583 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.066], loss: 0.005038, mean_absolute_error: 0.938856, mean_q: 1.015859\n",
            "  97849/150000: episode: 2049, duration: 0.269s, episode steps: 12, steps per second: 45, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.000 [5.000, 16.000], mean observation: -0.636 [-1.041, 1.012], loss: 0.005872, mean_absolute_error: 0.941584, mean_q: 1.016207\n",
            "  97861/150000: episode: 2050, duration: 0.280s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.250 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.002], loss: 0.004069, mean_absolute_error: 0.940836, mean_q: 1.010242\n",
            "  97917/150000: episode: 2051, duration: 0.858s, episode steps: 56, steps per second: 65, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 5.732 [0.000, 18.000], mean observation: -0.609 [-1.017, 8.381], loss: 0.006145, mean_absolute_error: 0.940348, mean_q: 1.007888\n",
            "  98030/150000: episode: 2052, duration: 1.625s, episode steps: 113, steps per second: 70, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 4.611 [0.000, 20.000], mean observation: -0.631 [-1.009, 1.279], loss: 0.008019, mean_absolute_error: 0.942362, mean_q: 1.014757\n",
            "  98056/150000: episode: 2053, duration: 0.484s, episode steps: 26, steps per second: 54, episode reward: 1.000, mean reward: 0.038 [0.000, 0.900], mean action: 5.269 [3.000, 20.000], mean observation: -0.634 [-1.009, 1.097], loss: 0.007012, mean_absolute_error: 0.942052, mean_q: 1.014311\n",
            "  98125/150000: episode: 2054, duration: 1.165s, episode steps: 69, steps per second: 59, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 12.420 [0.000, 20.000], mean observation: -0.642 [-1.060, 1.000], loss: 0.008211, mean_absolute_error: 0.937249, mean_q: 1.012378\n",
            "  98168/150000: episode: 2055, duration: 0.721s, episode steps: 43, steps per second: 60, episode reward: 1.000, mean reward: 0.023 [0.000, 0.900], mean action: 4.349 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.004962, mean_absolute_error: 0.935795, mean_q: 1.011857\n",
            "  98198/150000: episode: 2056, duration: 0.499s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.700 [0.000, 13.000], mean observation: -0.634 [-1.011, 1.015], loss: 0.006782, mean_absolute_error: 0.935287, mean_q: 1.004265\n",
            "  98281/150000: episode: 2057, duration: 1.229s, episode steps: 83, steps per second: 68, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 10.675 [3.000, 20.000], mean observation: -0.632 [-1.011, 1.028], loss: 0.008513, mean_absolute_error: 0.930997, mean_q: 1.006074\n",
            "  98311/150000: episode: 2058, duration: 0.504s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.900 [2.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005472, mean_absolute_error: 0.931271, mean_q: 1.025082\n",
            "  98348/150000: episode: 2059, duration: 0.598s, episode steps: 37, steps per second: 62, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 9.514 [5.000, 18.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.009395, mean_absolute_error: 0.934573, mean_q: 1.003800\n",
            "  98451/150000: episode: 2060, duration: 1.447s, episode steps: 103, steps per second: 71, episode reward: 2.000, mean reward: 0.019 [0.000, 1.000], mean action: 11.699 [5.000, 20.000], mean observation: -0.631 [-1.011, 1.271], loss: 0.005856, mean_absolute_error: 0.931687, mean_q: 1.010705\n",
            "  98460/150000: episode: 2061, duration: 0.232s, episode steps: 9, steps per second: 39, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 9.889 [0.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.006022, mean_absolute_error: 0.930751, mean_q: 1.009028\n",
            "  98546/150000: episode: 2062, duration: 1.218s, episode steps: 86, steps per second: 71, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 8.047 [5.000, 19.000], mean observation: -0.636 [-1.009, 1.406], loss: 0.006070, mean_absolute_error: 0.938404, mean_q: 1.009621\n",
            "  98647/150000: episode: 2063, duration: 1.398s, episode steps: 101, steps per second: 72, episode reward: 2.000, mean reward: 0.020 [0.000, 1.100], mean action: 9.842 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007726, mean_absolute_error: 0.945381, mean_q: 1.019992\n",
            "  98659/150000: episode: 2064, duration: 0.282s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 7.333 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.002], loss: 0.014664, mean_absolute_error: 0.945698, mean_q: 1.018138\n",
            "  98689/150000: episode: 2065, duration: 0.508s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.633 [1.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004896, mean_absolute_error: 0.936105, mean_q: 1.001078\n",
            "  98719/150000: episode: 2066, duration: 0.503s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.100 [5.000, 8.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.008756, mean_absolute_error: 0.946066, mean_q: 1.026673\n",
            "  98782/150000: episode: 2067, duration: 0.978s, episode steps: 63, steps per second: 64, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 7.444 [1.000, 20.000], mean observation: -0.615 [-1.011, 8.114], loss: 0.007130, mean_absolute_error: 0.944046, mean_q: 1.020842\n",
            "  98791/150000: episode: 2068, duration: 0.219s, episode steps: 9, steps per second: 41, episode reward: 2.000, mean reward: 0.222 [0.000, 2.000], mean action: 6.667 [6.000, 12.000], mean observation: -0.632 [-1.011, 1.019], loss: 0.004474, mean_absolute_error: 0.943390, mean_q: 1.027576\n",
            "  98932/150000: episode: 2069, duration: 2.001s, episode steps: 141, steps per second: 70, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 6.227 [1.000, 16.000], mean observation: -0.629 [-1.009, 1.012], loss: 0.005005, mean_absolute_error: 0.958751, mean_q: 1.044130\n",
            "  98972/150000: episode: 2070, duration: 0.670s, episode steps: 40, steps per second: 60, episode reward: 1.000, mean reward: 0.025 [0.000, 0.900], mean action: 5.950 [5.000, 6.000], mean observation: -0.632 [-1.009, 1.015], loss: 0.006877, mean_absolute_error: 0.981207, mean_q: 1.067652\n",
            "  99001/150000: episode: 2071, duration: 0.529s, episode steps: 29, steps per second: 55, episode reward: 1.000, mean reward: 0.034 [0.000, 0.900], mean action: 5.931 [5.000, 16.000], mean observation: -0.633 [-1.009, 1.005], loss: 0.008166, mean_absolute_error: 0.979050, mean_q: 1.060679\n",
            "  99040/150000: episode: 2072, duration: 0.605s, episode steps: 39, steps per second: 64, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 5.590 [2.000, 19.000], mean observation: -0.632 [-1.009, 1.016], loss: 0.007941, mean_absolute_error: 0.986572, mean_q: 1.061647\n",
            "  99068/150000: episode: 2073, duration: 0.477s, episode steps: 28, steps per second: 59, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 5.464 [0.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008310, mean_absolute_error: 0.978811, mean_q: 1.068807\n",
            "  99148/150000: episode: 2074, duration: 1.146s, episode steps: 80, steps per second: 70, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 3.962 [2.000, 19.000], mean observation: -0.639 [-1.011, 1.312], loss: 0.007278, mean_absolute_error: 0.987143, mean_q: 1.071844\n",
            "  99241/150000: episode: 2075, duration: 1.355s, episode steps: 93, steps per second: 69, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 5.871 [2.000, 20.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.009716, mean_absolute_error: 0.984245, mean_q: 1.055979\n",
            "  99257/150000: episode: 2076, duration: 0.335s, episode steps: 16, steps per second: 48, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 6.438 [5.000, 18.000], mean observation: -0.635 [-1.015, 1.000], loss: 0.005390, mean_absolute_error: 0.977304, mean_q: 1.063341\n",
            "  99294/150000: episode: 2077, duration: 0.622s, episode steps: 37, steps per second: 60, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.459 [0.000, 17.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.006437, mean_absolute_error: 0.979219, mean_q: 1.050137\n",
            "  99324/150000: episode: 2078, duration: 0.525s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.833 [0.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007784, mean_absolute_error: 0.983985, mean_q: 1.058376\n",
            "  99354/150000: episode: 2079, duration: 0.528s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.533 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.015], loss: 0.007758, mean_absolute_error: 0.975968, mean_q: 1.063491\n",
            "  99422/150000: episode: 2080, duration: 1.103s, episode steps: 68, steps per second: 62, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 5.750 [0.000, 15.000], mean observation: -0.615 [-1.017, 8.381], loss: 0.006737, mean_absolute_error: 0.987920, mean_q: 1.066292\n",
            "  99448/150000: episode: 2081, duration: 0.480s, episode steps: 26, steps per second: 54, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 10.000 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.002], loss: 0.006464, mean_absolute_error: 0.987649, mean_q: 1.054481\n",
            "  99478/150000: episode: 2082, duration: 0.523s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007778, mean_absolute_error: 0.994387, mean_q: 1.072245\n",
            "  99572/150000: episode: 2083, duration: 1.337s, episode steps: 94, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 9.032 [3.000, 17.000], mean observation: -0.618 [-1.030, 8.236], loss: 0.005088, mean_absolute_error: 0.995061, mean_q: 1.068696\n",
            "  99657/150000: episode: 2084, duration: 1.226s, episode steps: 85, steps per second: 69, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 6.353 [0.000, 19.000], mean observation: -0.634 [-1.011, 1.071], loss: 0.007369, mean_absolute_error: 0.996032, mean_q: 1.072396\n",
            "  99703/150000: episode: 2085, duration: 0.720s, episode steps: 46, steps per second: 64, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 9.087 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.005], loss: 0.009563, mean_absolute_error: 0.996823, mean_q: 1.071744\n",
            "  99738/150000: episode: 2086, duration: 0.568s, episode steps: 35, steps per second: 62, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 5.400 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.008636, mean_absolute_error: 0.994027, mean_q: 1.076798\n",
            "  99817/150000: episode: 2087, duration: 1.121s, episode steps: 79, steps per second: 70, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 7.456 [0.000, 20.000], mean observation: -0.637 [-1.011, 1.095], loss: 0.008669, mean_absolute_error: 0.988597, mean_q: 1.062007\n",
            "  99897/150000: episode: 2088, duration: 1.132s, episode steps: 80, steps per second: 71, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 6.350 [2.000, 18.000], mean observation: -0.636 [-1.011, 1.238], loss: 0.005881, mean_absolute_error: 0.991976, mean_q: 1.069227\n",
            " 100038/150000: episode: 2089, duration: 1.956s, episode steps: 141, steps per second: 72, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 12.830 [2.000, 20.000], mean observation: -0.630 [-1.009, 1.000], loss: 0.006009, mean_absolute_error: 0.986616, mean_q: 1.053711\n",
            " 100050/150000: episode: 2090, duration: 0.280s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 8.167 [5.000, 20.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.007377, mean_absolute_error: 0.975078, mean_q: 1.038834\n",
            " 100084/150000: episode: 2091, duration: 0.592s, episode steps: 34, steps per second: 57, episode reward: 1.000, mean reward: 0.029 [0.000, 0.900], mean action: 6.029 [4.000, 12.000], mean observation: -0.633 [-1.011, 1.002], loss: 0.008862, mean_absolute_error: 0.981405, mean_q: 1.059015\n",
            " 100096/150000: episode: 2092, duration: 0.279s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.006376, mean_absolute_error: 0.975744, mean_q: 1.056278\n",
            " 100108/150000: episode: 2093, duration: 0.297s, episode steps: 12, steps per second: 40, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 7.417 [4.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.008796, mean_absolute_error: 0.976881, mean_q: 1.052959\n",
            " 100122/150000: episode: 2094, duration: 0.299s, episode steps: 14, steps per second: 47, episode reward: 2.000, mean reward: 0.143 [0.000, 1.100], mean action: 6.500 [2.000, 19.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.008371, mean_absolute_error: 0.975175, mean_q: 1.048741\n",
            " 100286/150000: episode: 2095, duration: 2.277s, episode steps: 164, steps per second: 72, episode reward: 1.000, mean reward: 0.006 [0.000, 0.900], mean action: 7.030 [0.000, 19.000], mean observation: -0.623 [-1.081, 10.493], loss: 0.006802, mean_absolute_error: 0.975975, mean_q: 1.050981\n",
            " 100370/150000: episode: 2096, duration: 1.175s, episode steps: 84, steps per second: 71, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 10.976 [2.000, 20.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.008583, mean_absolute_error: 0.966174, mean_q: 1.042898\n",
            " 100400/150000: episode: 2097, duration: 0.506s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.800 [4.000, 15.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.009870, mean_absolute_error: 0.961029, mean_q: 1.035588\n",
            " 100505/150000: episode: 2098, duration: 1.459s, episode steps: 105, steps per second: 72, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 8.333 [0.000, 20.000], mean observation: -0.632 [-1.009, 1.000], loss: 0.008206, mean_absolute_error: 0.961878, mean_q: 1.041333\n",
            " 100559/150000: episode: 2099, duration: 0.814s, episode steps: 54, steps per second: 66, episode reward: 0.900, mean reward: 0.017 [0.000, 0.900], mean action: 6.389 [2.000, 15.000], mean observation: -0.614 [-1.011, 7.668], loss: 0.007752, mean_absolute_error: 0.969005, mean_q: 1.047066\n",
            " 100589/150000: episode: 2100, duration: 0.499s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.900 [5.000, 15.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.007059, mean_absolute_error: 0.972460, mean_q: 1.047459\n",
            " 100625/150000: episode: 2101, duration: 0.585s, episode steps: 36, steps per second: 61, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 5.639 [2.000, 18.000], mean observation: -0.633 [-1.011, 1.006], loss: 0.006608, mean_absolute_error: 0.972830, mean_q: 1.040566\n",
            " 100704/150000: episode: 2102, duration: 1.132s, episode steps: 79, steps per second: 70, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 6.152 [3.000, 16.000], mean observation: -0.634 [-1.009, 1.000], loss: 0.008256, mean_absolute_error: 0.968822, mean_q: 1.045054\n",
            " 100739/150000: episode: 2103, duration: 0.597s, episode steps: 35, steps per second: 59, episode reward: 1.000, mean reward: 0.029 [0.000, 0.900], mean action: 6.171 [5.000, 18.000], mean observation: -0.633 [-1.009, 1.012], loss: 0.010144, mean_absolute_error: 0.966614, mean_q: 1.042152\n",
            " 100773/150000: episode: 2104, duration: 0.572s, episode steps: 34, steps per second: 59, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 5.618 [5.000, 14.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.005929, mean_absolute_error: 0.962530, mean_q: 1.050179\n",
            " 100847/150000: episode: 2105, duration: 1.098s, episode steps: 74, steps per second: 67, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 6.068 [1.000, 19.000], mean observation: -0.636 [-1.009, 1.192], loss: 0.005774, mean_absolute_error: 0.976524, mean_q: 1.055932\n",
            " 100862/150000: episode: 2106, duration: 0.293s, episode steps: 15, steps per second: 51, episode reward: 2.000, mean reward: 0.133 [0.000, 1.000], mean action: 6.400 [5.000, 12.000], mean observation: -0.634 [-1.009, 1.022], loss: 0.003965, mean_absolute_error: 0.977094, mean_q: 1.058908\n",
            " 100959/150000: episode: 2107, duration: 1.365s, episode steps: 97, steps per second: 71, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 6.928 [5.000, 19.000], mean observation: -0.636 [-1.009, 1.293], loss: 0.006274, mean_absolute_error: 0.977754, mean_q: 1.053963\n",
            " 100989/150000: episode: 2108, duration: 0.509s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.867 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.009631, mean_absolute_error: 0.977158, mean_q: 1.050980\n",
            " 101092/150000: episode: 2109, duration: 1.433s, episode steps: 103, steps per second: 72, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 4.922 [1.000, 18.000], mean observation: -0.636 [-1.011, 1.182], loss: 0.008775, mean_absolute_error: 0.972913, mean_q: 1.048562\n",
            " 101166/150000: episode: 2110, duration: 1.118s, episode steps: 74, steps per second: 66, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 6.486 [1.000, 19.000], mean observation: -0.618 [-1.009, 7.975], loss: 0.007389, mean_absolute_error: 0.975049, mean_q: 1.048470\n",
            " 101216/150000: episode: 2111, duration: 0.779s, episode steps: 50, steps per second: 64, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 7.760 [0.000, 15.000], mean observation: -0.612 [-1.011, 7.651], loss: 0.005612, mean_absolute_error: 0.971727, mean_q: 1.045268\n",
            " 101514/150000: episode: 2112, duration: 3.989s, episode steps: 298, steps per second: 75, episode reward: 0.900, mean reward: 0.003 [0.000, 0.900], mean action: 5.785 [0.000, 19.000], mean observation: -0.624 [-1.018, 8.028], loss: 0.007847, mean_absolute_error: 0.969308, mean_q: 1.046742\n",
            " 101562/150000: episode: 2113, duration: 0.776s, episode steps: 48, steps per second: 62, episode reward: 2.000, mean reward: 0.042 [0.000, 1.000], mean action: 4.250 [0.000, 16.000], mean observation: -0.633 [-1.011, 1.008], loss: 0.008872, mean_absolute_error: 0.966608, mean_q: 1.044203\n",
            " 101592/150000: episode: 2114, duration: 0.505s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.933 [4.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005236, mean_absolute_error: 0.974451, mean_q: 1.050696\n",
            " 101616/150000: episode: 2115, duration: 0.431s, episode steps: 24, steps per second: 56, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 7.167 [0.000, 12.000], mean observation: -0.634 [-1.011, 1.021], loss: 0.009800, mean_absolute_error: 0.977899, mean_q: 1.053018\n",
            " 101625/150000: episode: 2116, duration: 0.218s, episode steps: 9, steps per second: 41, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 5.778 [5.000, 12.000], mean observation: -0.634 [-1.009, 1.023], loss: 0.006684, mean_absolute_error: 0.980727, mean_q: 1.058501\n",
            " 101727/150000: episode: 2117, duration: 1.406s, episode steps: 102, steps per second: 73, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 11.755 [4.000, 19.000], mean observation: -0.631 [-1.011, 1.098], loss: 0.004296, mean_absolute_error: 0.975164, mean_q: 1.045061\n",
            " 101824/150000: episode: 2118, duration: 1.358s, episode steps: 97, steps per second: 71, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 4.412 [0.000, 19.000], mean observation: -0.632 [-1.009, 1.254], loss: 0.007992, mean_absolute_error: 0.969443, mean_q: 1.047339\n",
            " 101978/150000: episode: 2119, duration: 2.130s, episode steps: 154, steps per second: 72, episode reward: 0.900, mean reward: 0.006 [0.000, 0.900], mean action: 3.773 [0.000, 19.000], mean observation: -0.638 [-1.028, 2.323], loss: 0.008489, mean_absolute_error: 0.982454, mean_q: 1.061064\n",
            " 102056/150000: episode: 2120, duration: 1.135s, episode steps: 78, steps per second: 69, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 6.526 [2.000, 19.000], mean observation: -0.615 [-1.009, 8.177], loss: 0.005925, mean_absolute_error: 0.978205, mean_q: 1.055125\n",
            " 102393/150000: episode: 2121, duration: 4.532s, episode steps: 337, steps per second: 74, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 9.798 [0.000, 20.000], mean observation: -0.629 [-1.011, 1.941], loss: 0.006991, mean_absolute_error: 0.970341, mean_q: 1.042804\n",
            " 102430/150000: episode: 2122, duration: 0.604s, episode steps: 37, steps per second: 61, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.054 [0.000, 6.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.006734, mean_absolute_error: 0.963439, mean_q: 1.037827\n",
            " 102529/150000: episode: 2123, duration: 1.390s, episode steps: 99, steps per second: 71, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 5.667 [1.000, 18.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.006452, mean_absolute_error: 0.964575, mean_q: 1.041301\n",
            " 102559/150000: episode: 2124, duration: 0.507s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.333 [5.000, 15.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006054, mean_absolute_error: 0.959330, mean_q: 1.034539\n",
            " 102603/150000: episode: 2125, duration: 0.671s, episode steps: 44, steps per second: 66, episode reward: 2.000, mean reward: 0.045 [0.000, 1.100], mean action: 7.568 [2.000, 19.000], mean observation: -0.633 [-1.011, 2.160], loss: 0.007891, mean_absolute_error: 0.964996, mean_q: 1.032060\n",
            " 102673/150000: episode: 2126, duration: 1.035s, episode steps: 70, steps per second: 68, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 4.429 [0.000, 11.000], mean observation: -0.614 [-1.035, 8.126], loss: 0.007240, mean_absolute_error: 0.960848, mean_q: 1.036206\n",
            " 102695/150000: episode: 2127, duration: 0.399s, episode steps: 22, steps per second: 55, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 7.045 [5.000, 20.000], mean observation: -0.634 [-1.009, 1.018], loss: 0.006804, mean_absolute_error: 0.958572, mean_q: 1.030183\n",
            " 102816/150000: episode: 2128, duration: 1.655s, episode steps: 121, steps per second: 73, episode reward: 0.900, mean reward: 0.007 [0.000, 0.900], mean action: 6.149 [1.000, 19.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.007402, mean_absolute_error: 0.953756, mean_q: 1.021371\n",
            " 103216/150000: episode: 2129, duration: 5.204s, episode steps: 400, steps per second: 77, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 6.425 [0.000, 19.000], mean observation: -0.639 [-1.011, 9.305], loss: 0.006799, mean_absolute_error: 0.942667, mean_q: 1.014901\n",
            " 103274/150000: episode: 2130, duration: 0.895s, episode steps: 58, steps per second: 65, episode reward: 1.000, mean reward: 0.017 [0.000, 0.900], mean action: 5.293 [1.000, 20.000], mean observation: -0.611 [-1.017, 8.381], loss: 0.006930, mean_absolute_error: 0.936574, mean_q: 1.009222\n",
            " 103310/150000: episode: 2131, duration: 0.584s, episode steps: 36, steps per second: 62, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 5.500 [5.000, 18.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.006256, mean_absolute_error: 0.933410, mean_q: 1.007213\n",
            " 103404/150000: episode: 2132, duration: 1.374s, episode steps: 94, steps per second: 68, episode reward: 0.200, mean reward: 0.002 [0.000, 0.200], mean action: 6.245 [0.000, 20.000], mean observation: -0.634 [-1.009, 1.683], loss: 0.007615, mean_absolute_error: 0.937311, mean_q: 1.006038\n",
            " 103434/150000: episode: 2133, duration: 0.496s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 10.167 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007021, mean_absolute_error: 0.932276, mean_q: 0.994544\n",
            " 103503/150000: episode: 2134, duration: 1.043s, episode steps: 69, steps per second: 66, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 7.855 [5.000, 20.000], mean observation: -0.616 [-1.017, 8.381], loss: 0.006782, mean_absolute_error: 0.932290, mean_q: 1.003848\n",
            " 103536/150000: episode: 2135, duration: 0.550s, episode steps: 33, steps per second: 60, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.303 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.007007, mean_absolute_error: 0.929424, mean_q: 1.015743\n",
            " 103573/150000: episode: 2136, duration: 0.593s, episode steps: 37, steps per second: 62, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 8.135 [3.000, 15.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.006195, mean_absolute_error: 0.934508, mean_q: 1.004976\n",
            " 103684/150000: episode: 2137, duration: 1.524s, episode steps: 111, steps per second: 73, episode reward: 2.000, mean reward: 0.018 [0.000, 1.000], mean action: 7.595 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.036], loss: 0.006989, mean_absolute_error: 0.939500, mean_q: 1.010889\n",
            " 103700/150000: episode: 2138, duration: 0.347s, episode steps: 16, steps per second: 46, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 14.188 [5.000, 19.000], mean observation: -0.635 [-1.039, 1.000], loss: 0.007465, mean_absolute_error: 0.945391, mean_q: 1.014436\n",
            " 103712/150000: episode: 2139, duration: 0.282s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.250 [5.000, 16.000], mean observation: -0.634 [-1.035, 1.041], loss: 0.003727, mean_absolute_error: 0.947273, mean_q: 1.022163\n",
            " 103724/150000: episode: 2140, duration: 0.310s, episode steps: 12, steps per second: 39, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.917 [5.000, 18.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.007945, mean_absolute_error: 0.947487, mean_q: 1.029704\n",
            " 103754/150000: episode: 2141, duration: 0.523s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.667 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.007], loss: 0.006521, mean_absolute_error: 0.937660, mean_q: 1.013562\n",
            " 103967/150000: episode: 2142, duration: 2.921s, episode steps: 213, steps per second: 73, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 7.038 [1.000, 19.000], mean observation: -0.635 [-1.011, 1.348], loss: 0.006950, mean_absolute_error: 0.940329, mean_q: 1.013390\n",
            " 104053/150000: episode: 2143, duration: 1.247s, episode steps: 86, steps per second: 69, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 10.047 [4.000, 16.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.008832, mean_absolute_error: 0.941573, mean_q: 1.020918\n",
            " 104083/150000: episode: 2144, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010712, mean_absolute_error: 0.944777, mean_q: 1.023604\n",
            " 104121/150000: episode: 2145, duration: 0.611s, episode steps: 38, steps per second: 62, episode reward: 2.000, mean reward: 0.053 [0.000, 1.000], mean action: 5.211 [1.000, 19.000], mean observation: -0.633 [-1.011, 1.009], loss: 0.007296, mean_absolute_error: 0.945550, mean_q: 1.029353\n",
            " 104204/150000: episode: 2146, duration: 1.172s, episode steps: 83, steps per second: 71, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 8.398 [2.000, 18.000], mean observation: -0.632 [-1.011, 1.000], loss: 0.006723, mean_absolute_error: 0.948272, mean_q: 1.021755\n",
            " 104216/150000: episode: 2147, duration: 0.277s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.000], mean action: 5.833 [5.000, 12.000], mean observation: -0.637 [-1.041, 1.000], loss: 0.008399, mean_absolute_error: 0.942786, mean_q: 1.023707\n",
            " 104246/150000: episode: 2148, duration: 0.509s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005625, mean_absolute_error: 0.953454, mean_q: 1.031089\n",
            " 104318/150000: episode: 2149, duration: 1.090s, episode steps: 72, steps per second: 66, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 5.875 [0.000, 17.000], mean observation: -0.621 [-1.011, 8.189], loss: 0.007035, mean_absolute_error: 0.952588, mean_q: 1.024815\n",
            " 104348/150000: episode: 2150, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.600 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005317, mean_absolute_error: 0.950257, mean_q: 1.027718\n",
            " 104363/150000: episode: 2151, duration: 0.327s, episode steps: 15, steps per second: 46, episode reward: 2.000, mean reward: 0.133 [0.000, 2.000], mean action: 7.067 [2.000, 15.000], mean observation: -0.633 [-1.039, 1.542], loss: 0.007599, mean_absolute_error: 0.955503, mean_q: 1.019605\n",
            " 104412/150000: episode: 2152, duration: 0.715s, episode steps: 49, steps per second: 69, episode reward: 1.000, mean reward: 0.020 [0.000, 0.900], mean action: 6.612 [0.000, 18.000], mean observation: -0.601 [-1.112, 9.759], loss: 0.007030, mean_absolute_error: 0.947562, mean_q: 1.025329\n",
            " 104424/150000: episode: 2153, duration: 0.302s, episode steps: 12, steps per second: 40, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.083 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.010], loss: 0.007244, mean_absolute_error: 0.944918, mean_q: 1.018018\n",
            " 104505/150000: episode: 2154, duration: 1.229s, episode steps: 81, steps per second: 66, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 8.284 [3.000, 16.000], mean observation: -0.635 [-1.009, 1.000], loss: 0.006912, mean_absolute_error: 0.949746, mean_q: 1.031363\n",
            " 104535/150000: episode: 2155, duration: 0.521s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.800 [0.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008293, mean_absolute_error: 0.952006, mean_q: 1.042719\n",
            " 104592/150000: episode: 2156, duration: 0.914s, episode steps: 57, steps per second: 62, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 10.263 [4.000, 17.000], mean observation: -0.630 [-1.009, 1.050], loss: 0.009538, mean_absolute_error: 0.954023, mean_q: 1.030281\n",
            " 104627/150000: episode: 2157, duration: 0.603s, episode steps: 35, steps per second: 58, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 4.171 [1.000, 13.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.004758, mean_absolute_error: 0.954182, mean_q: 1.030707\n",
            " 104657/150000: episode: 2158, duration: 0.527s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 11.833 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006582, mean_absolute_error: 0.954118, mean_q: 1.025014\n",
            " 104687/150000: episode: 2159, duration: 0.531s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.767 [4.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006715, mean_absolute_error: 0.953437, mean_q: 1.026651\n",
            " 104720/150000: episode: 2160, duration: 0.564s, episode steps: 33, steps per second: 59, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.455 [2.000, 17.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.005736, mean_absolute_error: 0.953409, mean_q: 1.026737\n",
            " 104806/150000: episode: 2161, duration: 1.291s, episode steps: 86, steps per second: 67, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 8.686 [4.000, 19.000], mean observation: -0.634 [-1.009, 1.000], loss: 0.007583, mean_absolute_error: 0.956983, mean_q: 1.033280\n",
            " 104836/150000: episode: 2162, duration: 0.531s, episode steps: 30, steps per second: 56, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [4.000, 6.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004353, mean_absolute_error: 0.959287, mean_q: 1.030539\n",
            " 104870/150000: episode: 2163, duration: 0.618s, episode steps: 34, steps per second: 55, episode reward: 1.000, mean reward: 0.029 [0.000, 0.900], mean action: 13.853 [0.000, 17.000], mean observation: -0.632 [-1.011, 1.356], loss: 0.006007, mean_absolute_error: 0.959468, mean_q: 1.024877\n",
            " 104900/150000: episode: 2164, duration: 0.518s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.100 [5.000, 8.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007087, mean_absolute_error: 0.949784, mean_q: 1.026385\n",
            " 104930/150000: episode: 2165, duration: 0.540s, episode steps: 30, steps per second: 56, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.433 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008390, mean_absolute_error: 0.952365, mean_q: 1.034092\n",
            " 105039/150000: episode: 2166, duration: 1.650s, episode steps: 109, steps per second: 66, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 6.257 [0.000, 20.000], mean observation: -0.632 [-1.009, 1.209], loss: 0.006920, mean_absolute_error: 0.961445, mean_q: 1.034721\n",
            " 105052/150000: episode: 2167, duration: 0.315s, episode steps: 13, steps per second: 41, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 6.615 [3.000, 19.000], mean observation: -0.636 [-1.037, 1.007], loss: 0.005476, mean_absolute_error: 0.965956, mean_q: 1.042606\n",
            " 105121/150000: episode: 2168, duration: 1.061s, episode steps: 69, steps per second: 65, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 11.580 [0.000, 20.000], mean observation: -0.633 [-1.009, 1.186], loss: 0.007687, mean_absolute_error: 0.957612, mean_q: 1.036392\n",
            " 105192/150000: episode: 2169, duration: 1.049s, episode steps: 71, steps per second: 68, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 3.197 [1.000, 20.000], mean observation: -0.618 [-1.032, 8.058], loss: 0.007459, mean_absolute_error: 0.961017, mean_q: 1.048050\n",
            " 105260/150000: episode: 2170, duration: 0.984s, episode steps: 68, steps per second: 69, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 6.206 [1.000, 18.000], mean observation: -0.639 [-1.009, 1.000], loss: 0.007057, mean_absolute_error: 0.967854, mean_q: 1.039880\n",
            " 105373/150000: episode: 2171, duration: 1.646s, episode steps: 113, steps per second: 69, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 5.637 [1.000, 18.000], mean observation: -0.633 [-1.009, 1.099], loss: 0.007176, mean_absolute_error: 0.961340, mean_q: 1.031322\n",
            " 105435/150000: episode: 2172, duration: 0.932s, episode steps: 62, steps per second: 66, episode reward: 0.900, mean reward: 0.015 [0.000, 0.900], mean action: 5.500 [2.000, 18.000], mean observation: -0.615 [-1.011, 8.005], loss: 0.006035, mean_absolute_error: 0.956296, mean_q: 1.029312\n",
            " 105467/150000: episode: 2173, duration: 0.515s, episode steps: 32, steps per second: 62, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 5.250 [1.000, 18.000], mean observation: -0.634 [-1.009, 1.015], loss: 0.007788, mean_absolute_error: 0.962386, mean_q: 1.038878\n",
            " 105572/150000: episode: 2174, duration: 1.442s, episode steps: 105, steps per second: 73, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 5.390 [1.000, 20.000], mean observation: -0.630 [-1.009, 1.049], loss: 0.005123, mean_absolute_error: 0.962452, mean_q: 1.036774\n",
            " 105605/150000: episode: 2175, duration: 0.553s, episode steps: 33, steps per second: 60, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.061 [3.000, 10.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.009529, mean_absolute_error: 0.962307, mean_q: 1.040492\n",
            " 105645/150000: episode: 2176, duration: 0.645s, episode steps: 40, steps per second: 62, episode reward: 2.000, mean reward: 0.050 [0.000, 1.000], mean action: 8.975 [2.000, 19.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.008001, mean_absolute_error: 0.967209, mean_q: 1.038908\n",
            " 105657/150000: episode: 2177, duration: 0.270s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 8.667 [5.000, 14.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.008597, mean_absolute_error: 0.965192, mean_q: 1.046659\n",
            " 105688/150000: episode: 2178, duration: 0.523s, episode steps: 31, steps per second: 59, episode reward: 2.000, mean reward: 0.065 [0.000, 1.000], mean action: 6.194 [4.000, 20.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.005962, mean_absolute_error: 0.964431, mean_q: 1.036926\n",
            " 105722/150000: episode: 2179, duration: 0.539s, episode steps: 34, steps per second: 63, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 11.912 [5.000, 19.000], mean observation: -0.633 [-1.009, 1.015], loss: 0.005130, mean_absolute_error: 0.967389, mean_q: 1.034604\n",
            " 105954/150000: episode: 2180, duration: 3.142s, episode steps: 232, steps per second: 74, episode reward: 0.900, mean reward: 0.004 [0.000, 0.900], mean action: 8.328 [1.000, 20.000], mean observation: -0.633 [-1.011, 1.228], loss: 0.007312, mean_absolute_error: 0.963723, mean_q: 1.037285\n",
            " 105977/150000: episode: 2181, duration: 0.410s, episode steps: 23, steps per second: 56, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 9.957 [5.000, 15.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.010039, mean_absolute_error: 0.965530, mean_q: 1.042514\n",
            " 106069/150000: episode: 2182, duration: 1.310s, episode steps: 92, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.717 [0.000, 18.000], mean observation: -0.632 [-1.009, 1.334], loss: 0.007303, mean_absolute_error: 0.958476, mean_q: 1.035029\n",
            " 106095/150000: episode: 2183, duration: 0.462s, episode steps: 26, steps per second: 56, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 5.615 [0.000, 14.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.008722, mean_absolute_error: 0.958380, mean_q: 1.041007\n",
            " 106125/150000: episode: 2184, duration: 0.498s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.900 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.009190, mean_absolute_error: 0.961513, mean_q: 1.035701\n",
            " 106237/150000: episode: 2185, duration: 1.599s, episode steps: 112, steps per second: 70, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 5.420 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.006098, mean_absolute_error: 0.966591, mean_q: 1.035539\n",
            " 106366/150000: episode: 2186, duration: 1.780s, episode steps: 129, steps per second: 72, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 8.419 [1.000, 19.000], mean observation: -0.632 [-1.009, 3.085], loss: 0.006770, mean_absolute_error: 0.957134, mean_q: 1.028129\n",
            " 106652/150000: episode: 2187, duration: 3.808s, episode steps: 286, steps per second: 75, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 8.696 [0.000, 20.000], mean observation: -0.628 [-1.011, 8.293], loss: 0.006419, mean_absolute_error: 0.951332, mean_q: 1.023973\n",
            " 106682/150000: episode: 2188, duration: 0.519s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 9.167 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.009261, mean_absolute_error: 0.948827, mean_q: 1.014754\n",
            " 106852/150000: episode: 2189, duration: 2.316s, episode steps: 170, steps per second: 73, episode reward: 0.900, mean reward: 0.005 [0.000, 0.900], mean action: 10.000 [0.000, 20.000], mean observation: -0.636 [-1.009, 1.000], loss: 0.006937, mean_absolute_error: 0.945995, mean_q: 1.019009\n",
            " 106871/150000: episode: 2190, duration: 0.361s, episode steps: 19, steps per second: 53, episode reward: 2.000, mean reward: 0.105 [0.000, 1.000], mean action: 9.947 [4.000, 18.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.004017, mean_absolute_error: 0.943653, mean_q: 1.015517\n",
            " 106886/150000: episode: 2191, duration: 0.315s, episode steps: 15, steps per second: 48, episode reward: 2.000, mean reward: 0.133 [0.000, 2.000], mean action: 13.000 [12.000, 20.000], mean observation: -0.631 [-1.011, 2.233], loss: 0.005829, mean_absolute_error: 0.940255, mean_q: 1.005482\n",
            " 106895/150000: episode: 2192, duration: 0.217s, episode steps: 9, steps per second: 41, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.556 [5.000, 15.000], mean observation: -0.635 [-1.011, 1.020], loss: 0.006618, mean_absolute_error: 0.939138, mean_q: 1.005874\n",
            " 107025/150000: episode: 2193, duration: 1.798s, episode steps: 130, steps per second: 72, episode reward: 0.900, mean reward: 0.007 [0.000, 0.900], mean action: 7.031 [0.000, 19.000], mean observation: -0.636 [-1.009, 1.350], loss: 0.006988, mean_absolute_error: 0.932726, mean_q: 1.002548\n",
            " 107171/150000: episode: 2194, duration: 1.970s, episode steps: 146, steps per second: 74, episode reward: 2.000, mean reward: 0.014 [0.000, 1.000], mean action: 12.712 [0.000, 20.000], mean observation: -0.631 [-1.009, 1.016], loss: 0.007735, mean_absolute_error: 0.941784, mean_q: 1.019454\n",
            " 107181/150000: episode: 2195, duration: 0.255s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 6.700 [5.000, 15.000], mean observation: -0.635 [-1.038, 1.001], loss: 0.003236, mean_absolute_error: 0.947057, mean_q: 1.031354\n",
            " 107265/150000: episode: 2196, duration: 1.240s, episode steps: 84, steps per second: 68, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 6.821 [0.000, 20.000], mean observation: -0.615 [-1.009, 8.353], loss: 0.005002, mean_absolute_error: 0.944720, mean_q: 1.019979\n",
            " 107286/150000: episode: 2197, duration: 0.427s, episode steps: 21, steps per second: 49, episode reward: 1.000, mean reward: 0.048 [0.000, 0.900], mean action: 4.429 [4.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007846, mean_absolute_error: 0.942190, mean_q: 1.016957\n",
            " 107305/150000: episode: 2198, duration: 0.381s, episode steps: 19, steps per second: 50, episode reward: 2.000, mean reward: 0.105 [0.000, 1.000], mean action: 4.789 [2.000, 12.000], mean observation: -0.636 [-1.011, 1.006], loss: 0.010611, mean_absolute_error: 0.948305, mean_q: 1.015767\n",
            " 107317/150000: episode: 2199, duration: 0.277s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.583 [2.000, 12.000], mean observation: -0.636 [-1.011, 1.003], loss: 0.011043, mean_absolute_error: 0.948178, mean_q: 1.026776\n",
            " 107329/150000: episode: 2200, duration: 0.283s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.083 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.001], loss: 0.007186, mean_absolute_error: 0.943216, mean_q: 1.034356\n",
            " 107341/150000: episode: 2201, duration: 0.285s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.002], loss: 0.006184, mean_absolute_error: 0.946048, mean_q: 1.029020\n",
            " 107353/150000: episode: 2202, duration: 0.286s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.250 [0.000, 17.000], mean observation: -0.635 [-1.011, 1.001], loss: 0.012819, mean_absolute_error: 0.951526, mean_q: 1.026891\n",
            " 107407/150000: episode: 2203, duration: 0.855s, episode steps: 54, steps per second: 63, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 9.685 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.008], loss: 0.006619, mean_absolute_error: 0.944233, mean_q: 1.016585\n",
            " 107443/150000: episode: 2204, duration: 0.561s, episode steps: 36, steps per second: 64, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 4.722 [1.000, 14.000], mean observation: -0.633 [-1.009, 1.014], loss: 0.004495, mean_absolute_error: 0.943298, mean_q: 1.015393\n",
            " 107524/150000: episode: 2205, duration: 1.166s, episode steps: 81, steps per second: 69, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 6.926 [1.000, 18.000], mean observation: -0.637 [-1.011, 1.382], loss: 0.004525, mean_absolute_error: 0.944790, mean_q: 1.014914\n",
            " 107557/150000: episode: 2206, duration: 0.550s, episode steps: 33, steps per second: 60, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.545 [4.000, 17.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.005622, mean_absolute_error: 0.941428, mean_q: 1.014452\n",
            " 107603/150000: episode: 2207, duration: 0.754s, episode steps: 46, steps per second: 61, episode reward: 1.000, mean reward: 0.022 [0.000, 0.900], mean action: 5.370 [3.000, 19.000], mean observation: -0.632 [-1.009, 1.012], loss: 0.004746, mean_absolute_error: 0.939869, mean_q: 1.007036\n",
            " 107632/150000: episode: 2208, duration: 0.526s, episode steps: 29, steps per second: 55, episode reward: 1.000, mean reward: 0.034 [0.000, 0.900], mean action: 6.793 [4.000, 20.000], mean observation: -0.634 [-1.011, 1.016], loss: 0.005531, mean_absolute_error: 0.936748, mean_q: 1.004185\n",
            " 107662/150000: episode: 2209, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.200 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005924, mean_absolute_error: 0.935124, mean_q: 1.000915\n",
            " 107695/150000: episode: 2210, duration: 0.529s, episode steps: 33, steps per second: 62, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 6.303 [5.000, 18.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.004492, mean_absolute_error: 0.931997, mean_q: 1.001555\n",
            " 107782/150000: episode: 2211, duration: 1.233s, episode steps: 87, steps per second: 71, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.736 [3.000, 17.000], mean observation: -0.636 [-1.011, 1.107], loss: 0.006004, mean_absolute_error: 0.935209, mean_q: 1.011550\n",
            " 107858/150000: episode: 2212, duration: 1.101s, episode steps: 76, steps per second: 69, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 7.197 [0.000, 19.000], mean observation: -0.636 [-1.011, 1.017], loss: 0.006073, mean_absolute_error: 0.937453, mean_q: 1.007220\n",
            " 107916/150000: episode: 2213, duration: 0.894s, episode steps: 58, steps per second: 65, episode reward: 1.000, mean reward: 0.017 [0.000, 0.900], mean action: 7.500 [5.000, 20.000], mean observation: -0.612 [-1.009, 8.189], loss: 0.003932, mean_absolute_error: 0.938344, mean_q: 1.005389\n",
            " 107947/150000: episode: 2214, duration: 0.567s, episode steps: 31, steps per second: 55, episode reward: 1.000, mean reward: 0.032 [0.000, 0.900], mean action: 8.710 [2.000, 20.000], mean observation: -0.633 [-1.011, 1.008], loss: 0.007439, mean_absolute_error: 0.940429, mean_q: 0.999885\n",
            " 107977/150000: episode: 2215, duration: 0.500s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 10.200 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007735, mean_absolute_error: 0.939628, mean_q: 1.012296\n",
            " 108082/150000: episode: 2216, duration: 1.519s, episode steps: 105, steps per second: 69, episode reward: 2.000, mean reward: 0.019 [0.000, 1.000], mean action: 7.238 [0.000, 18.000], mean observation: -0.632 [-1.011, 1.195], loss: 0.006802, mean_absolute_error: 0.935510, mean_q: 1.008276\n",
            " 108268/150000: episode: 2217, duration: 2.548s, episode steps: 186, steps per second: 73, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 8.005 [2.000, 20.000], mean observation: -0.624 [-1.018, 7.981], loss: 0.007727, mean_absolute_error: 0.936980, mean_q: 1.007636\n",
            " 108316/150000: episode: 2218, duration: 0.732s, episode steps: 48, steps per second: 66, episode reward: 2.000, mean reward: 0.042 [0.000, 1.000], mean action: 6.875 [0.000, 16.000], mean observation: -0.632 [-1.011, 1.000], loss: 0.011218, mean_absolute_error: 0.930371, mean_q: 1.004782\n",
            " 108368/150000: episode: 2219, duration: 0.795s, episode steps: 52, steps per second: 65, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 8.212 [2.000, 19.000], mean observation: -0.611 [-1.018, 7.981], loss: 0.004119, mean_absolute_error: 0.937218, mean_q: 1.003896\n",
            " 108398/150000: episode: 2220, duration: 0.502s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.133 [2.000, 15.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006709, mean_absolute_error: 0.939201, mean_q: 1.016303\n",
            " 108428/150000: episode: 2221, duration: 0.506s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.833 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005504, mean_absolute_error: 0.939073, mean_q: 1.007645\n",
            " 108463/150000: episode: 2222, duration: 0.603s, episode steps: 35, steps per second: 58, episode reward: 1.000, mean reward: 0.029 [0.000, 0.900], mean action: 12.657 [5.000, 19.000], mean observation: -0.631 [-1.009, 1.798], loss: 0.006047, mean_absolute_error: 0.941007, mean_q: 1.008717\n",
            " 108542/150000: episode: 2223, duration: 1.141s, episode steps: 79, steps per second: 69, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 17.494 [0.000, 20.000], mean observation: -0.620 [-1.009, 8.010], loss: 0.007038, mean_absolute_error: 0.934293, mean_q: 1.006019\n",
            " 108662/150000: episode: 2224, duration: 1.698s, episode steps: 120, steps per second: 71, episode reward: 0.500, mean reward: 0.004 [0.000, 0.200], mean action: 11.733 [4.000, 19.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.005821, mean_absolute_error: 0.936320, mean_q: 1.008223\n",
            " 108686/150000: episode: 2225, duration: 0.429s, episode steps: 24, steps per second: 56, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 4.792 [0.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.006608, mean_absolute_error: 0.928744, mean_q: 1.006194\n",
            " 108793/150000: episode: 2226, duration: 1.568s, episode steps: 107, steps per second: 68, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 10.131 [2.000, 20.000], mean observation: -0.632 [-1.011, 1.210], loss: 0.007625, mean_absolute_error: 0.931880, mean_q: 1.008422\n",
            " 108874/150000: episode: 2227, duration: 1.151s, episode steps: 81, steps per second: 70, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 11.654 [0.000, 17.000], mean observation: -0.617 [-1.013, 8.419], loss: 0.007275, mean_absolute_error: 0.940566, mean_q: 1.016402\n",
            " 108955/150000: episode: 2228, duration: 1.157s, episode steps: 81, steps per second: 70, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 6.247 [4.000, 15.000], mean observation: -0.634 [-1.011, 1.152], loss: 0.005358, mean_absolute_error: 0.939205, mean_q: 1.013480\n",
            " 109015/150000: episode: 2229, duration: 0.872s, episode steps: 60, steps per second: 69, episode reward: 2.000, mean reward: 0.033 [0.000, 1.000], mean action: 8.617 [0.000, 18.000], mean observation: -0.632 [-1.011, 1.016], loss: 0.007232, mean_absolute_error: 0.939671, mean_q: 1.013866\n",
            " 109037/150000: episode: 2230, duration: 0.406s, episode steps: 22, steps per second: 54, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 9.545 [5.000, 16.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007400, mean_absolute_error: 0.946300, mean_q: 1.025363\n",
            " 109100/150000: episode: 2231, duration: 0.927s, episode steps: 63, steps per second: 68, episode reward: 2.000, mean reward: 0.032 [0.000, 1.000], mean action: 5.968 [1.000, 20.000], mean observation: -0.632 [-1.011, 1.011], loss: 0.006044, mean_absolute_error: 0.948163, mean_q: 1.023853\n",
            " 109207/150000: episode: 2232, duration: 1.513s, episode steps: 107, steps per second: 71, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 5.374 [1.000, 19.000], mean observation: -0.639 [-1.009, 1.000], loss: 0.007508, mean_absolute_error: 0.947155, mean_q: 1.019841\n",
            " 109237/150000: episode: 2233, duration: 0.502s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 16.000 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007885, mean_absolute_error: 0.940992, mean_q: 1.015987\n",
            " 109259/150000: episode: 2234, duration: 0.447s, episode steps: 22, steps per second: 49, episode reward: 1.000, mean reward: 0.045 [0.000, 0.900], mean action: 18.000 [4.000, 20.000], mean observation: -0.635 [-1.009, 1.000], loss: 0.003514, mean_absolute_error: 0.938471, mean_q: 1.008129\n",
            " 109272/150000: episode: 2235, duration: 0.298s, episode steps: 13, steps per second: 44, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 17.538 [5.000, 20.000], mean observation: -0.636 [-1.040, 1.000], loss: 0.007727, mean_absolute_error: 0.940869, mean_q: 1.007805\n",
            " 109294/150000: episode: 2236, duration: 0.397s, episode steps: 22, steps per second: 55, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 18.636 [5.000, 20.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.006612, mean_absolute_error: 0.939130, mean_q: 1.003426\n",
            " 109365/150000: episode: 2237, duration: 1.028s, episode steps: 71, steps per second: 69, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 13.648 [5.000, 20.000], mean observation: -0.609 [-1.078, 9.523], loss: 0.007626, mean_absolute_error: 0.933762, mean_q: 0.997856\n",
            " 109499/150000: episode: 2238, duration: 1.878s, episode steps: 134, steps per second: 71, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 8.261 [1.000, 20.000], mean observation: -0.631 [-1.011, 1.175], loss: 0.005790, mean_absolute_error: 0.927509, mean_q: 0.997417\n",
            " 109590/150000: episode: 2239, duration: 1.298s, episode steps: 91, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 10.626 [0.000, 17.000], mean observation: -0.633 [-1.009, 1.012], loss: 0.008057, mean_absolute_error: 0.928675, mean_q: 1.002853\n",
            " 109668/150000: episode: 2240, duration: 1.124s, episode steps: 78, steps per second: 69, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 2.795 [0.000, 20.000], mean observation: -0.635 [-1.011, 1.023], loss: 0.008347, mean_absolute_error: 0.925169, mean_q: 0.992468\n",
            " 109742/150000: episode: 2241, duration: 1.068s, episode steps: 74, steps per second: 69, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 11.500 [1.000, 17.000], mean observation: -0.637 [-1.011, 1.000], loss: 0.008649, mean_absolute_error: 0.916970, mean_q: 0.987868\n",
            " 109816/150000: episode: 2242, duration: 1.071s, episode steps: 74, steps per second: 69, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 6.662 [0.000, 16.000], mean observation: -0.635 [-1.011, 1.008], loss: 0.005624, mean_absolute_error: 0.915032, mean_q: 0.983902\n",
            " 109911/150000: episode: 2243, duration: 1.364s, episode steps: 95, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 4.326 [0.000, 19.000], mean observation: -0.637 [-1.009, 1.000], loss: 0.006644, mean_absolute_error: 0.922126, mean_q: 0.997155\n",
            " 109946/150000: episode: 2244, duration: 0.614s, episode steps: 35, steps per second: 57, episode reward: 1.000, mean reward: 0.029 [0.000, 0.900], mean action: 14.800 [5.000, 19.000], mean observation: -0.632 [-1.011, 1.310], loss: 0.008513, mean_absolute_error: 0.922306, mean_q: 0.999279\n",
            " 109968/150000: episode: 2245, duration: 0.405s, episode steps: 22, steps per second: 54, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 15.318 [5.000, 16.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007527, mean_absolute_error: 0.924446, mean_q: 1.001856\n",
            " 110013/150000: episode: 2246, duration: 0.753s, episode steps: 45, steps per second: 60, episode reward: 0.900, mean reward: 0.020 [0.000, 0.900], mean action: 14.422 [0.000, 16.000], mean observation: -0.633 [-1.009, 1.000], loss: 0.004685, mean_absolute_error: 0.928295, mean_q: 1.010739\n",
            " 110116/150000: episode: 2247, duration: 1.474s, episode steps: 103, steps per second: 70, episode reward: 0.500, mean reward: 0.005 [0.000, 0.200], mean action: 7.796 [4.000, 17.000], mean observation: -0.637 [-1.009, 1.000], loss: 0.006524, mean_absolute_error: 0.939488, mean_q: 1.008472\n",
            " 110154/150000: episode: 2248, duration: 0.608s, episode steps: 38, steps per second: 62, episode reward: 2.000, mean reward: 0.053 [0.000, 1.000], mean action: 6.000 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.009], loss: 0.007449, mean_absolute_error: 0.936570, mean_q: 1.010594\n",
            " 110299/150000: episode: 2249, duration: 2.013s, episode steps: 145, steps per second: 72, episode reward: 0.900, mean reward: 0.006 [0.000, 0.900], mean action: 4.510 [0.000, 19.000], mean observation: -0.629 [-1.009, 1.000], loss: 0.005861, mean_absolute_error: 0.940686, mean_q: 1.011436\n",
            " 110371/150000: episode: 2250, duration: 1.068s, episode steps: 72, steps per second: 67, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 9.944 [2.000, 20.000], mean observation: -0.616 [-1.009, 8.160], loss: 0.007575, mean_absolute_error: 0.935031, mean_q: 0.999231\n",
            " 110427/150000: episode: 2251, duration: 0.866s, episode steps: 56, steps per second: 65, episode reward: 0.900, mean reward: 0.016 [0.000, 0.900], mean action: 5.839 [1.000, 20.000], mean observation: -0.615 [-1.011, 7.662], loss: 0.006662, mean_absolute_error: 0.932262, mean_q: 1.002563\n",
            " 110694/150000: episode: 2252, duration: 3.579s, episode steps: 267, steps per second: 75, episode reward: 0.900, mean reward: 0.003 [0.000, 0.900], mean action: 6.670 [0.000, 20.000], mean observation: -0.628 [-1.011, 8.923], loss: 0.007110, mean_absolute_error: 0.929125, mean_q: 0.997975\n",
            " 110868/150000: episode: 2253, duration: 2.397s, episode steps: 174, steps per second: 73, episode reward: 0.900, mean reward: 0.005 [0.000, 0.900], mean action: 5.695 [1.000, 20.000], mean observation: -0.637 [-1.009, 1.000], loss: 0.007305, mean_absolute_error: 0.922893, mean_q: 0.994736\n",
            " 110880/150000: episode: 2254, duration: 0.292s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.083 [5.000, 20.000], mean observation: -0.636 [-1.011, 1.005], loss: 0.006272, mean_absolute_error: 0.928604, mean_q: 0.992743\n",
            " 110967/150000: episode: 2255, duration: 1.234s, episode steps: 87, steps per second: 70, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 7.023 [0.000, 20.000], mean observation: -0.635 [-1.009, 1.000], loss: 0.007268, mean_absolute_error: 0.927600, mean_q: 0.999331\n",
            " 110997/150000: episode: 2256, duration: 0.497s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.233 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004352, mean_absolute_error: 0.929318, mean_q: 0.998765\n",
            " 111025/150000: episode: 2257, duration: 0.530s, episode steps: 28, steps per second: 53, episode reward: 1.000, mean reward: 0.036 [0.000, 0.900], mean action: 15.071 [0.000, 20.000], mean observation: -0.633 [-1.009, 1.000], loss: 0.007223, mean_absolute_error: 0.925366, mean_q: 0.986039\n",
            " 111035/150000: episode: 2258, duration: 0.244s, episode steps: 10, steps per second: 41, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.400 [3.000, 19.000], mean observation: -0.635 [-1.011, 1.009], loss: 0.004368, mean_absolute_error: 0.924505, mean_q: 0.995020\n",
            " 111069/150000: episode: 2259, duration: 0.559s, episode steps: 34, steps per second: 61, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 14.059 [5.000, 18.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.008680, mean_absolute_error: 0.924525, mean_q: 0.989951\n",
            " 111082/150000: episode: 2260, duration: 0.292s, episode steps: 13, steps per second: 44, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 5.538 [5.000, 12.000], mean observation: -0.636 [-1.039, 1.018], loss: 0.006199, mean_absolute_error: 0.928591, mean_q: 0.998199\n",
            " 111112/150000: episode: 2261, duration: 0.498s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.200 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006516, mean_absolute_error: 0.921619, mean_q: 0.996393\n",
            " 111142/150000: episode: 2262, duration: 0.512s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 11.133 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006112, mean_absolute_error: 0.922655, mean_q: 0.994392\n",
            " 111154/150000: episode: 2263, duration: 0.257s, episode steps: 12, steps per second: 47, episode reward: 2.000, mean reward: 0.167 [0.000, 1.000], mean action: 11.250 [2.000, 16.000], mean observation: -0.636 [-1.011, 1.021], loss: 0.012614, mean_absolute_error: 0.920985, mean_q: 0.989322\n",
            " 111166/150000: episode: 2264, duration: 0.276s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 14.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.008447, mean_absolute_error: 0.918901, mean_q: 0.997442\n",
            " 111178/150000: episode: 2265, duration: 0.289s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 8.250 [1.000, 15.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.001396, mean_absolute_error: 0.919980, mean_q: 0.988895\n",
            " 111282/150000: episode: 2266, duration: 1.442s, episode steps: 104, steps per second: 72, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 5.654 [1.000, 20.000], mean observation: -0.620 [-1.009, 8.533], loss: 0.007495, mean_absolute_error: 0.917847, mean_q: 0.986206\n",
            " 111312/150000: episode: 2267, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 15.833 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007079, mean_absolute_error: 0.918872, mean_q: 0.993654\n",
            " 111385/150000: episode: 2268, duration: 1.074s, episode steps: 73, steps per second: 68, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 9.521 [0.000, 20.000], mean observation: -0.639 [-1.011, 1.000], loss: 0.007085, mean_absolute_error: 0.912475, mean_q: 0.983541\n",
            " 111397/150000: episode: 2269, duration: 0.276s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.083 [5.000, 15.000], mean observation: -0.636 [-1.011, 1.007], loss: 0.009355, mean_absolute_error: 0.912004, mean_q: 0.979814\n",
            " 111734/150000: episode: 2270, duration: 4.530s, episode steps: 337, steps per second: 74, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 6.350 [0.000, 20.000], mean observation: -0.630 [-1.011, 9.613], loss: 0.006325, mean_absolute_error: 0.909011, mean_q: 0.982303\n",
            " 111794/150000: episode: 2271, duration: 0.924s, episode steps: 60, steps per second: 65, episode reward: 1.000, mean reward: 0.017 [0.000, 0.900], mean action: 4.800 [1.000, 19.000], mean observation: -0.633 [-1.009, 1.015], loss: 0.006898, mean_absolute_error: 0.914676, mean_q: 0.981801\n",
            " 111817/150000: episode: 2272, duration: 0.428s, episode steps: 23, steps per second: 54, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 12.348 [2.000, 17.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.006003, mean_absolute_error: 0.909701, mean_q: 0.976980\n",
            " 111839/150000: episode: 2273, duration: 0.406s, episode steps: 22, steps per second: 54, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 6.000 [4.000, 13.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007748, mean_absolute_error: 0.910985, mean_q: 0.973476\n",
            " 111869/150000: episode: 2274, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.200 [1.000, 15.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005172, mean_absolute_error: 0.907613, mean_q: 0.972195\n",
            " 111964/150000: episode: 2275, duration: 1.336s, episode steps: 95, steps per second: 71, episode reward: 2.000, mean reward: 0.021 [0.000, 1.100], mean action: 2.789 [1.000, 20.000], mean observation: -0.602 [-1.071, 9.578], loss: 0.007555, mean_absolute_error: 0.901587, mean_q: 0.975854\n",
            " 112001/150000: episode: 2276, duration: 0.591s, episode steps: 37, steps per second: 63, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.757 [1.000, 20.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.006080, mean_absolute_error: 0.903088, mean_q: 0.974015\n",
            " 112013/150000: episode: 2277, duration: 0.288s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.250 [5.000, 13.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.004690, mean_absolute_error: 0.898927, mean_q: 0.963443\n",
            " 112035/150000: episode: 2278, duration: 0.424s, episode steps: 22, steps per second: 52, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 7.545 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.005], loss: 0.007533, mean_absolute_error: 0.904490, mean_q: 0.964345\n",
            " 112071/150000: episode: 2279, duration: 0.583s, episode steps: 36, steps per second: 62, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 5.556 [2.000, 20.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.008724, mean_absolute_error: 0.902019, mean_q: 0.963580\n",
            " 112107/150000: episode: 2280, duration: 0.590s, episode steps: 36, steps per second: 61, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 6.472 [2.000, 20.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.005392, mean_absolute_error: 0.902441, mean_q: 0.970720\n",
            " 112133/150000: episode: 2281, duration: 0.480s, episode steps: 26, steps per second: 54, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 6.192 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.005879, mean_absolute_error: 0.911350, mean_q: 0.989504\n",
            " 112145/150000: episode: 2282, duration: 0.282s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.000], mean action: 5.250 [1.000, 12.000], mean observation: -0.636 [-1.041, 1.000], loss: 0.004428, mean_absolute_error: 0.905489, mean_q: 0.975286\n",
            " 112175/150000: episode: 2283, duration: 0.502s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 9.267 [1.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005508, mean_absolute_error: 0.912217, mean_q: 0.979435\n",
            " 112243/150000: episode: 2284, duration: 1.010s, episode steps: 68, steps per second: 67, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 6.765 [0.000, 18.000], mean observation: -0.608 [-1.108, 9.693], loss: 0.006127, mean_absolute_error: 0.909298, mean_q: 0.974223\n",
            " 112273/150000: episode: 2285, duration: 0.506s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.100 [0.000, 13.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008694, mean_absolute_error: 0.908907, mean_q: 0.978773\n",
            " 112303/150000: episode: 2286, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.867 [1.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006927, mean_absolute_error: 0.910203, mean_q: 0.981846\n",
            " 112340/150000: episode: 2287, duration: 0.639s, episode steps: 37, steps per second: 58, episode reward: 1.000, mean reward: 0.027 [0.000, 0.900], mean action: 14.324 [4.000, 17.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.006761, mean_absolute_error: 0.903063, mean_q: 0.967482\n",
            " 112426/150000: episode: 2288, duration: 1.244s, episode steps: 86, steps per second: 69, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 14.512 [0.000, 19.000], mean observation: -0.639 [-1.011, 1.000], loss: 0.007239, mean_absolute_error: 0.905253, mean_q: 0.981777\n",
            " 112459/150000: episode: 2289, duration: 0.594s, episode steps: 33, steps per second: 56, episode reward: 1.000, mean reward: 0.030 [0.000, 0.900], mean action: 14.848 [4.000, 17.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.007613, mean_absolute_error: 0.909988, mean_q: 0.998235\n",
            " 112499/150000: episode: 2290, duration: 0.655s, episode steps: 40, steps per second: 61, episode reward: 1.000, mean reward: 0.025 [0.000, 0.900], mean action: 13.450 [4.000, 16.000], mean observation: -0.632 [-1.011, 1.014], loss: 0.006520, mean_absolute_error: 0.916619, mean_q: 0.995376\n",
            " 112616/150000: episode: 2291, duration: 1.627s, episode steps: 117, steps per second: 72, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 11.718 [1.000, 20.000], mean observation: -0.623 [-1.011, 1.018], loss: 0.007050, mean_absolute_error: 0.926053, mean_q: 1.000622\n",
            " 112646/150000: episode: 2292, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 8.467 [3.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006887, mean_absolute_error: 0.927023, mean_q: 1.005447\n",
            " 112676/150000: episode: 2293, duration: 0.502s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 10.833 [0.000, 15.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007583, mean_absolute_error: 0.928528, mean_q: 0.999063\n",
            " 112903/150000: episode: 2294, duration: 3.086s, episode steps: 227, steps per second: 74, episode reward: 1.000, mean reward: 0.004 [0.000, 0.900], mean action: 8.458 [0.000, 19.000], mean observation: -0.635 [-1.009, 5.459], loss: 0.006698, mean_absolute_error: 0.923851, mean_q: 0.998286\n",
            " 112915/150000: episode: 2295, duration: 0.257s, episode steps: 12, steps per second: 47, episode reward: 2.000, mean reward: 0.167 [0.000, 1.000], mean action: 5.000 [0.000, 15.000], mean observation: -0.634 [-1.009, 1.023], loss: 0.005943, mean_absolute_error: 0.918384, mean_q: 0.982444\n",
            " 112962/150000: episode: 2296, duration: 0.770s, episode steps: 47, steps per second: 61, episode reward: 1.000, mean reward: 0.021 [0.000, 0.900], mean action: 10.043 [1.000, 20.000], mean observation: -0.631 [-1.009, 1.015], loss: 0.005636, mean_absolute_error: 0.921194, mean_q: 0.996001\n",
            " 112978/150000: episode: 2297, duration: 0.330s, episode steps: 16, steps per second: 49, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 16.812 [5.000, 18.000], mean observation: -0.636 [-1.040, 1.002], loss: 0.004500, mean_absolute_error: 0.923144, mean_q: 1.004301\n",
            " 113073/150000: episode: 2298, duration: 1.349s, episode steps: 95, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 14.947 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.110], loss: 0.005822, mean_absolute_error: 0.939111, mean_q: 1.017531\n",
            " 113112/150000: episode: 2299, duration: 0.657s, episode steps: 39, steps per second: 59, episode reward: 1.000, mean reward: 0.026 [0.000, 0.900], mean action: 17.308 [4.000, 18.000], mean observation: -0.633 [-1.009, 1.014], loss: 0.007732, mean_absolute_error: 0.948817, mean_q: 1.031076\n",
            " 113208/150000: episode: 2300, duration: 1.351s, episode steps: 96, steps per second: 71, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 17.375 [0.000, 18.000], mean observation: -0.633 [-1.009, 1.000], loss: 0.004861, mean_absolute_error: 0.967500, mean_q: 1.053142\n",
            " 113238/150000: episode: 2301, duration: 0.510s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.100], mean action: 12.433 [4.000, 18.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.006846, mean_absolute_error: 0.981467, mean_q: 1.047606\n",
            " 113279/150000: episode: 2302, duration: 0.691s, episode steps: 41, steps per second: 59, episode reward: 1.000, mean reward: 0.024 [0.000, 0.900], mean action: 15.415 [5.000, 19.000], mean observation: -0.632 [-1.009, 1.010], loss: 0.008246, mean_absolute_error: 0.978627, mean_q: 1.054340\n",
            " 113366/150000: episode: 2303, duration: 1.215s, episode steps: 87, steps per second: 72, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 5.690 [1.000, 20.000], mean observation: -0.634 [-1.009, 1.337], loss: 0.006952, mean_absolute_error: 0.977998, mean_q: 1.058323\n",
            " 113766/150000: episode: 2304, duration: 5.283s, episode steps: 400, steps per second: 76, episode reward: 0.900, mean reward: 0.002 [0.000, 0.900], mean action: 6.335 [0.000, 20.000], mean observation: -0.637 [-1.034, 8.444], loss: 0.006977, mean_absolute_error: 0.972887, mean_q: 1.046739\n",
            " 113778/150000: episode: 2305, duration: 0.286s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.083 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.005009, mean_absolute_error: 0.979543, mean_q: 1.049534\n",
            " 113833/150000: episode: 2306, duration: 0.884s, episode steps: 55, steps per second: 62, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 13.600 [0.000, 20.000], mean observation: -0.633 [-1.009, 1.012], loss: 0.009828, mean_absolute_error: 0.973005, mean_q: 1.046500\n",
            " 113896/150000: episode: 2307, duration: 0.963s, episode steps: 63, steps per second: 65, episode reward: 0.900, mean reward: 0.014 [0.000, 0.900], mean action: 14.302 [1.000, 19.000], mean observation: -0.619 [-1.011, 7.668], loss: 0.007289, mean_absolute_error: 0.967882, mean_q: 1.041362\n",
            " 113963/150000: episode: 2308, duration: 1.011s, episode steps: 67, steps per second: 66, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 6.761 [4.000, 18.000], mean observation: -0.611 [-1.009, 8.308], loss: 0.009889, mean_absolute_error: 0.967089, mean_q: 1.033923\n",
            " 113993/150000: episode: 2309, duration: 0.517s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.767 [0.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006341, mean_absolute_error: 0.959468, mean_q: 1.029989\n",
            " 114030/150000: episode: 2310, duration: 0.582s, episode steps: 37, steps per second: 64, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 6.946 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.008295, mean_absolute_error: 0.962014, mean_q: 1.039210\n",
            " 114057/150000: episode: 2311, duration: 0.527s, episode steps: 27, steps per second: 51, episode reward: 1.000, mean reward: 0.037 [0.000, 0.900], mean action: 4.556 [2.000, 14.000], mean observation: -0.634 [-1.009, 1.023], loss: 0.006991, mean_absolute_error: 0.960577, mean_q: 1.033057\n",
            " 114130/150000: episode: 2312, duration: 1.052s, episode steps: 73, steps per second: 69, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 7.110 [2.000, 18.000], mean observation: -0.638 [-1.009, 1.000], loss: 0.008258, mean_absolute_error: 0.960983, mean_q: 1.034141\n",
            " 114162/150000: episode: 2313, duration: 0.524s, episode steps: 32, steps per second: 61, episode reward: 2.000, mean reward: 0.062 [0.000, 1.100], mean action: 5.625 [2.000, 15.000], mean observation: -0.633 [-1.011, 2.362], loss: 0.008200, mean_absolute_error: 0.958146, mean_q: 1.028547\n",
            " 114268/150000: episode: 2314, duration: 1.475s, episode steps: 106, steps per second: 72, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 14.142 [0.000, 18.000], mean observation: -0.630 [-1.009, 1.000], loss: 0.007652, mean_absolute_error: 0.960446, mean_q: 1.034174\n",
            " 114292/150000: episode: 2315, duration: 0.434s, episode steps: 24, steps per second: 55, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 6.000 [5.000, 13.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006294, mean_absolute_error: 0.956798, mean_q: 1.029762\n",
            " 114358/150000: episode: 2316, duration: 0.995s, episode steps: 66, steps per second: 66, episode reward: 0.900, mean reward: 0.014 [0.000, 0.900], mean action: 9.000 [5.000, 20.000], mean observation: -0.620 [-1.011, 7.662], loss: 0.006986, mean_absolute_error: 0.955492, mean_q: 1.030570\n",
            " 114467/150000: episode: 2317, duration: 1.546s, episode steps: 109, steps per second: 70, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 12.697 [4.000, 20.000], mean observation: -0.625 [-1.011, 1.013], loss: 0.006426, mean_absolute_error: 0.952062, mean_q: 1.018829\n",
            " 114573/150000: episode: 2318, duration: 1.463s, episode steps: 106, steps per second: 72, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 9.358 [0.000, 17.000], mean observation: -0.633 [-1.009, 1.139], loss: 0.007353, mean_absolute_error: 0.952907, mean_q: 1.022010\n",
            " 114603/150000: episode: 2319, duration: 0.508s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 8.600 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005117, mean_absolute_error: 0.955658, mean_q: 1.030621\n",
            " 114630/150000: episode: 2320, duration: 0.466s, episode steps: 27, steps per second: 58, episode reward: 2.000, mean reward: 0.074 [0.000, 1.000], mean action: 16.074 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.005719, mean_absolute_error: 0.949471, mean_q: 1.017704\n",
            " 114685/150000: episode: 2321, duration: 0.844s, episode steps: 55, steps per second: 65, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 6.891 [4.000, 18.000], mean observation: -0.609 [-1.017, 8.381], loss: 0.007156, mean_absolute_error: 0.951365, mean_q: 1.024856\n",
            " 114758/150000: episode: 2322, duration: 1.063s, episode steps: 73, steps per second: 69, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 9.192 [2.000, 19.000], mean observation: -0.638 [-1.011, 1.000], loss: 0.007571, mean_absolute_error: 0.949862, mean_q: 1.018776\n",
            " 114827/150000: episode: 2323, duration: 1.027s, episode steps: 69, steps per second: 67, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 9.522 [0.000, 20.000], mean observation: -0.614 [-1.025, 8.415], loss: 0.005797, mean_absolute_error: 0.944979, mean_q: 1.010556\n",
            " 114879/150000: episode: 2324, duration: 0.810s, episode steps: 52, steps per second: 64, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 6.558 [1.000, 15.000], mean observation: -0.611 [-1.020, 7.962], loss: 0.005630, mean_absolute_error: 0.940344, mean_q: 1.005992\n",
            " 114909/150000: episode: 2325, duration: 0.504s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.433 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007550, mean_absolute_error: 0.940978, mean_q: 1.014175\n",
            " 114995/150000: episode: 2326, duration: 1.247s, episode steps: 86, steps per second: 69, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 5.605 [2.000, 19.000], mean observation: -0.635 [-1.011, 1.387], loss: 0.006271, mean_absolute_error: 0.942987, mean_q: 1.011129\n",
            " 115025/150000: episode: 2327, duration: 0.526s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.767 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005977, mean_absolute_error: 0.945780, mean_q: 1.012763\n",
            " 115121/150000: episode: 2328, duration: 1.365s, episode steps: 96, steps per second: 70, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 13.000 [1.000, 20.000], mean observation: -0.638 [-1.020, 1.000], loss: 0.007293, mean_absolute_error: 0.940671, mean_q: 1.014950\n",
            " 115208/150000: episode: 2329, duration: 1.271s, episode steps: 87, steps per second: 68, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 8.793 [4.000, 17.000], mean observation: -0.619 [-1.011, 8.257], loss: 0.007616, mean_absolute_error: 0.936455, mean_q: 1.005056\n",
            " 115244/150000: episode: 2330, duration: 0.586s, episode steps: 36, steps per second: 61, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 5.972 [3.000, 17.000], mean observation: -0.633 [-1.011, 1.008], loss: 0.006242, mean_absolute_error: 0.936827, mean_q: 1.010725\n",
            " 115328/150000: episode: 2331, duration: 1.214s, episode steps: 84, steps per second: 69, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 6.440 [2.000, 20.000], mean observation: -0.615 [-1.046, 8.442], loss: 0.006954, mean_absolute_error: 0.931269, mean_q: 1.001368\n",
            " 115358/150000: episode: 2332, duration: 0.514s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.433 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.003332, mean_absolute_error: 0.930805, mean_q: 1.010231\n",
            " 115388/150000: episode: 2333, duration: 0.509s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.500 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004609, mean_absolute_error: 0.941434, mean_q: 1.004705\n",
            " 115427/150000: episode: 2334, duration: 0.672s, episode steps: 39, steps per second: 58, episode reward: 1.000, mean reward: 0.026 [0.000, 0.900], mean action: 12.154 [1.000, 20.000], mean observation: -0.632 [-1.011, 1.013], loss: 0.007795, mean_absolute_error: 0.935697, mean_q: 1.005180\n",
            " 115528/150000: episode: 2335, duration: 1.415s, episode steps: 101, steps per second: 71, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 8.653 [0.000, 19.000], mean observation: -0.637 [-1.009, 1.227], loss: 0.004966, mean_absolute_error: 0.929166, mean_q: 0.999637\n",
            " 115563/150000: episode: 2336, duration: 0.571s, episode steps: 35, steps per second: 61, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 9.571 [4.000, 18.000], mean observation: -0.633 [-1.011, 1.007], loss: 0.004407, mean_absolute_error: 0.922256, mean_q: 0.994093\n",
            " 115671/150000: episode: 2337, duration: 1.545s, episode steps: 108, steps per second: 70, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 15.102 [0.000, 18.000], mean observation: -0.633 [-1.011, 1.523], loss: 0.007033, mean_absolute_error: 0.927879, mean_q: 1.004370\n",
            " 115687/150000: episode: 2338, duration: 0.341s, episode steps: 16, steps per second: 47, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 12.562 [4.000, 18.000], mean observation: -0.635 [-1.039, 1.000], loss: 0.005432, mean_absolute_error: 0.938206, mean_q: 1.013246\n",
            " 115699/150000: episode: 2339, duration: 0.302s, episode steps: 12, steps per second: 40, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.009410, mean_absolute_error: 0.932790, mean_q: 1.014330\n",
            " 115712/150000: episode: 2340, duration: 0.288s, episode steps: 13, steps per second: 45, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 11.615 [5.000, 18.000], mean observation: -0.634 [-1.037, 1.330], loss: 0.008468, mean_absolute_error: 0.933829, mean_q: 1.002209\n",
            " 115851/150000: episode: 2341, duration: 1.945s, episode steps: 139, steps per second: 71, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 6.669 [1.000, 17.000], mean observation: -0.630 [-1.011, 1.193], loss: 0.007085, mean_absolute_error: 0.931662, mean_q: 1.003462\n",
            " 115950/150000: episode: 2342, duration: 1.406s, episode steps: 99, steps per second: 70, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 16.141 [1.000, 19.000], mean observation: -0.634 [-1.009, 1.064], loss: 0.007029, mean_absolute_error: 0.932459, mean_q: 1.012722\n",
            " 116007/150000: episode: 2343, duration: 0.889s, episode steps: 57, steps per second: 64, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 11.579 [5.000, 17.000], mean observation: -0.632 [-1.009, 1.013], loss: 0.007947, mean_absolute_error: 0.941576, mean_q: 1.015005\n",
            " 116017/150000: episode: 2344, duration: 0.255s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 5.200 [0.000, 12.000], mean observation: -0.635 [-1.011, 1.008], loss: 0.004161, mean_absolute_error: 0.938666, mean_q: 1.005916\n",
            " 116037/150000: episode: 2345, duration: 0.402s, episode steps: 20, steps per second: 50, episode reward: 2.000, mean reward: 0.100 [0.000, 1.000], mean action: 7.750 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.008376, mean_absolute_error: 0.935509, mean_q: 1.005664\n",
            " 116148/150000: episode: 2346, duration: 1.541s, episode steps: 111, steps per second: 72, episode reward: 2.000, mean reward: 0.018 [0.000, 1.000], mean action: 13.117 [0.000, 19.000], mean observation: -0.631 [-1.011, 1.318], loss: 0.004944, mean_absolute_error: 0.932659, mean_q: 1.002690\n",
            " 116166/150000: episode: 2347, duration: 0.378s, episode steps: 18, steps per second: 48, episode reward: 2.000, mean reward: 0.111 [0.000, 1.000], mean action: 14.000 [5.000, 20.000], mean observation: -0.636 [-1.011, 1.006], loss: 0.007929, mean_absolute_error: 0.932110, mean_q: 1.001576\n",
            " 116258/150000: episode: 2348, duration: 1.308s, episode steps: 92, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.859 [1.000, 20.000], mean observation: -0.632 [-1.011, 1.033], loss: 0.006638, mean_absolute_error: 0.932852, mean_q: 1.008431\n",
            " 116304/150000: episode: 2349, duration: 0.706s, episode steps: 46, steps per second: 65, episode reward: 0.900, mean reward: 0.020 [0.000, 0.900], mean action: 5.478 [2.000, 17.000], mean observation: -0.602 [-1.100, 9.435], loss: 0.006452, mean_absolute_error: 0.934664, mean_q: 1.008458\n",
            " 116328/150000: episode: 2350, duration: 0.428s, episode steps: 24, steps per second: 56, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 10.667 [5.000, 16.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.006755, mean_absolute_error: 0.937963, mean_q: 1.005377\n",
            " 116358/150000: episode: 2351, duration: 0.516s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 12.633 [0.000, 19.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008904, mean_absolute_error: 0.934398, mean_q: 1.003454\n",
            " 116492/150000: episode: 2352, duration: 1.852s, episode steps: 134, steps per second: 72, episode reward: 2.000, mean reward: 0.015 [0.000, 1.000], mean action: 4.149 [0.000, 20.000], mean observation: -0.629 [-1.011, 1.012], loss: 0.006420, mean_absolute_error: 0.934361, mean_q: 1.004859\n",
            " 116530/150000: episode: 2353, duration: 0.645s, episode steps: 38, steps per second: 59, episode reward: 1.000, mean reward: 0.026 [0.000, 0.900], mean action: 4.684 [4.000, 14.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.008273, mean_absolute_error: 0.936026, mean_q: 1.006490\n",
            " 116559/150000: episode: 2354, duration: 0.483s, episode steps: 29, steps per second: 60, episode reward: 2.000, mean reward: 0.069 [0.000, 1.000], mean action: 5.621 [3.000, 15.000], mean observation: -0.634 [-1.009, 1.021], loss: 0.006984, mean_absolute_error: 0.927769, mean_q: 1.002383\n",
            " 116573/150000: episode: 2355, duration: 0.300s, episode steps: 14, steps per second: 47, episode reward: 2.000, mean reward: 0.143 [0.000, 1.000], mean action: 10.429 [5.000, 12.000], mean observation: -0.636 [-1.041, 1.011], loss: 0.001927, mean_absolute_error: 0.934384, mean_q: 1.006194\n",
            " 116585/150000: episode: 2356, duration: 0.272s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.139], loss: 0.010368, mean_absolute_error: 0.940630, mean_q: 1.017531\n",
            " 116665/150000: episode: 2357, duration: 1.141s, episode steps: 80, steps per second: 70, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 8.812 [2.000, 20.000], mean observation: -0.636 [-1.009, 1.000], loss: 0.007922, mean_absolute_error: 0.936145, mean_q: 1.009970\n",
            " 116744/150000: episode: 2358, duration: 1.148s, episode steps: 79, steps per second: 69, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 8.684 [0.000, 18.000], mean observation: -0.616 [-1.050, 9.102], loss: 0.005805, mean_absolute_error: 0.939437, mean_q: 1.011006\n",
            " 116771/150000: episode: 2359, duration: 0.480s, episode steps: 27, steps per second: 56, episode reward: 2.000, mean reward: 0.074 [0.000, 1.000], mean action: 5.259 [4.000, 13.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.006865, mean_absolute_error: 0.931900, mean_q: 1.009312\n",
            " 116822/150000: episode: 2360, duration: 0.779s, episode steps: 51, steps per second: 65, episode reward: 2.000, mean reward: 0.039 [0.000, 1.000], mean action: 13.373 [1.000, 20.000], mean observation: -0.632 [-1.011, 1.012], loss: 0.007317, mean_absolute_error: 0.930144, mean_q: 1.005387\n",
            " 116848/150000: episode: 2361, duration: 0.455s, episode steps: 26, steps per second: 57, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 11.154 [2.000, 16.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.008763, mean_absolute_error: 0.930188, mean_q: 1.005425\n",
            " 116991/150000: episode: 2362, duration: 1.964s, episode steps: 143, steps per second: 73, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 5.469 [0.000, 19.000], mean observation: -0.630 [-1.009, 1.051], loss: 0.006647, mean_absolute_error: 0.926919, mean_q: 0.994175\n",
            " 117015/150000: episode: 2363, duration: 0.434s, episode steps: 24, steps per second: 55, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 5.792 [2.000, 13.000], mean observation: -0.635 [-1.011, 1.007], loss: 0.007187, mean_absolute_error: 0.913512, mean_q: 0.979997\n",
            " 117089/150000: episode: 2364, duration: 1.099s, episode steps: 74, steps per second: 67, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 5.230 [1.000, 20.000], mean observation: -0.614 [-1.011, 8.397], loss: 0.006444, mean_absolute_error: 0.918372, mean_q: 0.988471\n",
            " 117127/150000: episode: 2365, duration: 0.598s, episode steps: 38, steps per second: 64, episode reward: 2.000, mean reward: 0.053 [0.000, 1.000], mean action: 6.211 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.007217, mean_absolute_error: 0.918706, mean_q: 0.995138\n",
            " 117140/150000: episode: 2366, duration: 0.296s, episode steps: 13, steps per second: 44, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 5.846 [5.000, 12.000], mean observation: -0.634 [-1.039, 1.050], loss: 0.006092, mean_absolute_error: 0.927398, mean_q: 0.997972\n",
            " 117391/150000: episode: 2367, duration: 3.588s, episode steps: 251, steps per second: 70, episode reward: 1.000, mean reward: 0.004 [0.000, 0.900], mean action: 7.020 [0.000, 18.000], mean observation: -0.639 [-1.011, 6.174], loss: 0.006940, mean_absolute_error: 0.925342, mean_q: 0.997934\n",
            " 117536/150000: episode: 2368, duration: 2.002s, episode steps: 145, steps per second: 72, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 5.538 [3.000, 18.000], mean observation: -0.630 [-1.009, 1.378], loss: 0.007068, mean_absolute_error: 0.928251, mean_q: 1.004274\n",
            " 117548/150000: episode: 2369, duration: 0.284s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.009562, mean_absolute_error: 0.932420, mean_q: 1.004687\n",
            " 117578/150000: episode: 2370, duration: 0.510s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.300 [4.000, 15.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008513, mean_absolute_error: 0.933771, mean_q: 1.006562\n",
            " 117610/150000: episode: 2371, duration: 0.546s, episode steps: 32, steps per second: 59, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 4.281 [0.000, 18.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.006616, mean_absolute_error: 0.930938, mean_q: 1.010384\n",
            " 117639/150000: episode: 2372, duration: 0.472s, episode steps: 29, steps per second: 61, episode reward: 2.000, mean reward: 0.069 [0.000, 1.000], mean action: 4.345 [0.000, 12.000], mean observation: -0.633 [-1.009, 1.020], loss: 0.005603, mean_absolute_error: 0.936645, mean_q: 1.010929\n",
            " 117901/150000: episode: 2373, duration: 3.584s, episode steps: 262, steps per second: 73, episode reward: 0.900, mean reward: 0.003 [0.000, 0.900], mean action: 4.756 [0.000, 20.000], mean observation: -0.637 [-1.009, 1.028], loss: 0.008087, mean_absolute_error: 0.921475, mean_q: 0.990616\n",
            " 117957/150000: episode: 2374, duration: 0.851s, episode steps: 56, steps per second: 66, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 5.107 [5.000, 10.000], mean observation: -0.609 [-1.017, 8.381], loss: 0.007616, mean_absolute_error: 0.928187, mean_q: 1.016638\n",
            " 118011/150000: episode: 2375, duration: 0.817s, episode steps: 54, steps per second: 66, episode reward: 2.000, mean reward: 0.037 [0.000, 1.000], mean action: 7.407 [1.000, 17.000], mean observation: -0.633 [-1.011, 1.008], loss: 0.008569, mean_absolute_error: 0.938232, mean_q: 1.014472\n",
            " 118052/150000: episode: 2376, duration: 0.653s, episode steps: 41, steps per second: 63, episode reward: 2.000, mean reward: 0.049 [0.000, 1.000], mean action: 6.780 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.004], loss: 0.006165, mean_absolute_error: 0.943028, mean_q: 1.013154\n",
            " 118131/150000: episode: 2377, duration: 1.126s, episode steps: 79, steps per second: 70, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 9.937 [3.000, 15.000], mean observation: -0.633 [-1.009, 1.385], loss: 0.005652, mean_absolute_error: 0.945355, mean_q: 1.018072\n",
            " 118212/150000: episode: 2378, duration: 1.165s, episode steps: 81, steps per second: 70, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 6.148 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.005223, mean_absolute_error: 0.939685, mean_q: 1.008821\n",
            " 118307/150000: episode: 2379, duration: 1.399s, episode steps: 95, steps per second: 68, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.853 [0.000, 20.000], mean observation: -0.624 [-1.009, 1.036], loss: 0.007171, mean_absolute_error: 0.934242, mean_q: 1.002464\n",
            " 118398/150000: episode: 2380, duration: 1.321s, episode steps: 91, steps per second: 69, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 8.088 [5.000, 17.000], mean observation: -0.625 [-1.011, 2.124], loss: 0.006286, mean_absolute_error: 0.941884, mean_q: 1.017153\n",
            " 118798/150000: episode: 2381, duration: 5.270s, episode steps: 400, steps per second: 76, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 9.690 [0.000, 19.000], mean observation: -0.638 [-1.009, 2.437], loss: 0.005853, mean_absolute_error: 0.939600, mean_q: 1.011547\n",
            " 118891/150000: episode: 2382, duration: 1.350s, episode steps: 93, steps per second: 69, episode reward: -0.100, mean reward: -0.001 [-1.000, 0.900], mean action: 10.430 [1.000, 19.000], mean observation: -0.625 [-1.015, 8.572], loss: 0.007692, mean_absolute_error: 0.941760, mean_q: 1.016719\n",
            " 118934/150000: episode: 2383, duration: 0.657s, episode steps: 43, steps per second: 65, episode reward: 2.000, mean reward: 0.047 [0.000, 1.000], mean action: 6.930 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.007688, mean_absolute_error: 0.939577, mean_q: 1.007208\n",
            " 118988/150000: episode: 2384, duration: 0.845s, episode steps: 54, steps per second: 64, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 5.333 [1.000, 6.000], mean observation: -0.631 [-1.009, 1.014], loss: 0.005988, mean_absolute_error: 0.939755, mean_q: 1.014056\n",
            " 119024/150000: episode: 2385, duration: 0.569s, episode steps: 36, steps per second: 63, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 12.361 [2.000, 17.000], mean observation: -0.633 [-1.011, 1.016], loss: 0.007606, mean_absolute_error: 0.936552, mean_q: 1.009431\n",
            " 119049/150000: episode: 2386, duration: 0.444s, episode steps: 25, steps per second: 56, episode reward: 2.000, mean reward: 0.080 [0.000, 1.000], mean action: 12.480 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.006294, mean_absolute_error: 0.941438, mean_q: 1.013257\n",
            " 119086/150000: episode: 2387, duration: 0.636s, episode steps: 37, steps per second: 58, episode reward: 1.000, mean reward: 0.027 [0.000, 0.900], mean action: 5.459 [2.000, 13.000], mean observation: -0.635 [-1.011, 1.008], loss: 0.006844, mean_absolute_error: 0.940162, mean_q: 1.019674\n",
            " 119123/150000: episode: 2388, duration: 0.577s, episode steps: 37, steps per second: 64, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.243 [1.000, 18.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.006973, mean_absolute_error: 0.939990, mean_q: 1.010870\n",
            " 119158/150000: episode: 2389, duration: 0.612s, episode steps: 35, steps per second: 57, episode reward: 1.000, mean reward: 0.029 [0.000, 0.900], mean action: 6.029 [4.000, 16.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.005738, mean_absolute_error: 0.939747, mean_q: 1.013135\n",
            " 119489/150000: episode: 2390, duration: 4.413s, episode steps: 331, steps per second: 75, episode reward: 0.900, mean reward: 0.003 [0.000, 0.900], mean action: 9.517 [0.000, 20.000], mean observation: -0.637 [-1.009, 7.081], loss: 0.006707, mean_absolute_error: 0.936527, mean_q: 1.006764\n",
            " 119549/150000: episode: 2391, duration: 0.881s, episode steps: 60, steps per second: 68, episode reward: 2.000, mean reward: 0.033 [0.000, 1.000], mean action: 11.750 [5.000, 14.000], mean observation: -0.632 [-1.011, 1.396], loss: 0.005642, mean_absolute_error: 0.940569, mean_q: 1.019081\n",
            " 119579/150000: episode: 2392, duration: 0.498s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.400 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004616, mean_absolute_error: 0.939731, mean_q: 1.015350\n",
            " 119591/150000: episode: 2393, duration: 0.300s, episode steps: 12, steps per second: 40, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 7.333 [0.000, 19.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.003516, mean_absolute_error: 0.937877, mean_q: 1.006346\n",
            " 119742/150000: episode: 2394, duration: 2.191s, episode steps: 151, steps per second: 69, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 8.232 [0.000, 20.000], mean observation: -0.624 [-1.076, 8.890], loss: 0.006282, mean_absolute_error: 0.942856, mean_q: 1.019614\n",
            " 119806/150000: episode: 2395, duration: 1.032s, episode steps: 64, steps per second: 62, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 10.641 [0.000, 20.000], mean observation: -0.617 [-1.021, 7.972], loss: 0.006996, mean_absolute_error: 0.943774, mean_q: 1.012673\n",
            " 119843/150000: episode: 2396, duration: 0.620s, episode steps: 37, steps per second: 60, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 7.405 [3.000, 18.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.006451, mean_absolute_error: 0.945598, mean_q: 1.016503\n",
            " 119958/150000: episode: 2397, duration: 1.673s, episode steps: 115, steps per second: 69, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 11.965 [0.000, 18.000], mean observation: -0.634 [-1.011, 1.256], loss: 0.007797, mean_absolute_error: 0.940108, mean_q: 1.014964\n",
            " 119988/150000: episode: 2398, duration: 0.516s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.767 [3.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005675, mean_absolute_error: 0.944987, mean_q: 1.015985\n",
            " 120095/150000: episode: 2399, duration: 1.480s, episode steps: 107, steps per second: 72, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 9.748 [0.000, 20.000], mean observation: -0.632 [-1.011, 1.049], loss: 0.005506, mean_absolute_error: 0.943728, mean_q: 1.015933\n",
            " 120154/150000: episode: 2400, duration: 0.898s, episode steps: 59, steps per second: 66, episode reward: 1.000, mean reward: 0.017 [0.000, 0.900], mean action: 9.797 [3.000, 17.000], mean observation: -0.609 [-1.015, 8.397], loss: 0.007776, mean_absolute_error: 0.942848, mean_q: 1.014218\n",
            " 120184/150000: episode: 2401, duration: 0.498s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.733 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.009220, mean_absolute_error: 0.941753, mean_q: 1.018712\n",
            " 120243/150000: episode: 2402, duration: 0.911s, episode steps: 59, steps per second: 65, episode reward: 1.000, mean reward: 0.017 [0.000, 0.900], mean action: 6.559 [2.000, 13.000], mean observation: -0.633 [-1.009, 1.000], loss: 0.007227, mean_absolute_error: 0.948896, mean_q: 1.023717\n",
            " 120289/150000: episode: 2403, duration: 0.744s, episode steps: 46, steps per second: 62, episode reward: 1.000, mean reward: 0.022 [0.000, 0.900], mean action: 12.283 [0.000, 17.000], mean observation: -0.631 [-1.011, 1.011], loss: 0.005795, mean_absolute_error: 0.944144, mean_q: 1.014375\n",
            " 120410/150000: episode: 2404, duration: 1.675s, episode steps: 121, steps per second: 72, episode reward: 0.900, mean reward: 0.007 [0.000, 0.900], mean action: 6.281 [3.000, 20.000], mean observation: -0.634 [-1.011, 1.318], loss: 0.005794, mean_absolute_error: 0.944964, mean_q: 1.012808\n",
            " 120543/150000: episode: 2405, duration: 1.838s, episode steps: 133, steps per second: 72, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 9.278 [3.000, 20.000], mean observation: -0.633 [-1.011, 1.197], loss: 0.006279, mean_absolute_error: 0.945090, mean_q: 1.018006\n",
            " 120644/150000: episode: 2406, duration: 1.463s, episode steps: 101, steps per second: 69, episode reward: 0.000, mean reward: 0.000 [-1.000, 0.900], mean action: 7.307 [1.000, 19.000], mean observation: -0.624 [-1.011, 8.531], loss: 0.006282, mean_absolute_error: 0.941899, mean_q: 1.007278\n",
            " 120694/150000: episode: 2407, duration: 0.797s, episode steps: 50, steps per second: 63, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 12.180 [4.000, 20.000], mean observation: -0.611 [-1.011, 7.662], loss: 0.007114, mean_absolute_error: 0.939621, mean_q: 1.013022\n",
            " 120721/150000: episode: 2408, duration: 0.497s, episode steps: 27, steps per second: 54, episode reward: 1.000, mean reward: 0.037 [0.000, 0.900], mean action: 15.259 [3.000, 18.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.009559, mean_absolute_error: 0.938500, mean_q: 1.015410\n",
            " 120789/150000: episode: 2409, duration: 0.978s, episode steps: 68, steps per second: 70, episode reward: 0.200, mean reward: 0.003 [0.000, 0.200], mean action: 7.500 [4.000, 18.000], mean observation: -0.637 [-1.009, 1.000], loss: 0.009664, mean_absolute_error: 0.937556, mean_q: 1.012113\n",
            " 120881/150000: episode: 2410, duration: 1.270s, episode steps: 92, steps per second: 72, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 11.728 [0.000, 20.000], mean observation: -0.614 [-1.026, 9.611], loss: 0.007895, mean_absolute_error: 0.935155, mean_q: 1.010347\n",
            " 121021/150000: episode: 2411, duration: 1.953s, episode steps: 140, steps per second: 72, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 7.014 [0.000, 19.000], mean observation: -0.628 [-1.011, 2.395], loss: 0.005551, mean_absolute_error: 0.933299, mean_q: 1.004159\n",
            " 121061/150000: episode: 2412, duration: 0.678s, episode steps: 40, steps per second: 59, episode reward: 1.000, mean reward: 0.025 [0.000, 0.900], mean action: 11.800 [3.000, 15.000], mean observation: -0.632 [-1.009, 1.011], loss: 0.007399, mean_absolute_error: 0.927686, mean_q: 0.994179\n",
            " 121158/150000: episode: 2413, duration: 1.380s, episode steps: 97, steps per second: 70, episode reward: 0.000, mean reward: 0.000 [-1.000, 0.900], mean action: 4.876 [1.000, 16.000], mean observation: -0.624 [-1.011, 8.391], loss: 0.005664, mean_absolute_error: 0.925129, mean_q: 0.997232\n",
            " 121261/150000: episode: 2414, duration: 1.418s, episode steps: 103, steps per second: 73, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 7.437 [0.000, 20.000], mean observation: -0.631 [-1.009, 1.000], loss: 0.006928, mean_absolute_error: 0.920652, mean_q: 0.989901\n",
            " 121321/150000: episode: 2415, duration: 0.847s, episode steps: 60, steps per second: 71, episode reward: 2.000, mean reward: 0.033 [0.000, 1.000], mean action: 8.267 [0.000, 14.000], mean observation: -0.632 [-1.009, 1.015], loss: 0.006112, mean_absolute_error: 0.919984, mean_q: 0.991738\n",
            " 121424/150000: episode: 2416, duration: 1.484s, episode steps: 103, steps per second: 69, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 6.913 [1.000, 19.000], mean observation: -0.629 [-1.011, 1.000], loss: 0.007668, mean_absolute_error: 0.920288, mean_q: 0.999924\n",
            " 121437/150000: episode: 2417, duration: 0.289s, episode steps: 13, steps per second: 45, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 5.538 [5.000, 12.000], mean observation: -0.635 [-1.039, 1.000], loss: 0.001285, mean_absolute_error: 0.923833, mean_q: 0.998986\n",
            " 121517/150000: episode: 2418, duration: 1.184s, episode steps: 80, steps per second: 68, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 12.562 [0.000, 20.000], mean observation: -0.627 [-1.009, 1.014], loss: 0.007026, mean_absolute_error: 0.928561, mean_q: 1.002959\n",
            " 121610/150000: episode: 2419, duration: 1.332s, episode steps: 93, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 10.516 [1.000, 20.000], mean observation: -0.631 [-1.011, 1.130], loss: 0.006390, mean_absolute_error: 0.930819, mean_q: 1.004625\n",
            " 121642/150000: episode: 2420, duration: 0.519s, episode steps: 32, steps per second: 62, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 5.969 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.006536, mean_absolute_error: 0.929026, mean_q: 1.001441\n",
            " 121674/150000: episode: 2421, duration: 0.534s, episode steps: 32, steps per second: 60, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 11.031 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.007188, mean_absolute_error: 0.927823, mean_q: 0.995351\n",
            " 121763/150000: episode: 2422, duration: 1.264s, episode steps: 89, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 11.427 [0.000, 19.000], mean observation: -0.633 [-1.009, 1.219], loss: 0.007579, mean_absolute_error: 0.923810, mean_q: 0.988205\n",
            " 121802/150000: episode: 2423, duration: 0.617s, episode steps: 39, steps per second: 63, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 5.205 [2.000, 14.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008354, mean_absolute_error: 0.915122, mean_q: 0.988963\n",
            " 121878/150000: episode: 2424, duration: 1.102s, episode steps: 76, steps per second: 69, episode reward: 2.000, mean reward: 0.026 [0.000, 1.000], mean action: 4.671 [0.000, 20.000], mean observation: -0.632 [-1.011, 1.010], loss: 0.005502, mean_absolute_error: 0.916576, mean_q: 0.984434\n",
            " 121888/150000: episode: 2425, duration: 0.260s, episode steps: 10, steps per second: 38, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 8.400 [1.000, 12.000], mean observation: -0.635 [-1.011, 1.007], loss: 0.005518, mean_absolute_error: 0.913090, mean_q: 0.980504\n",
            " 121900/150000: episode: 2426, duration: 0.279s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.750 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.021], loss: 0.005628, mean_absolute_error: 0.915356, mean_q: 0.983270\n",
            " 121969/150000: episode: 2427, duration: 1.032s, episode steps: 69, steps per second: 67, episode reward: 2.000, mean reward: 0.029 [0.000, 1.000], mean action: 5.188 [1.000, 20.000], mean observation: -0.632 [-1.011, 1.014], loss: 0.005846, mean_absolute_error: 0.919343, mean_q: 0.986506\n",
            " 122116/150000: episode: 2428, duration: 2.044s, episode steps: 147, steps per second: 72, episode reward: 2.000, mean reward: 0.014 [0.000, 1.000], mean action: 8.163 [0.000, 18.000], mean observation: -0.629 [-1.011, 1.233], loss: 0.007860, mean_absolute_error: 0.913296, mean_q: 0.982540\n",
            " 122130/150000: episode: 2429, duration: 0.303s, episode steps: 14, steps per second: 46, episode reward: 2.000, mean reward: 0.143 [0.000, 1.000], mean action: 5.429 [4.000, 12.000], mean observation: -0.636 [-1.040, 1.000], loss: 0.003804, mean_absolute_error: 0.904395, mean_q: 0.987814\n",
            " 122146/150000: episode: 2430, duration: 0.335s, episode steps: 16, steps per second: 48, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 10.688 [5.000, 17.000], mean observation: -0.636 [-1.040, 1.003], loss: 0.005288, mean_absolute_error: 0.910344, mean_q: 0.978753\n",
            " 122172/150000: episode: 2431, duration: 0.470s, episode steps: 26, steps per second: 55, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 15.538 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.005988, mean_absolute_error: 0.911438, mean_q: 0.982411\n",
            " 122184/150000: episode: 2432, duration: 0.278s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.062], loss: 0.006823, mean_absolute_error: 0.906896, mean_q: 0.971635\n",
            " 122194/150000: episode: 2433, duration: 0.256s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 7.800 [2.000, 12.000], mean observation: -0.635 [-1.011, 1.012], loss: 0.005933, mean_absolute_error: 0.909713, mean_q: 0.988280\n",
            " 122222/150000: episode: 2434, duration: 0.517s, episode steps: 28, steps per second: 54, episode reward: 1.000, mean reward: 0.036 [0.000, 0.900], mean action: 10.643 [4.000, 17.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.006265, mean_absolute_error: 0.910324, mean_q: 0.973446\n",
            " 122248/150000: episode: 2435, duration: 0.461s, episode steps: 26, steps per second: 56, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 16.962 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.007], loss: 0.004341, mean_absolute_error: 0.910772, mean_q: 0.972100\n",
            " 122299/150000: episode: 2436, duration: 0.766s, episode steps: 51, steps per second: 67, episode reward: 2.000, mean reward: 0.039 [0.000, 1.000], mean action: 7.706 [5.000, 14.000], mean observation: -0.632 [-1.009, 1.016], loss: 0.008770, mean_absolute_error: 0.901600, mean_q: 0.973915\n",
            " 122438/150000: episode: 2437, duration: 1.943s, episode steps: 139, steps per second: 72, episode reward: 2.000, mean reward: 0.014 [0.000, 1.000], mean action: 6.158 [1.000, 20.000], mean observation: -0.630 [-1.011, 1.008], loss: 0.007084, mean_absolute_error: 0.897058, mean_q: 0.968197\n",
            " 122568/150000: episode: 2438, duration: 1.804s, episode steps: 130, steps per second: 72, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 10.262 [1.000, 20.000], mean observation: -0.628 [-1.009, 1.005], loss: 0.006652, mean_absolute_error: 0.899232, mean_q: 0.969703\n",
            " 122663/150000: episode: 2439, duration: 1.345s, episode steps: 95, steps per second: 71, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 6.968 [0.000, 15.000], mean observation: -0.632 [-1.011, 1.000], loss: 0.007234, mean_absolute_error: 0.895023, mean_q: 0.961170\n",
            " 122737/150000: episode: 2440, duration: 1.129s, episode steps: 74, steps per second: 66, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 7.486 [3.000, 18.000], mean observation: -0.618 [-1.011, 7.922], loss: 0.006231, mean_absolute_error: 0.887885, mean_q: 0.953823\n",
            " 122790/150000: episode: 2441, duration: 0.829s, episode steps: 53, steps per second: 64, episode reward: 2.000, mean reward: 0.038 [0.000, 1.000], mean action: 5.113 [1.000, 14.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.006832, mean_absolute_error: 0.883581, mean_q: 0.946505\n",
            " 122830/150000: episode: 2442, duration: 0.634s, episode steps: 40, steps per second: 63, episode reward: 2.000, mean reward: 0.050 [0.000, 1.000], mean action: 5.900 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.009], loss: 0.006747, mean_absolute_error: 0.881115, mean_q: 0.945126\n",
            " 122860/150000: episode: 2443, duration: 0.491s, episode steps: 30, steps per second: 61, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.500 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.004868, mean_absolute_error: 0.883341, mean_q: 0.965039\n",
            " 123008/150000: episode: 2444, duration: 2.042s, episode steps: 148, steps per second: 72, episode reward: 2.000, mean reward: 0.014 [0.000, 1.000], mean action: 5.439 [1.000, 20.000], mean observation: -0.631 [-1.012, 1.102], loss: 0.006996, mean_absolute_error: 0.883348, mean_q: 0.948954\n",
            " 123118/150000: episode: 2445, duration: 1.577s, episode steps: 110, steps per second: 70, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 15.345 [4.000, 18.000], mean observation: -0.630 [-1.011, 2.445], loss: 0.004802, mean_absolute_error: 0.880865, mean_q: 0.950083\n",
            " 123148/150000: episode: 2446, duration: 0.496s, episode steps: 30, steps per second: 61, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 17.200 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004011, mean_absolute_error: 0.886811, mean_q: 0.960810\n",
            " 123178/150000: episode: 2447, duration: 0.522s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 14.567 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.008787, mean_absolute_error: 0.890571, mean_q: 0.958890\n",
            " 123289/150000: episode: 2448, duration: 1.567s, episode steps: 111, steps per second: 71, episode reward: 0.000, mean reward: 0.000 [-1.000, 0.900], mean action: 11.964 [0.000, 20.000], mean observation: -0.626 [-1.030, 8.811], loss: 0.007500, mean_absolute_error: 0.893171, mean_q: 0.961738\n",
            " 123374/150000: episode: 2449, duration: 1.214s, episode steps: 85, steps per second: 70, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 10.035 [1.000, 19.000], mean observation: -0.634 [-1.009, 1.367], loss: 0.007507, mean_absolute_error: 0.893602, mean_q: 0.962354\n",
            " 123443/150000: episode: 2450, duration: 1.032s, episode steps: 69, steps per second: 67, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 7.899 [0.000, 18.000], mean observation: -0.641 [-1.011, 1.000], loss: 0.007240, mean_absolute_error: 0.890205, mean_q: 0.958923\n",
            " 123527/150000: episode: 2451, duration: 1.293s, episode steps: 84, steps per second: 65, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 4.333 [1.000, 16.000], mean observation: -0.616 [-1.009, 8.353], loss: 0.006253, mean_absolute_error: 0.896629, mean_q: 0.966997\n",
            " 123539/150000: episode: 2452, duration: 0.280s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.250 [5.000, 13.000], mean observation: -0.631 [-1.041, 2.175], loss: 0.002473, mean_absolute_error: 0.897229, mean_q: 0.964921\n",
            " 123567/150000: episode: 2453, duration: 0.468s, episode steps: 28, steps per second: 60, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 12.571 [5.000, 20.000], mean observation: -0.634 [-1.009, 1.016], loss: 0.005277, mean_absolute_error: 0.891612, mean_q: 0.959854\n",
            " 123617/150000: episode: 2454, duration: 0.774s, episode steps: 50, steps per second: 65, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 7.420 [5.000, 14.000], mean observation: -0.612 [-1.011, 7.668], loss: 0.005389, mean_absolute_error: 0.892932, mean_q: 0.955368\n",
            " 123645/150000: episode: 2455, duration: 0.494s, episode steps: 28, steps per second: 57, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 13.357 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007599, mean_absolute_error: 0.891854, mean_q: 0.964885\n",
            " 123753/150000: episode: 2456, duration: 1.510s, episode steps: 108, steps per second: 72, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 10.185 [0.000, 19.000], mean observation: -0.617 [-1.009, 8.178], loss: 0.005790, mean_absolute_error: 0.889627, mean_q: 0.960791\n",
            " 123836/150000: episode: 2457, duration: 1.187s, episode steps: 83, steps per second: 70, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 13.024 [0.000, 19.000], mean observation: -0.616 [-1.071, 9.180], loss: 0.007793, mean_absolute_error: 0.889914, mean_q: 0.960962\n",
            " 123858/150000: episode: 2458, duration: 0.418s, episode steps: 22, steps per second: 53, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 9.045 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.015], loss: 0.001931, mean_absolute_error: 0.892912, mean_q: 0.957160\n",
            " 123871/150000: episode: 2459, duration: 0.292s, episode steps: 13, steps per second: 45, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 9.692 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.417], loss: 0.005076, mean_absolute_error: 0.893989, mean_q: 0.957782\n",
            " 124015/150000: episode: 2460, duration: 1.993s, episode steps: 144, steps per second: 72, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 11.729 [0.000, 19.000], mean observation: -0.630 [-1.011, 1.330], loss: 0.004677, mean_absolute_error: 0.891568, mean_q: 0.959309\n",
            " 124116/150000: episode: 2461, duration: 1.445s, episode steps: 101, steps per second: 70, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 7.267 [1.000, 19.000], mean observation: -0.634 [-1.009, 1.006], loss: 0.005364, mean_absolute_error: 0.892830, mean_q: 0.962309\n",
            " 124214/150000: episode: 2462, duration: 1.418s, episode steps: 98, steps per second: 69, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 5.337 [2.000, 19.000], mean observation: -0.634 [-1.009, 1.138], loss: 0.006492, mean_absolute_error: 0.898589, mean_q: 0.969264\n",
            " 124309/150000: episode: 2463, duration: 1.400s, episode steps: 95, steps per second: 68, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 7.547 [5.000, 18.000], mean observation: -0.625 [-1.046, 8.538], loss: 0.006593, mean_absolute_error: 0.897508, mean_q: 0.973083\n",
            " 124386/150000: episode: 2464, duration: 1.146s, episode steps: 77, steps per second: 67, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 7.701 [0.000, 19.000], mean observation: -0.615 [-1.016, 8.385], loss: 0.007318, mean_absolute_error: 0.902984, mean_q: 0.974911\n",
            " 124478/150000: episode: 2465, duration: 1.297s, episode steps: 92, steps per second: 71, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 4.207 [1.000, 19.000], mean observation: -0.635 [-1.009, 1.000], loss: 0.007696, mean_absolute_error: 0.906392, mean_q: 0.976036\n",
            " 124551/150000: episode: 2466, duration: 1.093s, episode steps: 73, steps per second: 67, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 6.740 [5.000, 20.000], mean observation: -0.612 [-1.011, 8.360], loss: 0.007652, mean_absolute_error: 0.902155, mean_q: 0.974427\n",
            " 124575/150000: episode: 2467, duration: 0.434s, episode steps: 24, steps per second: 55, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 9.333 [2.000, 15.000], mean observation: -0.631 [-1.011, 2.325], loss: 0.006841, mean_absolute_error: 0.901136, mean_q: 0.969813\n",
            " 124630/150000: episode: 2468, duration: 0.855s, episode steps: 55, steps per second: 64, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 10.582 [2.000, 15.000], mean observation: -0.613 [-1.009, 7.835], loss: 0.005044, mean_absolute_error: 0.900944, mean_q: 0.961456\n",
            " 124680/150000: episode: 2469, duration: 0.771s, episode steps: 50, steps per second: 65, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 12.900 [3.000, 20.000], mean observation: -0.611 [-1.011, 7.668], loss: 0.008104, mean_absolute_error: 0.902292, mean_q: 0.972925\n",
            " 124738/150000: episode: 2470, duration: 0.861s, episode steps: 58, steps per second: 67, episode reward: 0.900, mean reward: 0.016 [0.000, 0.900], mean action: 15.379 [2.000, 19.000], mean observation: -0.613 [-1.039, 9.219], loss: 0.006538, mean_absolute_error: 0.905997, mean_q: 0.977672\n",
            " 124815/150000: episode: 2471, duration: 1.173s, episode steps: 77, steps per second: 66, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 8.208 [2.000, 20.000], mean observation: -0.618 [-1.011, 8.189], loss: 0.007597, mean_absolute_error: 0.907366, mean_q: 0.982027\n",
            " 124911/150000: episode: 2472, duration: 1.364s, episode steps: 96, steps per second: 70, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 9.281 [1.000, 19.000], mean observation: -0.632 [-1.011, 1.000], loss: 0.004830, mean_absolute_error: 0.909516, mean_q: 0.976632\n",
            " 125005/150000: episode: 2473, duration: 1.333s, episode steps: 94, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 6.000 [0.000, 14.000], mean observation: -0.632 [-1.009, 1.000], loss: 0.006653, mean_absolute_error: 0.908399, mean_q: 0.976861\n",
            " 125035/150000: episode: 2474, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.433 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008460, mean_absolute_error: 0.906499, mean_q: 0.977210\n",
            " 125131/150000: episode: 2475, duration: 1.407s, episode steps: 96, steps per second: 68, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 10.292 [1.000, 20.000], mean observation: -0.633 [-1.011, 1.886], loss: 0.006081, mean_absolute_error: 0.907725, mean_q: 0.981632\n",
            " 125143/150000: episode: 2476, duration: 0.281s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.500 [5.000, 20.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.003643, mean_absolute_error: 0.906881, mean_q: 0.972805\n",
            " 125153/150000: episode: 2477, duration: 0.249s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.100 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.014], loss: 0.013420, mean_absolute_error: 0.905041, mean_q: 0.958938\n",
            " 125441/150000: episode: 2478, duration: 4.048s, episode steps: 288, steps per second: 71, episode reward: 0.200, mean reward: 0.001 [0.000, 0.200], mean action: 8.538 [0.000, 20.000], mean observation: -0.634 [-1.009, 5.974], loss: 0.006475, mean_absolute_error: 0.902157, mean_q: 0.971205\n",
            " 125478/150000: episode: 2479, duration: 0.673s, episode steps: 37, steps per second: 55, episode reward: 1.000, mean reward: 0.027 [0.000, 0.900], mean action: 4.703 [4.000, 18.000], mean observation: -0.633 [-1.009, 1.010], loss: 0.005256, mean_absolute_error: 0.899094, mean_q: 0.970142\n",
            " 125516/150000: episode: 2480, duration: 0.628s, episode steps: 38, steps per second: 61, episode reward: 2.000, mean reward: 0.053 [0.000, 1.000], mean action: 4.605 [3.000, 6.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.006374, mean_absolute_error: 0.900149, mean_q: 0.968200\n",
            " 125615/150000: episode: 2481, duration: 1.486s, episode steps: 99, steps per second: 67, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 7.323 [1.000, 20.000], mean observation: -0.632 [-1.011, 1.168], loss: 0.007582, mean_absolute_error: 0.900059, mean_q: 0.969023\n",
            " 125646/150000: episode: 2482, duration: 0.550s, episode steps: 31, steps per second: 56, episode reward: 2.000, mean reward: 0.065 [0.000, 1.000], mean action: 8.161 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008308, mean_absolute_error: 0.893724, mean_q: 0.959539\n",
            " 125668/150000: episode: 2483, duration: 0.424s, episode steps: 22, steps per second: 52, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 7.727 [0.000, 17.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.003237, mean_absolute_error: 0.894392, mean_q: 0.962072\n",
            " 125700/150000: episode: 2484, duration: 0.604s, episode steps: 32, steps per second: 53, episode reward: 1.000, mean reward: 0.031 [0.000, 0.900], mean action: 3.219 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.008], loss: 0.005963, mean_absolute_error: 0.894678, mean_q: 0.961583\n",
            " 125732/150000: episode: 2485, duration: 0.547s, episode steps: 32, steps per second: 59, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 6.688 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.009049, mean_absolute_error: 0.892496, mean_q: 0.963983\n",
            " 125765/150000: episode: 2486, duration: 0.623s, episode steps: 33, steps per second: 53, episode reward: 1.000, mean reward: 0.030 [0.000, 0.900], mean action: 14.030 [5.000, 16.000], mean observation: -0.632 [-1.011, 1.017], loss: 0.006934, mean_absolute_error: 0.888696, mean_q: 0.959315\n",
            " 125775/150000: episode: 2487, duration: 0.254s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 9.500 [3.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.008336, mean_absolute_error: 0.891786, mean_q: 0.967332\n",
            " 125856/150000: episode: 2488, duration: 1.206s, episode steps: 81, steps per second: 67, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 7.136 [1.000, 20.000], mean observation: -0.635 [-1.009, 1.294], loss: 0.007016, mean_absolute_error: 0.882263, mean_q: 0.953589\n",
            " 125888/150000: episode: 2489, duration: 0.546s, episode steps: 32, steps per second: 59, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 5.188 [0.000, 18.000], mean observation: -0.634 [-1.011, 1.007], loss: 0.009164, mean_absolute_error: 0.885042, mean_q: 0.960586\n",
            " 126057/150000: episode: 2490, duration: 2.421s, episode steps: 169, steps per second: 70, episode reward: 2.000, mean reward: 0.012 [0.000, 1.000], mean action: 7.467 [0.000, 20.000], mean observation: -0.624 [-1.020, 8.043], loss: 0.005973, mean_absolute_error: 0.900097, mean_q: 0.969835\n",
            " 126103/150000: episode: 2491, duration: 0.766s, episode steps: 46, steps per second: 60, episode reward: 1.000, mean reward: 0.022 [0.000, 0.900], mean action: 5.957 [0.000, 19.000], mean observation: -0.633 [-1.009, 1.014], loss: 0.007806, mean_absolute_error: 0.901940, mean_q: 0.965756\n",
            " 126130/150000: episode: 2492, duration: 0.476s, episode steps: 27, steps per second: 57, episode reward: 2.000, mean reward: 0.074 [0.000, 1.000], mean action: 7.519 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.007257, mean_absolute_error: 0.901406, mean_q: 0.966153\n",
            " 126207/150000: episode: 2493, duration: 1.150s, episode steps: 77, steps per second: 67, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 8.078 [1.000, 20.000], mean observation: -0.636 [-1.011, 1.829], loss: 0.006184, mean_absolute_error: 0.891871, mean_q: 0.954380\n",
            " 126284/150000: episode: 2494, duration: 1.142s, episode steps: 77, steps per second: 67, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 6.364 [1.000, 15.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.008512, mean_absolute_error: 0.888852, mean_q: 0.955472\n",
            " 126378/150000: episode: 2495, duration: 1.329s, episode steps: 94, steps per second: 71, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 6.638 [0.000, 20.000], mean observation: -0.630 [-1.009, 1.376], loss: 0.007253, mean_absolute_error: 0.890380, mean_q: 0.964386\n",
            " 126468/150000: episode: 2496, duration: 1.275s, episode steps: 90, steps per second: 71, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 8.944 [2.000, 20.000], mean observation: -0.632 [-1.011, 1.105], loss: 0.007315, mean_absolute_error: 0.895005, mean_q: 0.963501\n",
            " 126538/150000: episode: 2497, duration: 1.036s, episode steps: 70, steps per second: 68, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 10.829 [1.000, 18.000], mean observation: -0.638 [-1.031, 1.000], loss: 0.007059, mean_absolute_error: 0.887654, mean_q: 0.954183\n",
            " 126588/150000: episode: 2498, duration: 0.798s, episode steps: 50, steps per second: 63, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 7.200 [2.000, 20.000], mean observation: -0.611 [-1.009, 7.668], loss: 0.007605, mean_absolute_error: 0.887869, mean_q: 0.954504\n",
            " 126618/150000: episode: 2499, duration: 0.504s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.167 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006734, mean_absolute_error: 0.891484, mean_q: 0.963789\n",
            " 126648/150000: episode: 2500, duration: 0.518s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 7.767 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006831, mean_absolute_error: 0.890061, mean_q: 0.964083\n",
            " 126668/150000: episode: 2501, duration: 0.379s, episode steps: 20, steps per second: 53, episode reward: 2.000, mean reward: 0.100 [0.000, 1.000], mean action: 8.900 [5.000, 20.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.004172, mean_absolute_error: 0.891142, mean_q: 0.962006\n",
            " 126680/150000: episode: 2502, duration: 0.282s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.167 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.009062, mean_absolute_error: 0.890952, mean_q: 0.962759\n",
            " 126706/150000: episode: 2503, duration: 0.466s, episode steps: 26, steps per second: 56, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 4.962 [0.000, 13.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.006684, mean_absolute_error: 0.892027, mean_q: 0.963891\n",
            " 126736/150000: episode: 2504, duration: 0.506s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007047, mean_absolute_error: 0.889476, mean_q: 0.960031\n",
            " 126758/150000: episode: 2505, duration: 0.451s, episode steps: 22, steps per second: 49, episode reward: 1.000, mean reward: 0.045 [0.000, 0.900], mean action: 5.091 [4.000, 13.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.006289, mean_absolute_error: 0.887863, mean_q: 0.957737\n",
            " 126841/150000: episode: 2506, duration: 1.254s, episode steps: 83, steps per second: 66, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 12.277 [4.000, 20.000], mean observation: -0.619 [-1.033, 8.085], loss: 0.007396, mean_absolute_error: 0.889909, mean_q: 0.956021\n",
            " 126956/150000: episode: 2507, duration: 1.646s, episode steps: 115, steps per second: 70, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 6.765 [0.000, 20.000], mean observation: -0.633 [-1.009, 1.182], loss: 0.005847, mean_absolute_error: 0.887307, mean_q: 0.952161\n",
            " 127158/150000: episode: 2508, duration: 2.804s, episode steps: 202, steps per second: 72, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 6.748 [1.000, 20.000], mean observation: -0.633 [-1.011, 3.852], loss: 0.006768, mean_absolute_error: 0.892563, mean_q: 0.962341\n",
            " 127188/150000: episode: 2509, duration: 0.496s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.367 [2.000, 20.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.006965, mean_absolute_error: 0.895826, mean_q: 0.965879\n",
            " 127447/150000: episode: 2510, duration: 3.549s, episode steps: 259, steps per second: 73, episode reward: 0.900, mean reward: 0.003 [0.000, 0.900], mean action: 7.193 [0.000, 20.000], mean observation: -0.639 [-1.011, 1.012], loss: 0.005967, mean_absolute_error: 0.892430, mean_q: 0.960133\n",
            " 127548/150000: episode: 2511, duration: 1.492s, episode steps: 101, steps per second: 68, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 5.347 [0.000, 15.000], mean observation: -0.633 [-1.011, 1.009], loss: 0.006352, mean_absolute_error: 0.890931, mean_q: 0.949568\n",
            " 127612/150000: episode: 2512, duration: 0.968s, episode steps: 64, steps per second: 66, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 7.031 [2.000, 19.000], mean observation: -0.612 [-1.041, 8.373], loss: 0.006358, mean_absolute_error: 0.889167, mean_q: 0.953554\n",
            " 127651/150000: episode: 2513, duration: 0.676s, episode steps: 39, steps per second: 58, episode reward: 1.000, mean reward: 0.026 [0.000, 0.900], mean action: 5.513 [4.000, 20.000], mean observation: -0.634 [-1.009, 1.014], loss: 0.004123, mean_absolute_error: 0.887321, mean_q: 0.948759\n",
            " 127688/150000: episode: 2514, duration: 0.604s, episode steps: 37, steps per second: 61, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.054 [2.000, 8.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.010184, mean_absolute_error: 0.885164, mean_q: 0.951339\n",
            " 127706/150000: episode: 2515, duration: 0.333s, episode steps: 18, steps per second: 54, episode reward: 2.000, mean reward: 0.111 [0.000, 1.100], mean action: 5.222 [2.000, 12.000], mean observation: -0.635 [-1.009, 1.018], loss: 0.008922, mean_absolute_error: 0.881727, mean_q: 0.943944\n",
            " 127792/150000: episode: 2516, duration: 1.242s, episode steps: 86, steps per second: 69, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 10.930 [1.000, 20.000], mean observation: -0.618 [-1.054, 7.941], loss: 0.004586, mean_absolute_error: 0.887306, mean_q: 0.949551\n",
            " 127802/150000: episode: 2517, duration: 0.253s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 12.300 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.098], loss: 0.006627, mean_absolute_error: 0.887900, mean_q: 0.956048\n",
            " 127871/150000: episode: 2518, duration: 1.013s, episode steps: 69, steps per second: 68, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 15.087 [3.000, 20.000], mean observation: -0.612 [-1.061, 9.281], loss: 0.006159, mean_absolute_error: 0.887763, mean_q: 0.958441\n",
            " 127941/150000: episode: 2519, duration: 1.057s, episode steps: 70, steps per second: 66, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 10.257 [1.000, 20.000], mean observation: -0.614 [-1.039, 8.395], loss: 0.007834, mean_absolute_error: 0.876722, mean_q: 0.943964\n",
            " 128035/150000: episode: 2520, duration: 1.334s, episode steps: 94, steps per second: 70, episode reward: 2.000, mean reward: 0.021 [0.000, 1.000], mean action: 5.596 [0.000, 20.000], mean observation: -0.631 [-1.011, 1.004], loss: 0.006424, mean_absolute_error: 0.882189, mean_q: 0.952433\n",
            " 128047/150000: episode: 2521, duration: 0.261s, episode steps: 12, steps per second: 46, episode reward: 2.000, mean reward: 0.167 [0.000, 1.000], mean action: 5.583 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.021], loss: 0.007610, mean_absolute_error: 0.887245, mean_q: 0.951931\n",
            " 128090/150000: episode: 2522, duration: 0.687s, episode steps: 43, steps per second: 63, episode reward: 2.000, mean reward: 0.047 [0.000, 1.000], mean action: 5.930 [1.000, 19.000], mean observation: -0.633 [-1.011, 1.008], loss: 0.007509, mean_absolute_error: 0.891206, mean_q: 0.960497\n",
            " 128172/150000: episode: 2523, duration: 1.187s, episode steps: 82, steps per second: 69, episode reward: 2.000, mean reward: 0.024 [0.000, 1.000], mean action: 10.841 [1.000, 19.000], mean observation: -0.632 [-1.011, 1.013], loss: 0.006121, mean_absolute_error: 0.888483, mean_q: 0.958660\n",
            " 128200/150000: episode: 2524, duration: 0.494s, episode steps: 28, steps per second: 57, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 5.714 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.006598, mean_absolute_error: 0.891952, mean_q: 0.961713\n",
            " 128283/150000: episode: 2525, duration: 1.224s, episode steps: 83, steps per second: 68, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 7.651 [2.000, 19.000], mean observation: -0.618 [-1.043, 8.350], loss: 0.006853, mean_absolute_error: 0.894281, mean_q: 0.965540\n",
            " 128338/150000: episode: 2526, duration: 0.847s, episode steps: 55, steps per second: 65, episode reward: 2.000, mean reward: 0.036 [0.000, 1.000], mean action: 8.582 [1.000, 18.000], mean observation: -0.633 [-1.011, 1.008], loss: 0.007663, mean_absolute_error: 0.892503, mean_q: 0.972836\n",
            " 128419/150000: episode: 2527, duration: 1.167s, episode steps: 81, steps per second: 69, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 6.160 [0.000, 20.000], mean observation: -0.614 [-1.039, 9.096], loss: 0.006624, mean_absolute_error: 0.898471, mean_q: 0.963731\n",
            " 128569/150000: episode: 2528, duration: 2.083s, episode steps: 150, steps per second: 72, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 9.973 [1.000, 19.000], mean observation: -0.625 [-1.009, 8.279], loss: 0.005454, mean_absolute_error: 0.892916, mean_q: 0.957993\n",
            " 128585/150000: episode: 2529, duration: 0.330s, episode steps: 16, steps per second: 48, episode reward: 2.000, mean reward: 0.125 [0.000, 1.100], mean action: 13.000 [1.000, 18.000], mean observation: -0.633 [-1.011, 1.510], loss: 0.004284, mean_absolute_error: 0.888747, mean_q: 0.959496\n",
            " 128635/150000: episode: 2530, duration: 0.833s, episode steps: 50, steps per second: 60, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 7.020 [0.000, 19.000], mean observation: -0.630 [-1.009, 2.788], loss: 0.005053, mean_absolute_error: 0.894026, mean_q: 0.963148\n",
            " 128665/150000: episode: 2531, duration: 0.503s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.800 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.011448, mean_absolute_error: 0.890693, mean_q: 0.958202\n",
            " 128740/150000: episode: 2532, duration: 1.095s, episode steps: 75, steps per second: 68, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 6.467 [2.000, 15.000], mean observation: -0.636 [-1.011, 1.361], loss: 0.006455, mean_absolute_error: 0.893034, mean_q: 0.970104\n",
            " 128770/150000: episode: 2533, duration: 0.505s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 9.400 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007292, mean_absolute_error: 0.891303, mean_q: 0.956211\n",
            " 128888/150000: episode: 2534, duration: 1.717s, episode steps: 118, steps per second: 69, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 7.186 [2.000, 17.000], mean observation: -0.630 [-1.009, 1.288], loss: 0.005933, mean_absolute_error: 0.898305, mean_q: 0.967299\n",
            " 128992/150000: episode: 2535, duration: 1.682s, episode steps: 104, steps per second: 62, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 7.106 [1.000, 20.000], mean observation: -0.632 [-1.011, 4.209], loss: 0.006982, mean_absolute_error: 0.894158, mean_q: 0.964228\n",
            " 129022/150000: episode: 2536, duration: 0.545s, episode steps: 30, steps per second: 55, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.633 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.004020, mean_absolute_error: 0.890617, mean_q: 0.954758\n",
            " 129052/150000: episode: 2537, duration: 0.505s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 7.500 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008266, mean_absolute_error: 0.895822, mean_q: 0.963380\n",
            " 129081/150000: episode: 2538, duration: 0.497s, episode steps: 29, steps per second: 58, episode reward: 2.000, mean reward: 0.069 [0.000, 1.000], mean action: 2.207 [0.000, 14.000], mean observation: -0.634 [-1.011, 1.016], loss: 0.005516, mean_absolute_error: 0.895687, mean_q: 0.963158\n",
            " 129111/150000: episode: 2539, duration: 0.505s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 0.700 [0.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006801, mean_absolute_error: 0.896013, mean_q: 0.965655\n",
            " 129200/150000: episode: 2540, duration: 1.257s, episode steps: 89, steps per second: 71, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 1.427 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.006949, mean_absolute_error: 0.899324, mean_q: 0.969872\n",
            " 129241/150000: episode: 2541, duration: 0.650s, episode steps: 41, steps per second: 63, episode reward: 2.000, mean reward: 0.049 [0.000, 1.000], mean action: 4.927 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.008417, mean_absolute_error: 0.906461, mean_q: 0.982326\n",
            " 129375/150000: episode: 2542, duration: 1.876s, episode steps: 134, steps per second: 71, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 6.440 [5.000, 20.000], mean observation: -0.631 [-1.011, 1.168], loss: 0.008004, mean_absolute_error: 0.907899, mean_q: 0.978268\n",
            " 129421/150000: episode: 2543, duration: 0.761s, episode steps: 46, steps per second: 60, episode reward: 1.000, mean reward: 0.022 [0.000, 0.900], mean action: 6.326 [5.000, 14.000], mean observation: -0.631 [-1.011, 1.010], loss: 0.008474, mean_absolute_error: 0.909188, mean_q: 0.983557\n",
            " 129506/150000: episode: 2544, duration: 1.221s, episode steps: 85, steps per second: 70, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 5.306 [0.000, 18.000], mean observation: -0.637 [-1.009, 1.000], loss: 0.007563, mean_absolute_error: 0.909511, mean_q: 0.980552\n",
            " 129544/150000: episode: 2545, duration: 0.610s, episode steps: 38, steps per second: 62, episode reward: 2.000, mean reward: 0.053 [0.000, 1.000], mean action: 6.447 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.008634, mean_absolute_error: 0.908644, mean_q: 0.975128\n",
            " 129618/150000: episode: 2546, duration: 1.066s, episode steps: 74, steps per second: 69, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 9.081 [4.000, 18.000], mean observation: -0.634 [-1.011, 1.206], loss: 0.006775, mean_absolute_error: 0.908915, mean_q: 0.978095\n",
            " 129704/150000: episode: 2547, duration: 1.278s, episode steps: 86, steps per second: 67, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 8.116 [0.000, 19.000], mean observation: -0.634 [-1.009, 1.276], loss: 0.007122, mean_absolute_error: 0.905691, mean_q: 0.970965\n",
            " 129742/150000: episode: 2548, duration: 0.598s, episode steps: 38, steps per second: 64, episode reward: 2.000, mean reward: 0.053 [0.000, 1.000], mean action: 5.763 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.005112, mean_absolute_error: 0.897432, mean_q: 0.976632\n",
            " 129855/150000: episode: 2549, duration: 1.595s, episode steps: 113, steps per second: 71, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 5.327 [0.000, 16.000], mean observation: -0.607 [-1.017, 8.381], loss: 0.007084, mean_absolute_error: 0.903111, mean_q: 0.978585\n",
            " 129943/150000: episode: 2550, duration: 1.291s, episode steps: 88, steps per second: 68, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 3.955 [0.000, 19.000], mean observation: -0.616 [-1.041, 8.373], loss: 0.007150, mean_absolute_error: 0.905705, mean_q: 0.976044\n",
            " 129973/150000: episode: 2551, duration: 0.505s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 3.000 [0.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008627, mean_absolute_error: 0.908944, mean_q: 0.975621\n",
            " 129993/150000: episode: 2552, duration: 0.384s, episode steps: 20, steps per second: 52, episode reward: 2.000, mean reward: 0.100 [0.000, 1.000], mean action: 5.750 [4.000, 14.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.010020, mean_absolute_error: 0.911077, mean_q: 0.978676\n",
            " 130023/150000: episode: 2553, duration: 0.520s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.633 [3.000, 16.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.006326, mean_absolute_error: 0.910259, mean_q: 0.979357\n",
            " 130053/150000: episode: 2554, duration: 0.505s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.933 [0.000, 19.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006529, mean_absolute_error: 0.913250, mean_q: 0.982861\n",
            " 130092/150000: episode: 2555, duration: 0.646s, episode steps: 39, steps per second: 60, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 14.692 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.007794, mean_absolute_error: 0.914337, mean_q: 0.982957\n",
            " 130153/150000: episode: 2556, duration: 0.978s, episode steps: 61, steps per second: 62, episode reward: 0.900, mean reward: 0.015 [0.000, 0.900], mean action: 9.738 [1.000, 20.000], mean observation: -0.618 [-1.011, 7.668], loss: 0.006361, mean_absolute_error: 0.908860, mean_q: 0.978126\n",
            " 130210/150000: episode: 2557, duration: 0.835s, episode steps: 57, steps per second: 68, episode reward: 0.900, mean reward: 0.016 [0.000, 0.900], mean action: 4.053 [0.000, 20.000], mean observation: -0.608 [-1.073, 9.570], loss: 0.007643, mean_absolute_error: 0.911102, mean_q: 0.981515\n",
            " 130246/150000: episode: 2558, duration: 0.587s, episode steps: 36, steps per second: 61, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 4.472 [1.000, 5.000], mean observation: -0.634 [-1.011, 1.007], loss: 0.004548, mean_absolute_error: 0.913388, mean_q: 0.981900\n",
            " 130356/150000: episode: 2559, duration: 1.562s, episode steps: 110, steps per second: 70, episode reward: 2.000, mean reward: 0.018 [0.000, 1.000], mean action: 11.045 [1.000, 17.000], mean observation: -0.631 [-1.011, 1.097], loss: 0.006663, mean_absolute_error: 0.901739, mean_q: 0.970387\n",
            " 130380/150000: episode: 2560, duration: 0.457s, episode steps: 24, steps per second: 53, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 6.500 [5.000, 14.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.010191, mean_absolute_error: 0.896695, mean_q: 0.973159\n",
            " 130501/150000: episode: 2561, duration: 1.713s, episode steps: 121, steps per second: 71, episode reward: 2.000, mean reward: 0.017 [0.000, 1.000], mean action: 9.132 [1.000, 19.000], mean observation: -0.609 [-1.011, 8.020], loss: 0.007026, mean_absolute_error: 0.903231, mean_q: 0.980661\n",
            " 130513/150000: episode: 2562, duration: 0.277s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.583 [3.000, 12.000], mean observation: -0.635 [-1.011, 1.002], loss: 0.009261, mean_absolute_error: 0.906967, mean_q: 0.988603\n",
            " 130523/150000: episode: 2563, duration: 0.254s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.600 [5.000, 13.000], mean observation: -0.636 [-1.011, 1.011], loss: 0.006911, mean_absolute_error: 0.903502, mean_q: 0.974699\n",
            " 130535/150000: episode: 2564, duration: 0.286s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.008086, mean_absolute_error: 0.904259, mean_q: 0.978454\n",
            " 130547/150000: episode: 2565, duration: 0.289s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.002], loss: 0.003242, mean_absolute_error: 0.906248, mean_q: 0.972898\n",
            " 130556/150000: episode: 2566, duration: 0.232s, episode steps: 9, steps per second: 39, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 5.778 [5.000, 12.000], mean observation: -0.636 [-1.039, 1.000], loss: 0.008568, mean_absolute_error: 0.911140, mean_q: 0.980375\n",
            " 130586/150000: episode: 2567, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.900 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.006645, mean_absolute_error: 0.913889, mean_q: 0.985514\n",
            " 130674/150000: episode: 2568, duration: 1.242s, episode steps: 88, steps per second: 71, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 8.727 [2.000, 20.000], mean observation: -0.634 [-1.009, 1.029], loss: 0.006893, mean_absolute_error: 0.909497, mean_q: 0.977037\n",
            " 130705/150000: episode: 2569, duration: 0.505s, episode steps: 31, steps per second: 61, episode reward: 2.000, mean reward: 0.065 [0.000, 1.000], mean action: 13.097 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.016], loss: 0.007533, mean_absolute_error: 0.910785, mean_q: 0.979979\n",
            " 130735/150000: episode: 2570, duration: 0.509s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.767 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006452, mean_absolute_error: 0.911455, mean_q: 0.983647\n",
            " 130765/150000: episode: 2571, duration: 0.499s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.100 [5.000, 8.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007275, mean_absolute_error: 0.912002, mean_q: 0.986329\n",
            " 130795/150000: episode: 2572, duration: 0.530s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.900 [2.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005760, mean_absolute_error: 0.921571, mean_q: 1.005609\n",
            " 130861/150000: episode: 2573, duration: 0.981s, episode steps: 66, steps per second: 67, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 5.424 [0.000, 15.000], mean observation: -0.612 [-1.009, 8.310], loss: 0.005527, mean_absolute_error: 0.923452, mean_q: 0.996893\n",
            " 130875/150000: episode: 2574, duration: 0.303s, episode steps: 14, steps per second: 46, episode reward: 2.000, mean reward: 0.143 [0.000, 2.000], mean action: 10.500 [4.000, 13.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.008084, mean_absolute_error: 0.923563, mean_q: 0.997145\n",
            " 130915/150000: episode: 2575, duration: 0.645s, episode steps: 40, steps per second: 62, episode reward: 2.000, mean reward: 0.050 [0.000, 1.000], mean action: 5.900 [5.000, 19.000], mean observation: -0.633 [-1.011, 1.009], loss: 0.011136, mean_absolute_error: 0.917689, mean_q: 0.981670\n",
            " 131010/150000: episode: 2576, duration: 1.309s, episode steps: 95, steps per second: 73, episode reward: 2.000, mean reward: 0.021 [0.000, 1.000], mean action: 7.989 [1.000, 16.000], mean observation: -0.625 [-1.011, 1.304], loss: 0.007051, mean_absolute_error: 0.915645, mean_q: 0.988668\n",
            " 131025/150000: episode: 2577, duration: 0.345s, episode steps: 15, steps per second: 43, episode reward: 2.000, mean reward: 0.133 [0.000, 1.000], mean action: 11.867 [0.000, 16.000], mean observation: -0.634 [-1.011, 1.016], loss: 0.010088, mean_absolute_error: 0.912463, mean_q: 0.987080\n",
            " 131035/150000: episode: 2578, duration: 0.247s, episode steps: 10, steps per second: 41, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.100 [1.000, 13.000], mean observation: -0.635 [-1.011, 1.014], loss: 0.002956, mean_absolute_error: 0.915344, mean_q: 0.982192\n",
            " 131061/150000: episode: 2579, duration: 0.501s, episode steps: 26, steps per second: 52, episode reward: 1.000, mean reward: 0.038 [0.000, 0.900], mean action: 12.346 [4.000, 13.000], mean observation: -0.634 [-1.009, 1.010], loss: 0.005007, mean_absolute_error: 0.911536, mean_q: 0.987158\n",
            " 131092/150000: episode: 2580, duration: 0.550s, episode steps: 31, steps per second: 56, episode reward: 0.900, mean reward: 0.029 [0.000, 0.900], mean action: 10.452 [5.000, 13.000], mean observation: -0.631 [-1.011, 2.339], loss: 0.010561, mean_absolute_error: 0.917338, mean_q: 0.981346\n",
            " 131101/150000: episode: 2581, duration: 0.228s, episode steps: 9, steps per second: 40, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 12.667 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.022], loss: 0.001841, mean_absolute_error: 0.910997, mean_q: 0.982246\n",
            " 131113/150000: episode: 2582, duration: 0.281s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.833 [5.000, 17.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.006842, mean_absolute_error: 0.909522, mean_q: 0.976604\n",
            " 131126/150000: episode: 2583, duration: 0.296s, episode steps: 13, steps per second: 44, episode reward: 2.000, mean reward: 0.154 [0.000, 1.100], mean action: 12.077 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.248], loss: 0.006248, mean_absolute_error: 0.910889, mean_q: 0.978353\n",
            " 131135/150000: episode: 2584, duration: 0.217s, episode steps: 9, steps per second: 42, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 6.556 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.019], loss: 0.008547, mean_absolute_error: 0.912622, mean_q: 0.979932\n",
            " 131165/150000: episode: 2585, duration: 0.513s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.367 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006761, mean_absolute_error: 0.914512, mean_q: 0.987345\n",
            " 131195/150000: episode: 2586, duration: 0.500s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.933 [2.000, 6.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007224, mean_absolute_error: 0.914043, mean_q: 0.992008\n",
            " 131412/150000: episode: 2587, duration: 2.956s, episode steps: 217, steps per second: 73, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 5.954 [4.000, 19.000], mean observation: -0.634 [-1.011, 1.938], loss: 0.007482, mean_absolute_error: 0.920687, mean_q: 0.993309\n",
            " 131435/150000: episode: 2588, duration: 0.435s, episode steps: 23, steps per second: 53, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 5.348 [5.000, 13.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.008554, mean_absolute_error: 0.927642, mean_q: 0.994572\n",
            " 131513/150000: episode: 2589, duration: 1.132s, episode steps: 78, steps per second: 69, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 10.962 [3.000, 19.000], mean observation: -0.633 [-1.009, 1.067], loss: 0.007471, mean_absolute_error: 0.923538, mean_q: 0.997671\n",
            " 131549/150000: episode: 2590, duration: 0.568s, episode steps: 36, steps per second: 63, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 7.778 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.007290, mean_absolute_error: 0.923673, mean_q: 0.986294\n",
            " 131579/150000: episode: 2591, duration: 0.508s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 12.200 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008621, mean_absolute_error: 0.918236, mean_q: 0.986644\n",
            " 131695/150000: episode: 2592, duration: 1.643s, episode steps: 116, steps per second: 71, episode reward: 2.000, mean reward: 0.017 [0.000, 1.000], mean action: 10.957 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.055], loss: 0.006259, mean_absolute_error: 0.916333, mean_q: 0.986329\n",
            " 131734/150000: episode: 2593, duration: 0.629s, episode steps: 39, steps per second: 62, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 18.590 [5.000, 20.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.009453, mean_absolute_error: 0.921900, mean_q: 0.997977\n",
            " 131780/150000: episode: 2594, duration: 0.705s, episode steps: 46, steps per second: 65, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 15.022 [0.000, 19.000], mean observation: -0.632 [-1.011, 1.010], loss: 0.008439, mean_absolute_error: 0.926632, mean_q: 1.001073\n",
            " 131810/150000: episode: 2595, duration: 0.500s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.033 [0.000, 6.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004143, mean_absolute_error: 0.932424, mean_q: 0.996916\n",
            " 131825/150000: episode: 2596, duration: 0.324s, episode steps: 15, steps per second: 46, episode reward: 2.000, mean reward: 0.133 [0.000, 2.000], mean action: 10.200 [5.000, 13.000], mean observation: -0.631 [-1.011, 2.185], loss: 0.006370, mean_absolute_error: 0.928698, mean_q: 0.993704\n",
            " 132225/150000: episode: 2597, duration: 5.292s, episode steps: 400, steps per second: 76, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 8.710 [0.000, 19.000], mean observation: -0.631 [-1.011, 1.316], loss: 0.007846, mean_absolute_error: 0.931274, mean_q: 1.004159\n",
            " 132311/150000: episode: 2598, duration: 1.225s, episode steps: 86, steps per second: 70, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 11.093 [1.000, 19.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.006701, mean_absolute_error: 0.934083, mean_q: 1.005817\n",
            " 132349/150000: episode: 2599, duration: 0.642s, episode steps: 38, steps per second: 59, episode reward: 0.900, mean reward: 0.024 [0.000, 0.900], mean action: 7.105 [3.000, 17.000], mean observation: -0.631 [-1.009, 2.911], loss: 0.006842, mean_absolute_error: 0.936568, mean_q: 1.002370\n",
            " 132401/150000: episode: 2600, duration: 0.809s, episode steps: 52, steps per second: 64, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 11.500 [1.000, 19.000], mean observation: -0.610 [-1.009, 8.189], loss: 0.005500, mean_absolute_error: 0.928192, mean_q: 0.994574\n",
            " 132452/150000: episode: 2601, duration: 0.826s, episode steps: 51, steps per second: 62, episode reward: 2.000, mean reward: 0.039 [0.000, 1.000], mean action: 10.941 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.008945, mean_absolute_error: 0.923817, mean_q: 1.000021\n",
            " 132563/150000: episode: 2602, duration: 1.563s, episode steps: 111, steps per second: 71, episode reward: 2.000, mean reward: 0.018 [0.000, 1.000], mean action: 8.550 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.006681, mean_absolute_error: 0.920922, mean_q: 0.984369\n",
            " 132573/150000: episode: 2603, duration: 0.252s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 2.000], mean action: 3.000 [2.000, 12.000], mean observation: -0.632 [-1.011, 1.175], loss: 0.011143, mean_absolute_error: 0.912495, mean_q: 0.979394\n",
            " 132629/150000: episode: 2604, duration: 0.847s, episode steps: 56, steps per second: 66, episode reward: 0.900, mean reward: 0.016 [0.000, 0.900], mean action: 5.268 [2.000, 18.000], mean observation: -0.607 [-1.094, 9.540], loss: 0.006301, mean_absolute_error: 0.906309, mean_q: 0.977314\n",
            " 132687/150000: episode: 2605, duration: 0.874s, episode steps: 58, steps per second: 66, episode reward: 2.000, mean reward: 0.034 [0.000, 1.000], mean action: 4.586 [0.000, 19.000], mean observation: -0.632 [-1.011, 1.007], loss: 0.007034, mean_absolute_error: 0.913006, mean_q: 0.982982\n",
            " 132768/150000: episode: 2606, duration: 1.170s, episode steps: 81, steps per second: 69, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 8.185 [2.000, 19.000], mean observation: -0.617 [-1.009, 8.246], loss: 0.006246, mean_absolute_error: 0.909822, mean_q: 0.978677\n",
            " 132793/150000: episode: 2607, duration: 0.449s, episode steps: 25, steps per second: 56, episode reward: 2.000, mean reward: 0.080 [0.000, 1.000], mean action: 8.680 [1.000, 16.000], mean observation: -0.635 [-1.011, 1.005], loss: 0.006634, mean_absolute_error: 0.909196, mean_q: 0.979158\n",
            " 132823/150000: episode: 2608, duration: 0.522s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007678, mean_absolute_error: 0.910892, mean_q: 0.986827\n",
            " 132851/150000: episode: 2609, duration: 0.485s, episode steps: 28, steps per second: 58, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 5.179 [2.000, 13.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.006182, mean_absolute_error: 0.910281, mean_q: 0.991675\n",
            " 132881/150000: episode: 2610, duration: 0.516s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.267 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007984, mean_absolute_error: 0.918226, mean_q: 1.004367\n",
            " 132894/150000: episode: 2611, duration: 0.288s, episode steps: 13, steps per second: 45, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 5.538 [5.000, 12.000], mean observation: -0.636 [-1.046, 1.015], loss: 0.008747, mean_absolute_error: 0.914917, mean_q: 0.983302\n",
            " 132924/150000: episode: 2612, duration: 0.509s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 8.600 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006810, mean_absolute_error: 0.921358, mean_q: 0.989086\n",
            " 132961/150000: episode: 2613, duration: 0.606s, episode steps: 37, steps per second: 61, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 4.946 [3.000, 5.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.003920, mean_absolute_error: 0.924723, mean_q: 0.988963\n",
            " 132998/150000: episode: 2614, duration: 0.602s, episode steps: 37, steps per second: 61, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 4.919 [0.000, 7.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.004895, mean_absolute_error: 0.926893, mean_q: 1.002089\n",
            " 133025/150000: episode: 2615, duration: 0.466s, episode steps: 27, steps per second: 58, episode reward: 2.000, mean reward: 0.074 [0.000, 1.000], mean action: 6.926 [1.000, 13.000], mean observation: -0.634 [-1.011, 1.005], loss: 0.007695, mean_absolute_error: 0.920062, mean_q: 0.990365\n",
            " 133048/150000: episode: 2616, duration: 0.420s, episode steps: 23, steps per second: 55, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 13.522 [5.000, 19.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.007362, mean_absolute_error: 0.919149, mean_q: 0.992505\n",
            " 133087/150000: episode: 2617, duration: 0.648s, episode steps: 39, steps per second: 60, episode reward: 2.000, mean reward: 0.051 [0.000, 1.000], mean action: 18.154 [4.000, 19.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.006709, mean_absolute_error: 0.917581, mean_q: 0.992893\n",
            " 133120/150000: episode: 2618, duration: 0.570s, episode steps: 33, steps per second: 58, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.030 [4.000, 15.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.006681, mean_absolute_error: 0.920124, mean_q: 1.000718\n",
            " 133150/150000: episode: 2619, duration: 0.508s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.733 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008499, mean_absolute_error: 0.914126, mean_q: 1.002610\n",
            " 133170/150000: episode: 2620, duration: 0.383s, episode steps: 20, steps per second: 52, episode reward: 2.000, mean reward: 0.100 [0.000, 1.000], mean action: 8.650 [4.000, 12.000], mean observation: -0.635 [-1.011, 1.010], loss: 0.005780, mean_absolute_error: 0.923247, mean_q: 0.993092\n",
            " 133180/150000: episode: 2621, duration: 0.255s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 11.500 [5.000, 13.000], mean observation: -0.636 [-1.011, 1.016], loss: 0.010632, mean_absolute_error: 0.924373, mean_q: 1.002998\n",
            " 133192/150000: episode: 2622, duration: 0.276s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.667 [5.000, 12.000], mean observation: -0.633 [-1.011, 1.508], loss: 0.010077, mean_absolute_error: 0.924309, mean_q: 1.009909\n",
            " 133208/150000: episode: 2623, duration: 0.333s, episode steps: 16, steps per second: 48, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 9.812 [5.000, 12.000], mean observation: -0.636 [-1.041, 1.009], loss: 0.005673, mean_absolute_error: 0.924217, mean_q: 0.994344\n",
            " 133220/150000: episode: 2624, duration: 0.292s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.002], loss: 0.005998, mean_absolute_error: 0.925213, mean_q: 0.992646\n",
            " 133232/150000: episode: 2625, duration: 0.271s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.008097, mean_absolute_error: 0.926346, mean_q: 0.999246\n",
            " 133257/150000: episode: 2626, duration: 0.448s, episode steps: 25, steps per second: 56, episode reward: 2.000, mean reward: 0.080 [0.000, 1.000], mean action: 9.880 [5.000, 20.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.007475, mean_absolute_error: 0.925045, mean_q: 0.989571\n",
            " 133287/150000: episode: 2627, duration: 0.517s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.700 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.005159, mean_absolute_error: 0.921322, mean_q: 0.990681\n",
            " 133303/150000: episode: 2628, duration: 0.329s, episode steps: 16, steps per second: 49, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 9.250 [5.000, 18.000], mean observation: -0.635 [-1.040, 1.003], loss: 0.004377, mean_absolute_error: 0.923619, mean_q: 0.990522\n",
            " 133590/150000: episode: 2629, duration: 3.862s, episode steps: 287, steps per second: 74, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 6.634 [2.000, 20.000], mean observation: -0.631 [-1.009, 1.256], loss: 0.006643, mean_absolute_error: 0.922447, mean_q: 0.993785\n",
            " 133641/150000: episode: 2630, duration: 0.788s, episode steps: 51, steps per second: 65, episode reward: 2.000, mean reward: 0.039 [0.000, 1.000], mean action: 5.588 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.004956, mean_absolute_error: 0.922970, mean_q: 0.991204\n",
            " 133728/150000: episode: 2631, duration: 1.280s, episode steps: 87, steps per second: 68, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 8.345 [0.000, 20.000], mean observation: -0.632 [-1.011, 1.221], loss: 0.005918, mean_absolute_error: 0.917752, mean_q: 0.977196\n",
            " 133837/150000: episode: 2632, duration: 1.545s, episode steps: 109, steps per second: 71, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 6.798 [3.000, 19.000], mean observation: -0.630 [-1.011, 1.115], loss: 0.006534, mean_absolute_error: 0.915276, mean_q: 0.985409\n",
            " 133870/150000: episode: 2633, duration: 0.533s, episode steps: 33, steps per second: 62, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.424 [1.000, 16.000], mean observation: -0.633 [-1.011, 1.016], loss: 0.009792, mean_absolute_error: 0.921223, mean_q: 0.986202\n",
            " 134012/150000: episode: 2634, duration: 1.990s, episode steps: 142, steps per second: 71, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 11.676 [1.000, 20.000], mean observation: -0.634 [-1.011, 1.392], loss: 0.005916, mean_absolute_error: 0.912692, mean_q: 0.982123\n",
            " 134042/150000: episode: 2635, duration: 0.507s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 11.000 [4.000, 18.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.009725, mean_absolute_error: 0.903334, mean_q: 0.966303\n",
            " 134139/150000: episode: 2636, duration: 1.384s, episode steps: 97, steps per second: 70, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 10.526 [0.000, 20.000], mean observation: -0.632 [-1.011, 1.026], loss: 0.006114, mean_absolute_error: 0.904159, mean_q: 0.976227\n",
            " 134194/150000: episode: 2637, duration: 0.836s, episode steps: 55, steps per second: 66, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 8.382 [5.000, 18.000], mean observation: -0.609 [-1.017, 8.381], loss: 0.005768, mean_absolute_error: 0.903589, mean_q: 0.975447\n",
            " 134557/150000: episode: 2638, duration: 4.830s, episode steps: 363, steps per second: 75, episode reward: 0.900, mean reward: 0.002 [0.000, 0.900], mean action: 6.289 [0.000, 20.000], mean observation: -0.628 [-1.011, 8.338], loss: 0.007179, mean_absolute_error: 0.899818, mean_q: 0.970495\n",
            " 134629/150000: episode: 2639, duration: 1.066s, episode steps: 72, steps per second: 68, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 6.528 [5.000, 20.000], mean observation: -0.609 [-1.056, 9.514], loss: 0.005941, mean_absolute_error: 0.893882, mean_q: 0.962646\n",
            " 134659/150000: episode: 2640, duration: 0.523s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.300 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.007010, mean_absolute_error: 0.892325, mean_q: 0.980194\n",
            " 134691/150000: episode: 2641, duration: 0.539s, episode steps: 32, steps per second: 59, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 9.000 [1.000, 20.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.005930, mean_absolute_error: 0.901185, mean_q: 0.969002\n",
            " 134714/150000: episode: 2642, duration: 0.443s, episode steps: 23, steps per second: 52, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 6.957 [5.000, 20.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.006639, mean_absolute_error: 0.904171, mean_q: 0.983789\n",
            " 134726/150000: episode: 2643, duration: 0.285s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 13.500 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.005799, mean_absolute_error: 0.905515, mean_q: 0.973399\n",
            " 134820/150000: episode: 2644, duration: 1.370s, episode steps: 94, steps per second: 69, episode reward: -0.100, mean reward: -0.001 [-1.000, 0.900], mean action: 9.223 [5.000, 18.000], mean observation: -0.625 [-1.013, 8.619], loss: 0.007060, mean_absolute_error: 0.912447, mean_q: 0.985371\n",
            " 134979/150000: episode: 2645, duration: 2.220s, episode steps: 159, steps per second: 72, episode reward: 2.000, mean reward: 0.013 [0.000, 1.000], mean action: 9.484 [0.000, 16.000], mean observation: -0.631 [-1.011, 1.047], loss: 0.007443, mean_absolute_error: 0.914092, mean_q: 0.989501\n",
            " 135061/150000: episode: 2646, duration: 1.210s, episode steps: 82, steps per second: 68, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 9.159 [2.000, 20.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.006172, mean_absolute_error: 0.920037, mean_q: 0.988928\n",
            " 135091/150000: episode: 2647, duration: 0.506s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.033 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006473, mean_absolute_error: 0.916228, mean_q: 0.992514\n",
            " 135124/150000: episode: 2648, duration: 0.550s, episode steps: 33, steps per second: 60, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 9.030 [5.000, 16.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.005885, mean_absolute_error: 0.926155, mean_q: 1.000549\n",
            " 135140/150000: episode: 2649, duration: 0.327s, episode steps: 16, steps per second: 49, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 14.125 [2.000, 16.000], mean observation: -0.636 [-1.040, 1.000], loss: 0.001805, mean_absolute_error: 0.932359, mean_q: 1.005868\n",
            " 135215/150000: episode: 2650, duration: 1.130s, episode steps: 75, steps per second: 66, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 11.507 [0.000, 20.000], mean observation: -0.636 [-1.009, 1.000], loss: 0.006730, mean_absolute_error: 0.928610, mean_q: 0.997978\n",
            " 135277/150000: episode: 2651, duration: 0.954s, episode steps: 62, steps per second: 65, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 11.290 [1.000, 17.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.007126, mean_absolute_error: 0.930182, mean_q: 1.002388\n",
            " 135427/150000: episode: 2652, duration: 2.136s, episode steps: 150, steps per second: 70, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 11.820 [2.000, 20.000], mean observation: -0.629 [-1.009, 4.165], loss: 0.005942, mean_absolute_error: 0.919855, mean_q: 0.988462\n",
            " 135439/150000: episode: 2653, duration: 0.282s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.083 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.002], loss: 0.008257, mean_absolute_error: 0.915203, mean_q: 0.983543\n",
            " 135469/150000: episode: 2654, duration: 0.525s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.633 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.008], loss: 0.005816, mean_absolute_error: 0.910099, mean_q: 0.975551\n",
            " 135520/150000: episode: 2655, duration: 0.783s, episode steps: 51, steps per second: 65, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 8.667 [0.000, 20.000], mean observation: -0.605 [-1.111, 9.617], loss: 0.006430, mean_absolute_error: 0.907210, mean_q: 0.970101\n",
            " 135548/150000: episode: 2656, duration: 0.492s, episode steps: 28, steps per second: 57, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 7.929 [3.000, 20.000], mean observation: -0.634 [-1.011, 1.007], loss: 0.005396, mean_absolute_error: 0.907294, mean_q: 0.981834\n",
            " 135581/150000: episode: 2657, duration: 0.565s, episode steps: 33, steps per second: 58, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 8.242 [1.000, 13.000], mean observation: -0.633 [-1.011, 1.008], loss: 0.007057, mean_absolute_error: 0.903418, mean_q: 0.977847\n",
            " 135690/150000: episode: 2658, duration: 1.529s, episode steps: 109, steps per second: 71, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 6.367 [2.000, 17.000], mean observation: -0.632 [-1.009, 1.262], loss: 0.006703, mean_absolute_error: 0.910835, mean_q: 0.984104\n",
            " 135773/150000: episode: 2659, duration: 1.198s, episode steps: 83, steps per second: 69, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 6.988 [0.000, 18.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.007303, mean_absolute_error: 0.909316, mean_q: 0.983374\n",
            " 136151/150000: episode: 2660, duration: 5.060s, episode steps: 378, steps per second: 75, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 8.944 [0.000, 20.000], mean observation: -0.635 [-1.029, 9.250], loss: 0.005408, mean_absolute_error: 0.910382, mean_q: 0.977077\n",
            " 136181/150000: episode: 2661, duration: 0.516s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.367 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008345, mean_absolute_error: 0.904982, mean_q: 0.968094\n",
            " 136194/150000: episode: 2662, duration: 0.291s, episode steps: 13, steps per second: 45, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 5.538 [5.000, 12.000], mean observation: -0.634 [-1.034, 1.198], loss: 0.007799, mean_absolute_error: 0.902630, mean_q: 0.967533\n",
            " 136210/150000: episode: 2663, duration: 0.326s, episode steps: 16, steps per second: 49, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 8.062 [0.000, 16.000], mean observation: -0.636 [-1.041, 1.019], loss: 0.006048, mean_absolute_error: 0.903388, mean_q: 0.968082\n",
            " 136240/150000: episode: 2664, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 0.167 [0.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.003319, mean_absolute_error: 0.901511, mean_q: 0.973294\n",
            " 136336/150000: episode: 2665, duration: 1.344s, episode steps: 96, steps per second: 71, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 1.635 [0.000, 16.000], mean observation: -0.616 [-1.026, 9.187], loss: 0.004874, mean_absolute_error: 0.902018, mean_q: 0.963650\n",
            " 136371/150000: episode: 2666, duration: 0.584s, episode steps: 35, steps per second: 60, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 5.371 [3.000, 18.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.011848, mean_absolute_error: 0.890892, mean_q: 0.957505\n",
            " 136402/150000: episode: 2667, duration: 0.520s, episode steps: 31, steps per second: 60, episode reward: 2.000, mean reward: 0.065 [0.000, 1.000], mean action: 6.097 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010214, mean_absolute_error: 0.896158, mean_q: 0.979061\n",
            " 136438/150000: episode: 2668, duration: 0.593s, episode steps: 36, steps per second: 61, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 5.056 [3.000, 8.000], mean observation: -0.634 [-1.011, 1.007], loss: 0.008373, mean_absolute_error: 0.895361, mean_q: 0.974994\n",
            " 136462/150000: episode: 2669, duration: 0.426s, episode steps: 24, steps per second: 56, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 5.792 [5.000, 16.000], mean observation: -0.635 [-1.011, 1.006], loss: 0.006211, mean_absolute_error: 0.899239, mean_q: 0.978003\n",
            " 136565/150000: episode: 2670, duration: 1.501s, episode steps: 103, steps per second: 69, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 6.049 [2.000, 19.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.008427, mean_absolute_error: 0.906567, mean_q: 0.979591\n",
            " 136648/150000: episode: 2671, duration: 1.231s, episode steps: 83, steps per second: 67, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 8.627 [4.000, 19.000], mean observation: -0.614 [-1.009, 8.374], loss: 0.007525, mean_absolute_error: 0.908815, mean_q: 0.974081\n",
            " 136686/150000: episode: 2672, duration: 0.605s, episode steps: 38, steps per second: 63, episode reward: 2.000, mean reward: 0.053 [0.000, 1.000], mean action: 8.289 [3.000, 20.000], mean observation: -0.634 [-1.011, 1.014], loss: 0.005799, mean_absolute_error: 0.909034, mean_q: 0.973786\n",
            " 136790/150000: episode: 2673, duration: 1.465s, episode steps: 104, steps per second: 71, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 9.231 [1.000, 20.000], mean observation: -0.635 [-1.009, 1.000], loss: 0.005931, mean_absolute_error: 0.907550, mean_q: 0.975894\n",
            " 136877/150000: episode: 2674, duration: 1.228s, episode steps: 87, steps per second: 71, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 11.253 [3.000, 19.000], mean observation: -0.637 [-1.011, 1.137], loss: 0.006779, mean_absolute_error: 0.902369, mean_q: 0.978274\n",
            " 136927/150000: episode: 2675, duration: 0.791s, episode steps: 50, steps per second: 63, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 4.800 [2.000, 20.000], mean observation: -0.611 [-1.011, 7.668], loss: 0.006084, mean_absolute_error: 0.904466, mean_q: 0.977449\n",
            " 136947/150000: episode: 2676, duration: 0.386s, episode steps: 20, steps per second: 52, episode reward: 2.000, mean reward: 0.100 [0.000, 1.000], mean action: 7.050 [4.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.008769, mean_absolute_error: 0.901689, mean_q: 0.964511\n",
            " 136977/150000: episode: 2677, duration: 0.526s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.833 [5.000, 15.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.004322, mean_absolute_error: 0.905333, mean_q: 0.977763\n",
            " 137026/150000: episode: 2678, duration: 0.736s, episode steps: 49, steps per second: 67, episode reward: 2.000, mean reward: 0.041 [0.000, 1.000], mean action: 8.510 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.016], loss: 0.007312, mean_absolute_error: 0.903300, mean_q: 0.974163\n",
            " 137088/150000: episode: 2679, duration: 0.958s, episode steps: 62, steps per second: 65, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 8.274 [4.000, 16.000], mean observation: -0.610 [-1.015, 8.397], loss: 0.007187, mean_absolute_error: 0.903231, mean_q: 0.975018\n",
            " 137114/150000: episode: 2680, duration: 0.471s, episode steps: 26, steps per second: 55, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 5.269 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.008809, mean_absolute_error: 0.905163, mean_q: 0.974369\n",
            " 137140/150000: episode: 2681, duration: 0.468s, episode steps: 26, steps per second: 56, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 5.538 [5.000, 13.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.007507, mean_absolute_error: 0.904015, mean_q: 0.981378\n",
            " 137227/150000: episode: 2682, duration: 1.257s, episode steps: 87, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 5.356 [2.000, 17.000], mean observation: -0.632 [-1.011, 1.055], loss: 0.006308, mean_absolute_error: 0.905539, mean_q: 0.974450\n",
            " 137237/150000: episode: 2683, duration: 0.262s, episode steps: 10, steps per second: 38, episode reward: 2.000, mean reward: 0.200 [0.000, 2.000], mean action: 5.700 [5.000, 12.000], mean observation: -0.632 [-1.011, 1.196], loss: 0.005766, mean_absolute_error: 0.911242, mean_q: 0.986739\n",
            " 137263/150000: episode: 2684, duration: 0.474s, episode steps: 26, steps per second: 55, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 6.346 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.007036, mean_absolute_error: 0.905418, mean_q: 0.976378\n",
            " 137293/150000: episode: 2685, duration: 0.518s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.067 [5.000, 7.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.006611, mean_absolute_error: 0.901381, mean_q: 0.966068\n",
            " 137559/150000: episode: 2686, duration: 3.609s, episode steps: 266, steps per second: 74, episode reward: 1.000, mean reward: 0.004 [0.000, 0.900], mean action: 8.504 [0.000, 20.000], mean observation: -0.629 [-1.011, 8.023], loss: 0.006018, mean_absolute_error: 0.908877, mean_q: 0.979225\n",
            " 137630/150000: episode: 2687, duration: 1.079s, episode steps: 71, steps per second: 66, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 14.465 [0.000, 16.000], mean observation: -0.615 [-1.040, 8.378], loss: 0.007189, mean_absolute_error: 0.912092, mean_q: 0.987264\n",
            " 137714/150000: episode: 2688, duration: 1.201s, episode steps: 84, steps per second: 70, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 12.452 [0.000, 19.000], mean observation: -0.635 [-1.011, 1.308], loss: 0.006047, mean_absolute_error: 0.909173, mean_q: 0.973252\n",
            " 137821/150000: episode: 2689, duration: 1.559s, episode steps: 107, steps per second: 69, episode reward: 0.300, mean reward: 0.003 [0.000, 0.200], mean action: 7.841 [0.000, 17.000], mean observation: -0.636 [-1.009, 1.006], loss: 0.007500, mean_absolute_error: 0.913305, mean_q: 0.980730\n",
            " 137851/150000: episode: 2690, duration: 0.509s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005603, mean_absolute_error: 0.912391, mean_q: 0.984439\n",
            " 137883/150000: episode: 2691, duration: 0.544s, episode steps: 32, steps per second: 59, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 5.219 [3.000, 12.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.006555, mean_absolute_error: 0.916824, mean_q: 0.986897\n",
            " 137923/150000: episode: 2692, duration: 0.641s, episode steps: 40, steps per second: 62, episode reward: 2.000, mean reward: 0.050 [0.000, 1.000], mean action: 9.200 [5.000, 18.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.006771, mean_absolute_error: 0.919186, mean_q: 0.993716\n",
            " 138024/150000: episode: 2693, duration: 1.445s, episode steps: 101, steps per second: 70, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 11.119 [4.000, 20.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.006521, mean_absolute_error: 0.921751, mean_q: 0.993570\n",
            " 138118/150000: episode: 2694, duration: 1.334s, episode steps: 94, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 7.053 [1.000, 19.000], mean observation: -0.631 [-1.011, 1.267], loss: 0.005629, mean_absolute_error: 0.926777, mean_q: 0.996041\n",
            " 138242/150000: episode: 2695, duration: 1.717s, episode steps: 124, steps per second: 72, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 4.347 [0.000, 16.000], mean observation: -0.631 [-1.011, 1.037], loss: 0.006510, mean_absolute_error: 0.926027, mean_q: 0.990185\n",
            " 138297/150000: episode: 2696, duration: 0.840s, episode steps: 55, steps per second: 65, episode reward: 1.000, mean reward: 0.018 [0.000, 0.900], mean action: 6.927 [5.000, 17.000], mean observation: -0.609 [-1.017, 8.381], loss: 0.007382, mean_absolute_error: 0.924920, mean_q: 0.996465\n",
            " 138327/150000: episode: 2697, duration: 0.504s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 11.600 [5.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008120, mean_absolute_error: 0.927635, mean_q: 0.998358\n",
            " 138364/150000: episode: 2698, duration: 0.583s, episode steps: 37, steps per second: 63, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 11.027 [2.000, 16.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.008895, mean_absolute_error: 0.930261, mean_q: 1.000001\n",
            " 138395/150000: episode: 2699, duration: 0.522s, episode steps: 31, steps per second: 59, episode reward: 2.000, mean reward: 0.065 [0.000, 1.000], mean action: 9.032 [1.000, 18.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.005656, mean_absolute_error: 0.920830, mean_q: 0.991835\n",
            " 138483/150000: episode: 2700, duration: 1.279s, episode steps: 88, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 7.750 [1.000, 19.000], mean observation: -0.618 [-1.023, 8.314], loss: 0.007106, mean_absolute_error: 0.918967, mean_q: 0.991384\n",
            " 138513/150000: episode: 2701, duration: 0.504s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.500 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008844, mean_absolute_error: 0.906566, mean_q: 0.994755\n",
            " 138543/150000: episode: 2702, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005257, mean_absolute_error: 0.905092, mean_q: 0.979376\n",
            " 138578/150000: episode: 2703, duration: 0.613s, episode steps: 35, steps per second: 57, episode reward: 1.000, mean reward: 0.029 [0.000, 0.900], mean action: 5.429 [3.000, 20.000], mean observation: -0.634 [-1.011, 1.007], loss: 0.009924, mean_absolute_error: 0.907930, mean_q: 0.978368\n",
            " 138681/150000: episode: 2704, duration: 1.508s, episode steps: 103, steps per second: 68, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 5.495 [1.000, 20.000], mean observation: -0.633 [-1.009, 1.279], loss: 0.006314, mean_absolute_error: 0.909704, mean_q: 0.984045\n",
            " 138746/150000: episode: 2705, duration: 0.974s, episode steps: 65, steps per second: 67, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 9.031 [3.000, 19.000], mean observation: -0.612 [-1.020, 8.077], loss: 0.007400, mean_absolute_error: 0.914304, mean_q: 0.982201\n",
            " 138758/150000: episode: 2706, duration: 0.254s, episode steps: 12, steps per second: 47, episode reward: 2.000, mean reward: 0.167 [0.000, 1.000], mean action: 5.583 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.020], loss: 0.011141, mean_absolute_error: 0.915317, mean_q: 0.987823\n",
            " 138791/150000: episode: 2707, duration: 0.538s, episode steps: 33, steps per second: 61, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 5.697 [4.000, 19.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.002590, mean_absolute_error: 0.909852, mean_q: 0.981059\n",
            " 138811/150000: episode: 2708, duration: 0.392s, episode steps: 20, steps per second: 51, episode reward: 2.000, mean reward: 0.100 [0.000, 1.000], mean action: 6.700 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.006299, mean_absolute_error: 0.913005, mean_q: 0.973369\n",
            " 139106/150000: episode: 2709, duration: 4.012s, episode steps: 295, steps per second: 74, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 6.827 [0.000, 20.000], mean observation: -0.626 [-1.048, 10.062], loss: 0.007168, mean_absolute_error: 0.912378, mean_q: 0.984157\n",
            " 139147/150000: episode: 2710, duration: 0.639s, episode steps: 41, steps per second: 64, episode reward: 2.000, mean reward: 0.049 [0.000, 1.000], mean action: 6.049 [2.000, 15.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.007482, mean_absolute_error: 0.923806, mean_q: 1.006482\n",
            " 139177/150000: episode: 2711, duration: 0.510s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.833 [0.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006303, mean_absolute_error: 0.925558, mean_q: 1.001004\n",
            " 139228/150000: episode: 2712, duration: 0.819s, episode steps: 51, steps per second: 62, episode reward: 1.000, mean reward: 0.020 [0.000, 0.900], mean action: 5.608 [0.000, 20.000], mean observation: -0.631 [-1.009, 1.016], loss: 0.006840, mean_absolute_error: 0.926626, mean_q: 0.996793\n",
            " 139303/150000: episode: 2713, duration: 1.104s, episode steps: 75, steps per second: 68, episode reward: 0.900, mean reward: 0.012 [0.000, 0.900], mean action: 9.413 [2.000, 18.000], mean observation: -0.637 [-1.009, 1.000], loss: 0.008535, mean_absolute_error: 0.924051, mean_q: 0.992211\n",
            " 139703/150000: episode: 2714, duration: 5.265s, episode steps: 400, steps per second: 76, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 8.485 [0.000, 20.000], mean observation: -0.631 [-1.011, 1.159], loss: 0.007018, mean_absolute_error: 0.916186, mean_q: 0.982703\n",
            " 139746/150000: episode: 2715, duration: 0.672s, episode steps: 43, steps per second: 64, episode reward: 2.000, mean reward: 0.047 [0.000, 1.000], mean action: 8.814 [2.000, 20.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.005756, mean_absolute_error: 0.907253, mean_q: 0.973756\n",
            " 139756/150000: episode: 2716, duration: 0.259s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 5.100 [0.000, 12.000], mean observation: -0.635 [-1.038, 1.009], loss: 0.008191, mean_absolute_error: 0.895696, mean_q: 0.960154\n",
            " 139793/150000: episode: 2717, duration: 0.588s, episode steps: 37, steps per second: 63, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.649 [2.000, 18.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.007203, mean_absolute_error: 0.896944, mean_q: 0.983359\n",
            " 139995/150000: episode: 2718, duration: 2.887s, episode steps: 202, steps per second: 70, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 8.436 [0.000, 19.000], mean observation: -0.624 [-1.097, 9.221], loss: 0.005831, mean_absolute_error: 0.901920, mean_q: 0.967402\n",
            " 140025/150000: episode: 2719, duration: 0.537s, episode steps: 30, steps per second: 56, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.300 [5.000, 14.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.008085, mean_absolute_error: 0.896189, mean_q: 0.966877\n",
            " 140055/150000: episode: 2720, duration: 0.516s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 10.367 [4.000, 15.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.006198, mean_absolute_error: 0.899462, mean_q: 0.964610\n",
            " 140119/150000: episode: 2721, duration: 1.005s, episode steps: 64, steps per second: 64, episode reward: 2.000, mean reward: 0.031 [0.000, 1.000], mean action: 8.625 [0.000, 18.000], mean observation: -0.631 [-1.011, 1.012], loss: 0.005771, mean_absolute_error: 0.902622, mean_q: 0.973615\n",
            " 140198/150000: episode: 2722, duration: 1.155s, episode steps: 79, steps per second: 68, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 9.177 [0.000, 15.000], mean observation: -0.612 [-1.011, 9.683], loss: 0.006532, mean_absolute_error: 0.904063, mean_q: 0.969479\n",
            " 140236/150000: episode: 2723, duration: 0.618s, episode steps: 38, steps per second: 62, episode reward: 2.000, mean reward: 0.053 [0.000, 1.000], mean action: 5.211 [0.000, 15.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.008154, mean_absolute_error: 0.905150, mean_q: 0.990954\n",
            " 140273/150000: episode: 2724, duration: 0.582s, episode steps: 37, steps per second: 64, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 7.432 [5.000, 19.000], mean observation: -0.633 [-1.009, 1.016], loss: 0.007522, mean_absolute_error: 0.910531, mean_q: 0.978115\n",
            " 140365/150000: episode: 2725, duration: 1.330s, episode steps: 92, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 13.891 [0.000, 19.000], mean observation: -0.616 [-1.028, 9.759], loss: 0.006139, mean_absolute_error: 0.912034, mean_q: 0.981964\n",
            " 140438/150000: episode: 2726, duration: 1.097s, episode steps: 73, steps per second: 67, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 14.753 [0.000, 20.000], mean observation: -0.617 [-1.010, 8.069], loss: 0.005064, mean_absolute_error: 0.911057, mean_q: 0.979954\n",
            " 140468/150000: episode: 2727, duration: 0.507s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006424, mean_absolute_error: 0.908852, mean_q: 0.977023\n",
            " 140484/150000: episode: 2728, duration: 0.321s, episode steps: 16, steps per second: 50, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 9.250 [5.000, 14.000], mean observation: -0.636 [-1.040, 1.004], loss: 0.003758, mean_absolute_error: 0.907220, mean_q: 0.974930\n",
            " 140538/150000: episode: 2729, duration: 0.836s, episode steps: 54, steps per second: 65, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 11.870 [1.000, 18.000], mean observation: -0.612 [-1.018, 7.981], loss: 0.007712, mean_absolute_error: 0.903670, mean_q: 0.969578\n",
            " 140591/150000: episode: 2730, duration: 0.826s, episode steps: 53, steps per second: 64, episode reward: 2.000, mean reward: 0.038 [0.000, 1.000], mean action: 5.264 [2.000, 16.000], mean observation: -0.633 [-1.011, 1.013], loss: 0.007150, mean_absolute_error: 0.897938, mean_q: 0.972602\n",
            " 140683/150000: episode: 2731, duration: 1.308s, episode steps: 92, steps per second: 70, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 7.685 [0.000, 19.000], mean observation: -0.633 [-1.011, 1.083], loss: 0.006603, mean_absolute_error: 0.901635, mean_q: 0.967194\n",
            " 140704/150000: episode: 2732, duration: 0.432s, episode steps: 21, steps per second: 49, episode reward: 1.000, mean reward: 0.048 [0.000, 0.900], mean action: 6.476 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.011], loss: 0.006640, mean_absolute_error: 0.904832, mean_q: 0.967538\n",
            " 140740/150000: episode: 2733, duration: 0.585s, episode steps: 36, steps per second: 62, episode reward: 2.000, mean reward: 0.056 [0.000, 1.000], mean action: 3.667 [0.000, 18.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.005781, mean_absolute_error: 0.902662, mean_q: 0.971420\n",
            " 140770/150000: episode: 2734, duration: 0.520s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 13.267 [1.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.003176, mean_absolute_error: 0.904358, mean_q: 0.974332\n",
            " 140843/150000: episode: 2735, duration: 1.066s, episode steps: 73, steps per second: 68, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 11.945 [5.000, 17.000], mean observation: -0.637 [-1.009, 1.000], loss: 0.006053, mean_absolute_error: 0.902607, mean_q: 0.971160\n",
            " 140876/150000: episode: 2736, duration: 0.530s, episode steps: 33, steps per second: 62, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 11.212 [5.000, 17.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.006728, mean_absolute_error: 0.907524, mean_q: 0.974967\n",
            " 140910/150000: episode: 2737, duration: 0.546s, episode steps: 34, steps per second: 62, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 7.265 [2.000, 17.000], mean observation: -0.633 [-1.011, 1.015], loss: 0.004737, mean_absolute_error: 0.902112, mean_q: 0.973347\n",
            " 140934/150000: episode: 2738, duration: 0.435s, episode steps: 24, steps per second: 55, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 6.083 [5.000, 18.000], mean observation: -0.635 [-1.011, 1.015], loss: 0.005383, mean_absolute_error: 0.902298, mean_q: 0.977112\n",
            " 141240/150000: episode: 2739, duration: 4.185s, episode steps: 306, steps per second: 73, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 7.291 [1.000, 20.000], mean observation: -0.636 [-1.045, 5.938], loss: 0.006137, mean_absolute_error: 0.907823, mean_q: 0.978672\n",
            " 141252/150000: episode: 2740, duration: 0.291s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.000], mean action: 6.667 [3.000, 20.000], mean observation: -0.636 [-1.035, 1.000], loss: 0.004823, mean_absolute_error: 0.911954, mean_q: 0.981285\n",
            " 141261/150000: episode: 2741, duration: 0.237s, episode steps: 9, steps per second: 38, episode reward: 2.000, mean reward: 0.222 [0.000, 2.000], mean action: 6.111 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.002882, mean_absolute_error: 0.908136, mean_q: 0.971989\n",
            " 141302/150000: episode: 2742, duration: 0.697s, episode steps: 41, steps per second: 59, episode reward: 1.000, mean reward: 0.024 [0.000, 0.900], mean action: 4.659 [1.000, 20.000], mean observation: -0.632 [-1.009, 1.006], loss: 0.006892, mean_absolute_error: 0.909215, mean_q: 0.980688\n",
            " 141328/150000: episode: 2743, duration: 0.480s, episode steps: 26, steps per second: 54, episode reward: 0.900, mean reward: 0.035 [0.000, 0.900], mean action: 1.577 [1.000, 12.000], mean observation: -0.631 [-1.011, 2.889], loss: 0.008199, mean_absolute_error: 0.907276, mean_q: 0.983659\n",
            " 141346/150000: episode: 2744, duration: 0.359s, episode steps: 18, steps per second: 50, episode reward: 2.000, mean reward: 0.111 [0.000, 1.100], mean action: 5.111 [1.000, 19.000], mean observation: -0.636 [-1.011, 1.011], loss: 0.008948, mean_absolute_error: 0.914025, mean_q: 0.987210\n",
            " 141434/150000: episode: 2745, duration: 1.292s, episode steps: 88, steps per second: 68, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 6.420 [0.000, 19.000], mean observation: -0.635 [-1.011, 1.239], loss: 0.005578, mean_absolute_error: 0.909472, mean_q: 0.980521\n",
            " 141798/150000: episode: 2746, duration: 4.880s, episode steps: 364, steps per second: 75, episode reward: 0.900, mean reward: 0.002 [0.000, 0.900], mean action: 9.332 [0.000, 20.000], mean observation: -0.634 [-1.009, 5.188], loss: 0.006520, mean_absolute_error: 0.903597, mean_q: 0.972167\n",
            " 141810/150000: episode: 2747, duration: 0.297s, episode steps: 12, steps per second: 40, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.583 [2.000, 12.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.005709, mean_absolute_error: 0.897102, mean_q: 0.958972\n",
            " 141832/150000: episode: 2748, duration: 0.427s, episode steps: 22, steps per second: 52, episode reward: 1.000, mean reward: 0.045 [0.000, 0.900], mean action: 8.909 [5.000, 13.000], mean observation: -0.630 [-1.009, 2.525], loss: 0.008106, mean_absolute_error: 0.895553, mean_q: 0.959559\n",
            " 141844/150000: episode: 2749, duration: 0.281s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 13.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.005618, mean_absolute_error: 0.895698, mean_q: 0.957167\n",
            " 141856/150000: episode: 2750, duration: 0.290s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 7.833 [5.000, 18.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.005442, mean_absolute_error: 0.898365, mean_q: 0.962085\n",
            " 141949/150000: episode: 2751, duration: 1.349s, episode steps: 93, steps per second: 69, episode reward: -0.100, mean reward: -0.001 [-1.000, 0.900], mean action: 7.344 [0.000, 19.000], mean observation: -0.625 [-1.026, 8.742], loss: 0.007911, mean_absolute_error: 0.897619, mean_q: 0.969825\n",
            " 141975/150000: episode: 2752, duration: 0.469s, episode steps: 26, steps per second: 55, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 4.423 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.013], loss: 0.004686, mean_absolute_error: 0.897947, mean_q: 0.963582\n",
            " 141999/150000: episode: 2753, duration: 0.442s, episode steps: 24, steps per second: 54, episode reward: 2.000, mean reward: 0.083 [0.000, 1.000], mean action: 2.000 [0.000, 17.000], mean observation: -0.634 [-1.011, 1.006], loss: 0.007029, mean_absolute_error: 0.896217, mean_q: 0.964824\n",
            " 142064/150000: episode: 2754, duration: 0.992s, episode steps: 65, steps per second: 66, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 13.969 [0.000, 20.000], mean observation: -0.618 [-1.018, 7.981], loss: 0.006309, mean_absolute_error: 0.895940, mean_q: 0.965030\n",
            " 142143/150000: episode: 2755, duration: 1.191s, episode steps: 79, steps per second: 66, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 13.354 [5.000, 19.000], mean observation: -0.615 [-1.009, 8.360], loss: 0.005016, mean_absolute_error: 0.892289, mean_q: 0.958226\n",
            " 142168/150000: episode: 2756, duration: 0.450s, episode steps: 25, steps per second: 56, episode reward: 2.000, mean reward: 0.080 [0.000, 1.000], mean action: 13.360 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.008812, mean_absolute_error: 0.892720, mean_q: 0.960819\n",
            " 142180/150000: episode: 2757, duration: 0.288s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.500 [5.000, 13.000], mean observation: -0.634 [-1.011, 1.061], loss: 0.007547, mean_absolute_error: 0.890009, mean_q: 0.957922\n",
            " 142210/150000: episode: 2758, duration: 0.502s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.006231, mean_absolute_error: 0.890984, mean_q: 0.956497\n",
            " 142405/150000: episode: 2759, duration: 2.717s, episode steps: 195, steps per second: 72, episode reward: 1.000, mean reward: 0.005 [0.000, 0.900], mean action: 7.000 [0.000, 19.000], mean observation: -0.631 [-1.009, 7.856], loss: 0.006472, mean_absolute_error: 0.889512, mean_q: 0.957240\n",
            " 142432/150000: episode: 2760, duration: 0.471s, episode steps: 27, steps per second: 57, episode reward: 2.000, mean reward: 0.074 [0.000, 1.000], mean action: 9.407 [5.000, 18.000], mean observation: -0.634 [-1.011, 1.005], loss: 0.008346, mean_absolute_error: 0.892170, mean_q: 0.953127\n",
            " 142554/150000: episode: 2761, duration: 1.712s, episode steps: 122, steps per second: 71, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 6.197 [0.000, 20.000], mean observation: -0.632 [-1.009, 1.027], loss: 0.005981, mean_absolute_error: 0.888842, mean_q: 0.956895\n",
            " 142584/150000: episode: 2762, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 14.900 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004188, mean_absolute_error: 0.894051, mean_q: 0.955295\n",
            " 142657/150000: episode: 2763, duration: 1.110s, episode steps: 73, steps per second: 66, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 7.479 [1.000, 20.000], mean observation: -0.627 [-1.009, 1.015], loss: 0.008199, mean_absolute_error: 0.884753, mean_q: 0.951435\n",
            " 142755/150000: episode: 2764, duration: 1.390s, episode steps: 98, steps per second: 71, episode reward: 0.900, mean reward: 0.009 [0.000, 0.900], mean action: 7.061 [0.000, 19.000], mean observation: -0.637 [-1.011, 1.168], loss: 0.007476, mean_absolute_error: 0.883044, mean_q: 0.947649\n",
            " 142804/150000: episode: 2765, duration: 0.760s, episode steps: 49, steps per second: 64, episode reward: 2.000, mean reward: 0.041 [0.000, 1.000], mean action: 5.306 [1.000, 20.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.006820, mean_absolute_error: 0.885693, mean_q: 0.960066\n",
            " 142858/150000: episode: 2766, duration: 0.830s, episode steps: 54, steps per second: 65, episode reward: 1.000, mean reward: 0.019 [0.000, 0.900], mean action: 5.574 [0.000, 20.000], mean observation: -0.610 [-1.011, 8.059], loss: 0.007216, mean_absolute_error: 0.886787, mean_q: 0.959690\n",
            " 142888/150000: episode: 2767, duration: 0.511s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.500 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007745, mean_absolute_error: 0.890989, mean_q: 0.960798\n",
            " 142918/150000: episode: 2768, duration: 0.512s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004910, mean_absolute_error: 0.891938, mean_q: 0.964361\n",
            " 142950/150000: episode: 2769, duration: 0.541s, episode steps: 32, steps per second: 59, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 5.406 [4.000, 19.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.003779, mean_absolute_error: 0.894167, mean_q: 0.955601\n",
            " 142966/150000: episode: 2770, duration: 0.334s, episode steps: 16, steps per second: 48, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 6.375 [5.000, 17.000], mean observation: -0.635 [-1.040, 1.006], loss: 0.004353, mean_absolute_error: 0.897390, mean_q: 0.966658\n",
            " 142979/150000: episode: 2771, duration: 0.308s, episode steps: 13, steps per second: 42, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 6.462 [2.000, 20.000], mean observation: -0.636 [-1.045, 1.016], loss: 0.003973, mean_absolute_error: 0.895739, mean_q: 0.962231\n",
            " 143054/150000: episode: 2772, duration: 1.113s, episode steps: 75, steps per second: 67, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 5.613 [0.000, 20.000], mean observation: -0.636 [-1.011, 1.155], loss: 0.006231, mean_absolute_error: 0.902180, mean_q: 0.972380\n",
            " 143091/150000: episode: 2773, duration: 0.604s, episode steps: 37, steps per second: 61, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 10.297 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.005613, mean_absolute_error: 0.897039, mean_q: 0.958109\n",
            " 143170/150000: episode: 2774, duration: 1.160s, episode steps: 79, steps per second: 68, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 7.570 [5.000, 19.000], mean observation: -0.616 [-1.020, 8.158], loss: 0.008799, mean_absolute_error: 0.890822, mean_q: 0.960509\n",
            " 143182/150000: episode: 2775, duration: 0.288s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.005], loss: 0.002301, mean_absolute_error: 0.894508, mean_q: 0.968368\n",
            " 143197/150000: episode: 2776, duration: 0.315s, episode steps: 15, steps per second: 48, episode reward: 2.000, mean reward: 0.133 [0.000, 2.000], mean action: 12.533 [12.000, 20.000], mean observation: -0.632 [-1.011, 1.749], loss: 0.007651, mean_absolute_error: 0.896371, mean_q: 0.969029\n",
            " 143209/150000: episode: 2777, duration: 0.281s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.833 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.105], loss: 0.006710, mean_absolute_error: 0.894770, mean_q: 0.958931\n",
            " 143293/150000: episode: 2778, duration: 1.238s, episode steps: 84, steps per second: 68, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 8.357 [1.000, 19.000], mean observation: -0.620 [-1.009, 7.384], loss: 0.005573, mean_absolute_error: 0.896491, mean_q: 0.968147\n",
            " 143405/150000: episode: 2779, duration: 1.573s, episode steps: 112, steps per second: 71, episode reward: 0.900, mean reward: 0.008 [0.000, 0.900], mean action: 9.473 [0.000, 19.000], mean observation: -0.636 [-1.011, 1.228], loss: 0.006317, mean_absolute_error: 0.899188, mean_q: 0.972426\n",
            " 143470/150000: episode: 2780, duration: 0.997s, episode steps: 65, steps per second: 65, episode reward: 1.000, mean reward: 0.015 [0.000, 0.900], mean action: 8.615 [1.000, 18.000], mean observation: -0.633 [-1.011, 1.161], loss: 0.007025, mean_absolute_error: 0.899084, mean_q: 0.966126\n",
            " 143482/150000: episode: 2781, duration: 0.281s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 14.750 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.012314, mean_absolute_error: 0.895337, mean_q: 0.971181\n",
            " 143512/150000: episode: 2782, duration: 0.516s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 9.000 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005487, mean_absolute_error: 0.890977, mean_q: 0.961161\n",
            " 143547/150000: episode: 2783, duration: 0.565s, episode steps: 35, steps per second: 62, episode reward: 2.000, mean reward: 0.057 [0.000, 2.000], mean action: 11.886 [0.000, 17.000], mean observation: -0.633 [-1.011, 1.639], loss: 0.005097, mean_absolute_error: 0.888059, mean_q: 0.958287\n",
            " 143559/150000: episode: 2784, duration: 0.288s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 16.083 [5.000, 19.000], mean observation: -0.636 [-1.011, 1.012], loss: 0.005624, mean_absolute_error: 0.887424, mean_q: 0.951870\n",
            " 143959/150000: episode: 2785, duration: 5.279s, episode steps: 400, steps per second: 76, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 7.032 [0.000, 20.000], mean observation: -0.621 [-1.111, 10.150], loss: 0.007288, mean_absolute_error: 0.885826, mean_q: 0.958989\n",
            " 144000/150000: episode: 2786, duration: 0.685s, episode steps: 41, steps per second: 60, episode reward: 1.000, mean reward: 0.024 [0.000, 0.900], mean action: 6.171 [5.000, 19.000], mean observation: -0.632 [-1.009, 1.011], loss: 0.004041, mean_absolute_error: 0.892833, mean_q: 0.962685\n",
            " 144047/150000: episode: 2787, duration: 0.770s, episode steps: 47, steps per second: 61, episode reward: 1.000, mean reward: 0.021 [0.000, 0.900], mean action: 5.894 [2.000, 6.000], mean observation: -0.631 [-1.009, 1.012], loss: 0.005457, mean_absolute_error: 0.892478, mean_q: 0.970045\n",
            " 144079/150000: episode: 2788, duration: 0.566s, episode steps: 32, steps per second: 57, episode reward: 2.000, mean reward: 0.062 [0.000, 1.000], mean action: 5.375 [4.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005990, mean_absolute_error: 0.895662, mean_q: 0.972860\n",
            " 144095/150000: episode: 2789, duration: 0.332s, episode steps: 16, steps per second: 48, episode reward: 2.000, mean reward: 0.125 [0.000, 1.000], mean action: 6.125 [5.000, 16.000], mean observation: -0.636 [-1.011, 1.005], loss: 0.009864, mean_absolute_error: 0.898708, mean_q: 0.981159\n",
            " 144157/150000: episode: 2790, duration: 0.924s, episode steps: 62, steps per second: 67, episode reward: 2.000, mean reward: 0.032 [0.000, 1.000], mean action: 10.161 [2.000, 19.000], mean observation: -0.632 [-1.011, 1.014], loss: 0.006059, mean_absolute_error: 0.899534, mean_q: 0.966446\n",
            " 144208/150000: episode: 2791, duration: 0.801s, episode steps: 51, steps per second: 64, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 9.804 [0.000, 19.000], mean observation: -0.610 [-1.011, 8.031], loss: 0.007379, mean_absolute_error: 0.891885, mean_q: 0.960193\n",
            " 144309/150000: episode: 2792, duration: 1.443s, episode steps: 101, steps per second: 70, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 6.475 [0.000, 20.000], mean observation: -0.632 [-1.011, 1.089], loss: 0.006484, mean_absolute_error: 0.892788, mean_q: 0.961569\n",
            " 144339/150000: episode: 2793, duration: 0.519s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.267 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004978, mean_absolute_error: 0.891744, mean_q: 0.964273\n",
            " 144467/150000: episode: 2794, duration: 1.779s, episode steps: 128, steps per second: 72, episode reward: 1.000, mean reward: 0.008 [0.000, 0.900], mean action: 5.945 [0.000, 17.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.006101, mean_absolute_error: 0.891942, mean_q: 0.959377\n",
            " 144607/150000: episode: 2795, duration: 1.991s, episode steps: 140, steps per second: 70, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 11.707 [0.000, 20.000], mean observation: -0.627 [-1.045, 8.142], loss: 0.006828, mean_absolute_error: 0.886448, mean_q: 0.953031\n",
            " 144680/150000: episode: 2796, duration: 1.081s, episode steps: 73, steps per second: 68, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 6.890 [3.000, 19.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.008412, mean_absolute_error: 0.882038, mean_q: 0.954172\n",
            " 144710/150000: episode: 2797, duration: 0.518s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 15.200 [5.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.010253, mean_absolute_error: 0.882952, mean_q: 0.957155\n",
            " 144737/150000: episode: 2798, duration: 0.465s, episode steps: 27, steps per second: 58, episode reward: 2.000, mean reward: 0.074 [0.000, 1.000], mean action: 5.296 [5.000, 13.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.004017, mean_absolute_error: 0.883704, mean_q: 0.950155\n",
            " 144772/150000: episode: 2799, duration: 0.582s, episode steps: 35, steps per second: 60, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 5.771 [4.000, 19.000], mean observation: -0.633 [-1.011, 1.010], loss: 0.006930, mean_absolute_error: 0.881890, mean_q: 0.951778\n",
            " 144945/150000: episode: 2800, duration: 2.581s, episode steps: 173, steps per second: 67, episode reward: 0.900, mean reward: 0.005 [0.000, 0.900], mean action: 6.538 [0.000, 19.000], mean observation: -0.634 [-1.011, 4.766], loss: 0.007476, mean_absolute_error: 0.878909, mean_q: 0.952195\n",
            " 144980/150000: episode: 2801, duration: 0.594s, episode steps: 35, steps per second: 59, episode reward: 2.000, mean reward: 0.057 [0.000, 1.000], mean action: 6.429 [1.000, 12.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.008316, mean_absolute_error: 0.882797, mean_q: 0.951918\n",
            " 144989/150000: episode: 2802, duration: 0.209s, episode steps: 9, steps per second: 43, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 11.667 [5.000, 16.000], mean observation: -0.633 [-1.009, 1.021], loss: 0.005882, mean_absolute_error: 0.886234, mean_q: 0.952827\n",
            " 145001/150000: episode: 2803, duration: 0.286s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.005], loss: 0.004470, mean_absolute_error: 0.883148, mean_q: 0.957512\n",
            " 145010/150000: episode: 2804, duration: 0.252s, episode steps: 9, steps per second: 36, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 10.556 [5.000, 12.000], mean observation: -0.636 [-1.038, 1.000], loss: 0.009227, mean_absolute_error: 0.879788, mean_q: 0.949238\n",
            " 145022/150000: episode: 2805, duration: 0.279s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.167 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.006870, mean_absolute_error: 0.879167, mean_q: 0.936801\n",
            " 145134/150000: episode: 2806, duration: 1.559s, episode steps: 112, steps per second: 72, episode reward: 2.000, mean reward: 0.018 [0.000, 1.000], mean action: 7.446 [1.000, 20.000], mean observation: -0.632 [-1.011, 1.013], loss: 0.005805, mean_absolute_error: 0.879701, mean_q: 0.944588\n",
            " 145144/150000: episode: 2807, duration: 0.261s, episode steps: 10, steps per second: 38, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.100 [0.000, 12.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.005134, mean_absolute_error: 0.878015, mean_q: 0.948902\n",
            " 145154/150000: episode: 2808, duration: 0.258s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 10.600 [5.000, 12.000], mean observation: -0.635 [-1.011, 1.018], loss: 0.008338, mean_absolute_error: 0.878017, mean_q: 0.950095\n",
            " 145169/150000: episode: 2809, duration: 0.323s, episode steps: 15, steps per second: 46, episode reward: 2.000, mean reward: 0.133 [0.000, 1.100], mean action: 11.267 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.140], loss: 0.007907, mean_absolute_error: 0.879855, mean_q: 0.963069\n",
            " 145181/150000: episode: 2810, duration: 0.292s, episode steps: 12, steps per second: 41, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.667 [3.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.008632, mean_absolute_error: 0.884781, mean_q: 0.975621\n",
            " 145193/150000: episode: 2811, duration: 0.271s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.634 [-1.011, 1.114], loss: 0.004629, mean_absolute_error: 0.887388, mean_q: 0.965200\n",
            " 145205/150000: episode: 2812, duration: 0.272s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.083 [5.000, 20.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.006710, mean_absolute_error: 0.888218, mean_q: 0.967552\n",
            " 145217/150000: episode: 2813, duration: 0.286s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 10.917 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.006701, mean_absolute_error: 0.887969, mean_q: 0.969159\n",
            " 145229/150000: episode: 2814, duration: 0.265s, episode steps: 12, steps per second: 45, episode reward: 2.000, mean reward: 0.167 [0.000, 2.000], mean action: 7.583 [0.000, 20.000], mean observation: -0.631 [-1.011, 1.561], loss: 0.002587, mean_absolute_error: 0.889212, mean_q: 0.960769\n",
            " 145333/150000: episode: 2815, duration: 1.495s, episode steps: 104, steps per second: 70, episode reward: 1.000, mean reward: 0.010 [0.000, 0.900], mean action: 7.327 [0.000, 20.000], mean observation: -0.633 [-1.009, 1.128], loss: 0.007452, mean_absolute_error: 0.889064, mean_q: 0.965313\n",
            " 145345/150000: episode: 2816, duration: 0.277s, episode steps: 12, steps per second: 43, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.000 [5.000, 17.000], mean observation: -0.635 [-1.011, 1.002], loss: 0.006873, mean_absolute_error: 0.892617, mean_q: 0.973651\n",
            " 145357/150000: episode: 2817, duration: 0.286s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.167 [5.000, 18.000], mean observation: -0.635 [-1.011, 1.000], loss: 0.010344, mean_absolute_error: 0.888478, mean_q: 0.969094\n",
            " 145508/150000: episode: 2818, duration: 2.103s, episode steps: 151, steps per second: 72, episode reward: 2.000, mean reward: 0.013 [0.000, 1.000], mean action: 9.099 [0.000, 17.000], mean observation: -0.628 [-1.011, 1.047], loss: 0.006843, mean_absolute_error: 0.887695, mean_q: 0.958265\n",
            " 145542/150000: episode: 2819, duration: 0.568s, episode steps: 34, steps per second: 60, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 5.559 [2.000, 15.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.005593, mean_absolute_error: 0.891578, mean_q: 0.958720\n",
            " 145568/150000: episode: 2820, duration: 0.468s, episode steps: 26, steps per second: 56, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 7.615 [0.000, 18.000], mean observation: -0.634 [-1.011, 1.000], loss: 0.006248, mean_absolute_error: 0.891665, mean_q: 0.955489\n",
            " 145581/150000: episode: 2821, duration: 0.298s, episode steps: 13, steps per second: 44, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 7.769 [0.000, 18.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.010495, mean_absolute_error: 0.890221, mean_q: 0.950831\n",
            " 145620/150000: episode: 2822, duration: 0.699s, episode steps: 39, steps per second: 56, episode reward: 1.000, mean reward: 0.026 [0.000, 0.900], mean action: 4.590 [0.000, 15.000], mean observation: -0.632 [-1.011, 1.014], loss: 0.007406, mean_absolute_error: 0.887979, mean_q: 0.950522\n",
            " 145653/150000: episode: 2823, duration: 0.548s, episode steps: 33, steps per second: 60, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 6.970 [4.000, 17.000], mean observation: -0.633 [-1.011, 1.025], loss: 0.011213, mean_absolute_error: 0.882460, mean_q: 0.948536\n",
            " 145665/150000: episode: 2824, duration: 0.288s, episode steps: 12, steps per second: 42, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 12.000 [5.000, 19.000], mean observation: -0.634 [-1.011, 1.006], loss: 0.007911, mean_absolute_error: 0.878215, mean_q: 0.944510\n",
            " 145677/150000: episode: 2825, duration: 0.304s, episode steps: 12, steps per second: 39, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 9.000 [2.000, 17.000], mean observation: -0.636 [-1.011, 1.007], loss: 0.011416, mean_absolute_error: 0.878085, mean_q: 0.941462\n",
            " 146044/150000: episode: 2826, duration: 5.167s, episode steps: 367, steps per second: 71, episode reward: 0.900, mean reward: 0.002 [0.000, 0.900], mean action: 4.741 [1.000, 20.000], mean observation: -0.635 [-1.009, 3.050], loss: 0.006304, mean_absolute_error: 0.880953, mean_q: 0.949220\n",
            " 146108/150000: episode: 2827, duration: 1.020s, episode steps: 64, steps per second: 63, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 8.281 [5.000, 20.000], mean observation: -0.611 [-1.011, 8.366], loss: 0.007122, mean_absolute_error: 0.888791, mean_q: 0.951961\n",
            " 146138/150000: episode: 2828, duration: 0.533s, episode steps: 30, steps per second: 56, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.667 [0.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007568, mean_absolute_error: 0.883862, mean_q: 0.949252\n",
            " 146172/150000: episode: 2829, duration: 0.583s, episode steps: 34, steps per second: 58, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 13.441 [3.000, 20.000], mean observation: -0.633 [-1.011, 1.012], loss: 0.008640, mean_absolute_error: 0.888263, mean_q: 0.953401\n",
            " 146202/150000: episode: 2830, duration: 0.520s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.567 [4.000, 18.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.008364, mean_absolute_error: 0.883304, mean_q: 0.950764\n",
            " 146232/150000: episode: 2831, duration: 0.528s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.000 [3.000, 7.000], mean observation: -0.634 [-1.011, 1.009], loss: 0.011197, mean_absolute_error: 0.887024, mean_q: 0.962810\n",
            " 146632/150000: episode: 2832, duration: 5.380s, episode steps: 400, steps per second: 74, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 7.305 [0.000, 20.000], mean observation: -0.632 [-1.009, 6.498], loss: 0.006480, mean_absolute_error: 0.892064, mean_q: 0.961100\n",
            " 146662/150000: episode: 2833, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 12.967 [2.000, 16.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004420, mean_absolute_error: 0.882029, mean_q: 0.948566\n",
            " 146747/150000: episode: 2834, duration: 1.249s, episode steps: 85, steps per second: 68, episode reward: 0.900, mean reward: 0.011 [0.000, 0.900], mean action: 6.247 [2.000, 19.000], mean observation: -0.638 [-1.011, 1.278], loss: 0.006479, mean_absolute_error: 0.887037, mean_q: 0.955210\n",
            " 146757/150000: episode: 2835, duration: 0.258s, episode steps: 10, steps per second: 39, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 5.300 [1.000, 12.000], mean observation: -0.633 [-1.011, 1.133], loss: 0.005452, mean_absolute_error: 0.883198, mean_q: 0.954925\n",
            " 146869/150000: episode: 2836, duration: 1.582s, episode steps: 112, steps per second: 71, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 14.616 [1.000, 18.000], mean observation: -0.632 [-1.012, 1.000], loss: 0.006564, mean_absolute_error: 0.887782, mean_q: 0.952985\n",
            " 146895/150000: episode: 2837, duration: 0.456s, episode steps: 26, steps per second: 57, episode reward: 2.000, mean reward: 0.077 [0.000, 1.000], mean action: 7.923 [4.000, 17.000], mean observation: -0.634 [-1.011, 1.007], loss: 0.010101, mean_absolute_error: 0.889104, mean_q: 0.954252\n",
            " 146945/150000: episode: 2838, duration: 0.784s, episode steps: 50, steps per second: 64, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 5.860 [4.000, 20.000], mean observation: -0.611 [-1.011, 7.662], loss: 0.006406, mean_absolute_error: 0.890159, mean_q: 0.955893\n",
            " 146992/150000: episode: 2839, duration: 0.747s, episode steps: 47, steps per second: 63, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 6.319 [3.000, 20.000], mean observation: -0.633 [-1.011, 1.014], loss: 0.007559, mean_absolute_error: 0.886962, mean_q: 0.958650\n",
            " 147022/150000: episode: 2840, duration: 0.498s, episode steps: 30, steps per second: 60, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.667 [0.000, 11.000], mean observation: -0.634 [-1.011, 1.010], loss: 0.005928, mean_absolute_error: 0.891849, mean_q: 0.965252\n",
            " 147093/150000: episode: 2841, duration: 1.023s, episode steps: 71, steps per second: 69, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 9.099 [5.000, 20.000], mean observation: -0.636 [-1.011, 1.210], loss: 0.005258, mean_absolute_error: 0.896603, mean_q: 0.962188\n",
            " 147123/150000: episode: 2842, duration: 0.513s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 9.500 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004599, mean_absolute_error: 0.897523, mean_q: 0.963842\n",
            " 147187/150000: episode: 2843, duration: 0.970s, episode steps: 64, steps per second: 66, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 18.766 [2.000, 20.000], mean observation: -0.631 [-1.009, 1.013], loss: 0.006744, mean_absolute_error: 0.895542, mean_q: 0.968328\n",
            " 147217/150000: episode: 2844, duration: 0.554s, episode steps: 30, steps per second: 54, episode reward: 1.000, mean reward: 0.033 [0.000, 0.900], mean action: 18.533 [5.000, 20.000], mean observation: -0.633 [-1.009, 1.010], loss: 0.006053, mean_absolute_error: 0.895289, mean_q: 0.968518\n",
            " 147247/150000: episode: 2845, duration: 0.510s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.233 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.003730, mean_absolute_error: 0.898192, mean_q: 0.970272\n",
            " 147277/150000: episode: 2846, duration: 0.517s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 6.267 [0.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.008261, mean_absolute_error: 0.897734, mean_q: 0.962856\n",
            " 147308/150000: episode: 2847, duration: 0.536s, episode steps: 31, steps per second: 58, episode reward: 2.000, mean reward: 0.065 [0.000, 1.000], mean action: 9.548 [4.000, 20.000], mean observation: -0.634 [-1.011, 1.004], loss: 0.005765, mean_absolute_error: 0.892214, mean_q: 0.960959\n",
            " 147318/150000: episode: 2848, duration: 0.246s, episode steps: 10, steps per second: 41, episode reward: 2.000, mean reward: 0.200 [0.000, 2.000], mean action: 8.800 [5.000, 12.000], mean observation: -0.635 [-1.046, 1.003], loss: 0.011008, mean_absolute_error: 0.898113, mean_q: 0.966249\n",
            " 147330/150000: episode: 2849, duration: 0.275s, episode steps: 12, steps per second: 44, episode reward: 2.000, mean reward: 0.167 [0.000, 1.100], mean action: 11.417 [5.000, 12.000], mean observation: -0.636 [-1.011, 1.000], loss: 0.007716, mean_absolute_error: 0.899237, mean_q: 0.968644\n",
            " 147340/150000: episode: 2850, duration: 0.252s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 5.500 [3.000, 12.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.003999, mean_absolute_error: 0.898186, mean_q: 0.969378\n",
            " 147390/150000: episode: 2851, duration: 0.770s, episode steps: 50, steps per second: 65, episode reward: 0.900, mean reward: 0.018 [0.000, 0.900], mean action: 6.040 [4.000, 18.000], mean observation: -0.611 [-1.011, 7.668], loss: 0.009134, mean_absolute_error: 0.894617, mean_q: 0.965993\n",
            " 147484/150000: episode: 2852, duration: 1.325s, episode steps: 94, steps per second: 71, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 6.394 [2.000, 19.000], mean observation: -0.633 [-1.011, 1.000], loss: 0.007274, mean_absolute_error: 0.893982, mean_q: 0.961763\n",
            " 147507/150000: episode: 2853, duration: 0.433s, episode steps: 23, steps per second: 53, episode reward: 2.000, mean reward: 0.087 [0.000, 1.000], mean action: 12.174 [4.000, 13.000], mean observation: -0.635 [-1.011, 1.004], loss: 0.010243, mean_absolute_error: 0.885741, mean_q: 0.945933\n",
            " 147532/150000: episode: 2854, duration: 0.445s, episode steps: 25, steps per second: 56, episode reward: 2.000, mean reward: 0.080 [0.000, 1.000], mean action: 9.160 [2.000, 19.000], mean observation: -0.634 [-1.011, 1.005], loss: 0.005359, mean_absolute_error: 0.888148, mean_q: 0.953287\n",
            " 147594/150000: episode: 2855, duration: 0.962s, episode steps: 62, steps per second: 64, episode reward: 1.000, mean reward: 0.016 [0.000, 0.900], mean action: 17.145 [5.000, 19.000], mean observation: -0.630 [-1.009, 1.014], loss: 0.005569, mean_absolute_error: 0.891638, mean_q: 0.959282\n",
            " 147668/150000: episode: 2856, duration: 1.104s, episode steps: 74, steps per second: 67, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 8.541 [1.000, 17.000], mean observation: -0.631 [-1.009, 1.067], loss: 0.006954, mean_absolute_error: 0.888717, mean_q: 0.951404\n",
            " 147814/150000: episode: 2857, duration: 2.022s, episode steps: 146, steps per second: 72, episode reward: 0.900, mean reward: 0.006 [0.000, 0.900], mean action: 9.493 [2.000, 19.000], mean observation: -0.637 [-1.011, 1.024], loss: 0.007152, mean_absolute_error: 0.888267, mean_q: 0.960886\n",
            " 148079/150000: episode: 2858, duration: 3.606s, episode steps: 265, steps per second: 73, episode reward: 1.000, mean reward: 0.004 [0.000, 0.900], mean action: 10.468 [0.000, 20.000], mean observation: -0.626 [-1.015, 8.228], loss: 0.007827, mean_absolute_error: 0.894076, mean_q: 0.966136\n",
            " 148166/150000: episode: 2859, duration: 1.249s, episode steps: 87, steps per second: 70, episode reward: 0.900, mean reward: 0.010 [0.000, 0.900], mean action: 8.885 [1.000, 16.000], mean observation: -0.636 [-1.009, 1.031], loss: 0.005589, mean_absolute_error: 0.899635, mean_q: 0.965665\n",
            " 148535/150000: episode: 2860, duration: 4.961s, episode steps: 369, steps per second: 74, episode reward: 1.000, mean reward: 0.003 [0.000, 0.900], mean action: 6.512 [0.000, 20.000], mean observation: -0.635 [-1.040, 8.850], loss: 0.006961, mean_absolute_error: 0.903514, mean_q: 0.970998\n",
            " 148604/150000: episode: 2861, duration: 1.033s, episode steps: 69, steps per second: 67, episode reward: 0.900, mean reward: 0.013 [0.000, 0.900], mean action: 13.014 [4.000, 20.000], mean observation: -0.617 [-1.011, 8.005], loss: 0.004783, mean_absolute_error: 0.895782, mean_q: 0.963406\n",
            " 148634/150000: episode: 2862, duration: 0.505s, episode steps: 30, steps per second: 59, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 15.500 [5.000, 20.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007293, mean_absolute_error: 0.896141, mean_q: 0.962119\n",
            " 148667/150000: episode: 2863, duration: 0.553s, episode steps: 33, steps per second: 60, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 9.939 [4.000, 20.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.004716, mean_absolute_error: 0.890876, mean_q: 0.959780\n",
            " 148695/150000: episode: 2864, duration: 0.490s, episode steps: 28, steps per second: 57, episode reward: 2.000, mean reward: 0.071 [0.000, 1.000], mean action: 17.714 [3.000, 20.000], mean observation: -0.632 [-1.011, 2.029], loss: 0.007203, mean_absolute_error: 0.893160, mean_q: 0.959371\n",
            " 148720/150000: episode: 2865, duration: 0.457s, episode steps: 25, steps per second: 55, episode reward: 2.000, mean reward: 0.080 [0.000, 1.100], mean action: 10.000 [2.000, 19.000], mean observation: -0.635 [-1.011, 1.018], loss: 0.004995, mean_absolute_error: 0.893729, mean_q: 0.954409\n",
            " 148802/150000: episode: 2866, duration: 1.227s, episode steps: 82, steps per second: 67, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 5.549 [1.000, 20.000], mean observation: -0.616 [-1.021, 8.457], loss: 0.005989, mean_absolute_error: 0.892924, mean_q: 0.963649\n",
            " 148872/150000: episode: 2867, duration: 1.075s, episode steps: 70, steps per second: 65, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 4.143 [0.000, 14.000], mean observation: -0.634 [-1.011, 1.005], loss: 0.004141, mean_absolute_error: 0.898357, mean_q: 0.963543\n",
            " 148902/150000: episode: 2868, duration: 0.489s, episode steps: 30, steps per second: 61, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 16.133 [5.000, 17.000], mean observation: -0.633 [-1.009, 1.015], loss: 0.005661, mean_absolute_error: 0.897374, mean_q: 0.966287\n",
            " 148988/150000: episode: 2869, duration: 1.233s, episode steps: 86, steps per second: 70, episode reward: 1.000, mean reward: 0.012 [0.000, 0.900], mean action: 8.291 [0.000, 18.000], mean observation: -0.632 [-1.011, 1.127], loss: 0.006536, mean_absolute_error: 0.890871, mean_q: 0.956150\n",
            " 149123/150000: episode: 2870, duration: 1.854s, episode steps: 135, steps per second: 73, episode reward: 1.000, mean reward: 0.007 [0.000, 0.900], mean action: 4.807 [4.000, 19.000], mean observation: -0.630 [-1.009, 1.401], loss: 0.007410, mean_absolute_error: 0.886711, mean_q: 0.953699\n",
            " 149153/150000: episode: 2871, duration: 0.514s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.133 [2.000, 12.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004996, mean_absolute_error: 0.888349, mean_q: 0.955661\n",
            " 149162/150000: episode: 2872, duration: 0.211s, episode steps: 9, steps per second: 43, episode reward: 2.000, mean reward: 0.222 [0.000, 1.100], mean action: 6.667 [5.000, 13.000], mean observation: -0.634 [-1.009, 1.024], loss: 0.000192, mean_absolute_error: 0.891563, mean_q: 0.957761\n",
            " 149270/150000: episode: 2873, duration: 1.535s, episode steps: 108, steps per second: 70, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 6.102 [0.000, 20.000], mean observation: -0.633 [-1.011, 1.273], loss: 0.006032, mean_absolute_error: 0.890168, mean_q: 0.956901\n",
            " 149386/150000: episode: 2874, duration: 1.627s, episode steps: 116, steps per second: 71, episode reward: 1.000, mean reward: 0.009 [0.000, 0.900], mean action: 13.026 [0.000, 20.000], mean observation: -0.630 [-1.009, 1.000], loss: 0.006742, mean_absolute_error: 0.892316, mean_q: 0.963866\n",
            " 149455/150000: episode: 2875, duration: 1.010s, episode steps: 69, steps per second: 68, episode reward: 1.000, mean reward: 0.014 [0.000, 0.900], mean action: 13.116 [1.000, 19.000], mean observation: -0.613 [-1.011, 8.974], loss: 0.006336, mean_absolute_error: 0.894846, mean_q: 0.971498\n",
            " 149488/150000: episode: 2876, duration: 0.561s, episode steps: 33, steps per second: 59, episode reward: 2.000, mean reward: 0.061 [0.000, 1.000], mean action: 12.061 [4.000, 17.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.005947, mean_absolute_error: 0.895047, mean_q: 0.964922\n",
            " 149518/150000: episode: 2877, duration: 0.515s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.967 [4.000, 5.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.007990, mean_absolute_error: 0.894980, mean_q: 0.959961\n",
            " 149552/150000: episode: 2878, duration: 0.611s, episode steps: 34, steps per second: 56, episode reward: 2.000, mean reward: 0.059 [0.000, 1.000], mean action: 5.618 [4.000, 18.000], mean observation: -0.634 [-1.011, 1.011], loss: 0.009450, mean_absolute_error: 0.896122, mean_q: 0.973102\n",
            " 149589/150000: episode: 2879, duration: 0.610s, episode steps: 37, steps per second: 61, episode reward: 2.000, mean reward: 0.054 [0.000, 1.000], mean action: 5.541 [3.000, 18.000], mean observation: -0.633 [-1.011, 1.011], loss: 0.005262, mean_absolute_error: 0.898717, mean_q: 0.979709\n",
            " 149611/150000: episode: 2880, duration: 0.418s, episode steps: 22, steps per second: 53, episode reward: 2.000, mean reward: 0.091 [0.000, 1.000], mean action: 5.818 [1.000, 14.000], mean observation: -0.635 [-1.011, 1.010], loss: 0.003855, mean_absolute_error: 0.903625, mean_q: 0.967215\n",
            " 149704/150000: episode: 2881, duration: 1.348s, episode steps: 93, steps per second: 69, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 4.591 [0.000, 19.000], mean observation: -0.632 [-1.011, 1.211], loss: 0.005814, mean_absolute_error: 0.907971, mean_q: 0.975300\n",
            " 149734/150000: episode: 2882, duration: 0.524s, episode steps: 30, steps per second: 57, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 4.033 [0.000, 19.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.004702, mean_absolute_error: 0.907240, mean_q: 0.972403\n",
            " 149744/150000: episode: 2883, duration: 0.250s, episode steps: 10, steps per second: 40, episode reward: 2.000, mean reward: 0.200 [0.000, 1.100], mean action: 6.100 [0.000, 19.000], mean observation: -0.635 [-1.011, 1.015], loss: 0.009636, mean_absolute_error: 0.904084, mean_q: 0.962140\n",
            " 149823/150000: episode: 2884, duration: 1.147s, episode steps: 79, steps per second: 69, episode reward: 1.000, mean reward: 0.013 [0.000, 0.900], mean action: 10.291 [0.000, 20.000], mean observation: -0.618 [-1.009, 8.035], loss: 0.005243, mean_absolute_error: 0.901976, mean_q: 0.972428\n",
            " 149844/150000: episode: 2885, duration: 0.382s, episode steps: 21, steps per second: 55, episode reward: 2.000, mean reward: 0.095 [0.000, 1.000], mean action: 8.048 [0.000, 19.000], mean observation: -0.635 [-1.011, 1.020], loss: 0.007101, mean_absolute_error: 0.905889, mean_q: 0.969855\n",
            " 149857/150000: episode: 2886, duration: 0.305s, episode steps: 13, steps per second: 43, episode reward: 2.000, mean reward: 0.154 [0.000, 1.000], mean action: 6.077 [4.000, 13.000], mean observation: -0.636 [-1.011, 1.017], loss: 0.006106, mean_absolute_error: 0.903389, mean_q: 0.972858\n",
            " 149887/150000: episode: 2887, duration: 0.514s, episode steps: 30, steps per second: 58, episode reward: 2.000, mean reward: 0.067 [0.000, 1.000], mean action: 5.100 [5.000, 8.000], mean observation: -0.634 [-1.011, 1.012], loss: 0.011674, mean_absolute_error: 0.899114, mean_q: 0.967723\n",
            " 149978/150000: episode: 2888, duration: 1.348s, episode steps: 91, steps per second: 68, episode reward: 1.000, mean reward: 0.011 [0.000, 0.900], mean action: 6.077 [0.000, 19.000], mean observation: -0.635 [-1.009, 1.106], loss: 0.006133, mean_absolute_error: 0.904616, mean_q: 0.984139\n",
            "done, took 2226.696 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45gd9tWmzT7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import pickle\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def print_rewards(history):\n",
        "  train_rewards = history.history['episode_reward']\n",
        "  plt.plot(train_rewards)\n",
        "  plt.title('model episode reward')\n",
        "  plt.ylabel('reward')\n",
        "  plt.xlabel('episode')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAsUD6fU7Urh",
        "colab_type": "code",
        "outputId": "dd5062f2-74f4-4b55-b5c6-164e1196a21d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "print(h_train.history.keys())\n",
        "print_rewards(h_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['episode_reward', 'nb_episode_steps', 'nb_steps'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nogAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS-bK5qMQBfd",
        "colab_type": "text"
      },
      "source": [
        "Se observan las recompenzas obtenidas por el agente en cada episodio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u-vQMXw1ERS",
        "colab_type": "text"
      },
      "source": [
        "# Test!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fnjDC1OQHSX",
        "colab_type": "text"
      },
      "source": [
        "Se prueba 50 veces el agente en el mismo environment para probar que tan bueno es y se muestra en una gráfica su recompenza obtenida durante estos 50 episodios."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3uIt26auEs0",
        "colab_type": "code",
        "outputId": "c060febb-ec33-4bdd-da3e-b64ae38510d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "h_test = dqn.test(env, nb_episodes=50, visualize=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 50 episodes ...\n",
            "Episode 1: reward: 2.000, steps: 30\n",
            "Episode 2: reward: 2.000, steps: 30\n",
            "Episode 3: reward: 2.000, steps: 30\n",
            "Episode 4: reward: 2.000, steps: 30\n",
            "Episode 5: reward: 2.000, steps: 30\n",
            "Episode 6: reward: 2.000, steps: 30\n",
            "Episode 7: reward: 2.000, steps: 30\n",
            "Episode 8: reward: 2.000, steps: 30\n",
            "Episode 9: reward: 2.000, steps: 30\n",
            "Episode 10: reward: 2.000, steps: 30\n",
            "Episode 11: reward: 2.000, steps: 30\n",
            "Episode 12: reward: 2.000, steps: 30\n",
            "Episode 13: reward: 2.000, steps: 30\n",
            "Episode 14: reward: 2.000, steps: 30\n",
            "Episode 15: reward: 2.000, steps: 30\n",
            "Episode 16: reward: 2.000, steps: 30\n",
            "Episode 17: reward: 2.000, steps: 30\n",
            "Episode 18: reward: 2.000, steps: 30\n",
            "Episode 19: reward: 2.000, steps: 30\n",
            "Episode 20: reward: 2.000, steps: 30\n",
            "Episode 21: reward: 2.000, steps: 30\n",
            "Episode 22: reward: 2.000, steps: 30\n",
            "Episode 23: reward: 2.000, steps: 30\n",
            "Episode 24: reward: 2.000, steps: 30\n",
            "Episode 25: reward: 2.000, steps: 30\n",
            "Episode 26: reward: 2.000, steps: 30\n",
            "Episode 27: reward: 2.000, steps: 30\n",
            "Episode 28: reward: 2.000, steps: 30\n",
            "Episode 29: reward: 2.000, steps: 30\n",
            "Episode 30: reward: 2.000, steps: 30\n",
            "Episode 31: reward: 2.000, steps: 30\n",
            "Episode 32: reward: 2.000, steps: 30\n",
            "Episode 33: reward: 2.000, steps: 30\n",
            "Episode 34: reward: 2.000, steps: 30\n",
            "Episode 35: reward: 2.000, steps: 30\n",
            "Episode 36: reward: 2.000, steps: 30\n",
            "Episode 37: reward: 2.000, steps: 30\n",
            "Episode 38: reward: 2.000, steps: 30\n",
            "Episode 39: reward: 2.000, steps: 30\n",
            "Episode 40: reward: 2.000, steps: 30\n",
            "Episode 41: reward: 2.000, steps: 30\n",
            "Episode 42: reward: 2.000, steps: 30\n",
            "Episode 43: reward: 2.000, steps: 30\n",
            "Episode 44: reward: 2.000, steps: 30\n",
            "Episode 45: reward: 2.000, steps: 30\n",
            "Episode 46: reward: 2.000, steps: 30\n",
            "Episode 47: reward: 2.000, steps: 30\n",
            "Episode 48: reward: 2.000, steps: 30\n",
            "Episode 49: reward: 2.000, steps: 30\n",
            "Episode 50: reward: 2.000, steps: 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dThWfbqkTE4A",
        "colab_type": "text"
      },
      "source": [
        "Como bien dice el paper, se obtiene 0.1 puntos cada vez que se llega a un \"checkpoint\" (mientras mas se acerca el jugador al area rival) y 1 punto por cada gol. La máxima recompenza obtenible por el agente durante un episodio es de 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhuLiXbDX76s",
        "colab_type": "code",
        "outputId": "79223a4e-a10f-4188-b7a3-a62b99a6b923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "print_rewards(h_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}